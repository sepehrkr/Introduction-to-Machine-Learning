{"cells":[{"cell_type":"markdown","id":"1b46fe41","metadata":{"id":"1b46fe41"},"source":["<div align=center>\n","\n","<font size=5>\n","    In the Name of God\n","<font/>\n","<br/>\n","<br/>\n","<font>\n","    Sharif University of Technology - Departmenet of Electrical Engineering\n","</font>\n","<br/>\n","<font>\n","    Introducing with Machine Learing - Dr. S. Amini\n","</font>\n","<br/>\n","<br/>\n","Spring 2023\n","\n","</div>\n","\n","<hr/>\n","<div align=center>\n","<font size=6>\n","    Neural Networks Practical Assignment\n","    \n","    Question 1\n","</font>\n","<br/>\t\t\n","<font size=4>\n","<br/>\n","</div>"]},{"cell_type":"markdown","id":"24a0fc13","metadata":{"id":"24a0fc13"},"source":["# Personal Data"]},{"cell_type":"code","execution_count":1,"id":"44babb65","metadata":{"id":"44babb65","executionInfo":{"status":"ok","timestamp":1686153574759,"user_tz":-210,"elapsed":536,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["# Set your student number\n","student_number = 99106599\n","Name = 'Sepehr'\n","Last_Name = 'Kazemi Ranjbar'"]},{"cell_type":"markdown","id":"ca4a337a","metadata":{"id":"ca4a337a"},"source":["# Rules\n","- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n","\n","- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n","\n","- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch. "]},{"cell_type":"code","execution_count":null,"id":"12b76789","metadata":{"id":"12b76789"},"outputs":[],"source":["!pip install numpy\n","!pip install matplotlib\n","!pip install torchvision\n","!pip install torch"]},{"cell_type":"markdown","id":"886188c7","metadata":{"id":"886188c7"},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":2,"id":"55a0adcc","metadata":{"id":"55a0adcc","executionInfo":{"status":"ok","timestamp":1686155255586,"user_tz":-210,"elapsed":2128,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["import torch\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from typing import Dict\n","import math"]},{"cell_type":"markdown","id":"18510868","metadata":{"id":"18510868"},"source":["## Datasets and Dataloaders\n","\n","Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."]},{"cell_type":"code","execution_count":3,"id":"dc8759e2","metadata":{"id":"dc8759e2","executionInfo":{"status":"ok","timestamp":1686155257448,"user_tz":-210,"elapsed":4,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n","test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"]},{"cell_type":"code","execution_count":4,"id":"8f6763e6","metadata":{"id":"8f6763e6","executionInfo":{"status":"ok","timestamp":1686155259361,"user_tz":-210,"elapsed":4,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["image_shape = train_set[0][0].shape\n","input_dim = np.prod(image_shape).item()\n","num_classes = len(FashionMNIST.classes)"]},{"cell_type":"code","execution_count":5,"id":"c695ff60","metadata":{"id":"c695ff60","executionInfo":{"status":"ok","timestamp":1686155261430,"user_tz":-210,"elapsed":1,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["train_loader = DataLoader(train_set, 64, shuffle=True)\n","test_loader = DataLoader(test_set, 64, shuffle=True)"]},{"cell_type":"markdown","id":"f9dac6c2","metadata":{"id":"f9dac6c2"},"source":["## Visualization\n","\n","Visualize 1 random image from each class\n","\n","- **Hint**:  You can use `plt.subplots` for visualization"]},{"cell_type":"code","execution_count":6,"id":"e3d6b0c1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"e3d6b0c1","executionInfo":{"status":"ok","timestamp":1686155265453,"user_tz":-210,"elapsed":1524,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}},"outputId":"436ca5ea-804a-43e1-c122-bd7c7f2d48cc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["## FILL HERE\n","dataset_size = len(train_set)\n","indexes = np.zeros(num_classes)\n","for i in range(0,num_classes):\n","    index = np.random.choice(dataset_size)\n","    while train_set[index][1] != i:\n","        index = np.random.choice(dataset_size)\n","    indexes[i] = index\n","for i in range(num_classes):\n","    plt.subplot(2,5,i+1)\n","    plt.imshow(train_set[int(indexes[i])][0][0,:,:],cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","id":"a94c5aba","metadata":{"id":"a94c5aba"},"source":["## Initializing model's parameters\n","\n","In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."]},{"cell_type":"code","execution_count":7,"id":"e6d40952","metadata":{"id":"e6d40952","executionInfo":{"status":"ok","timestamp":1686155268235,"user_tz":-210,"elapsed":2,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def add_linear_layer(parameters: dict, shape, device, i=None):\n","    \"\"\"\n","    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n","    \"\"\"\n","    n_in, n_out = shape\n","    with torch.no_grad():\n","        w = torch.zeros(*shape, device=device)\n","        # kaiming initialization for ReLU activations:\n","        bound = 1 / np.sqrt(n_in).item()\n","        w.uniform_(-bound, bound)\n","        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n","    w.requires_grad = True\n","    b.requires_grad = True\n","    # `i` is used to give numbers to parameter names\n","    parameters.update({f'w{i}': w, f'b{i}': b})"]},{"cell_type":"markdown","id":"ce914706","metadata":{"id":"ce914706"},"source":["Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."]},{"cell_type":"code","execution_count":28,"id":"8f3867d7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f3867d7","executionInfo":{"status":"ok","timestamp":1686156673051,"user_tz":-210,"elapsed":589,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}},"outputId":"371a11ae-f9f3-4291-a960-8f2c6d8b2f61"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"]},"metadata":{},"execution_count":28}],"source":["layers = [\n","    (input_dim, 512),\n","    (512, 256),\n","    (256, 128),\n","    (128, 64),\n","    (64, num_classes)\n","]\n","num_layers = len(layers)\n","parameters = {}\n","\n","# setting the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# adding the parameters to the dictionary\n","for i, shape in enumerate(layers):\n","    add_linear_layer(parameters, shape, device, i)\n","\n","parameters.keys()"]},{"cell_type":"markdown","id":"8bfd2c8e","metadata":{"id":"8bfd2c8e"},"source":["## Defining the required functions\n","\n","In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."]},{"cell_type":"markdown","id":"f3b413d8","metadata":{"id":"f3b413d8"},"source":["Computing affine and relu outputs:"]},{"cell_type":"code","execution_count":9,"id":"bebeeb0e","metadata":{"id":"bebeeb0e","executionInfo":{"status":"ok","timestamp":1686155274129,"user_tz":-210,"elapsed":2,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def affine_forward(x, w, b):\n","    ## FILL HERE\n","    return torch.matmul(x,w) + b\n","\n","def relu(x):\n","    ## FILL HERE\n","    return torch.maximum(x,torch.zeros(x.shape,device=device))"]},{"cell_type":"markdown","id":"5d9baa5e","metadata":{"id":"5d9baa5e"},"source":["Function `model` returns output of the whole model for the input `x` using the parameters:"]},{"cell_type":"code","execution_count":10,"id":"d2562962","metadata":{"id":"d2562962","executionInfo":{"status":"ok","timestamp":1686155276095,"user_tz":-210,"elapsed":1,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def model(x: torch.Tensor, parameters, num_layers=num_layers):\n","    # number of batches\n","    B = x.shape[0]\n","    x = x.view(B, -1)\n","    \n","    ## FILL HERE\n","    output_size = len(parameters[f'b{num_layers-1}'])\n","    output = torch.zeros(B,output_size)\n","    \n","    # layer 1\n","    x = affine_forward(x,parameters[f'w{0}'],parameters[f'b{0}'].repeat(B,1))\n","    x = relu(x)\n","\n","    # layer 2\n","    x = affine_forward(x,parameters[f'w{1}'],parameters[f'b{1}'].repeat(B,1))\n","    x = relu(x)\n","\n","    # layer 3\n","    x = affine_forward(x,parameters[f'w{2}'],parameters[f'b{2}'].repeat(B,1))\n","    x = relu(x)\n","\n","    # layer 4\n","    x = affine_forward(x,parameters[f'w{3}'],parameters[f'b{3}'].repeat(B,1))\n","    x = relu(x)\n","\n","    # layer 5\n","    x = affine_forward(x,parameters[f'w{4}'],parameters[f'b{4}'].repeat(B,1))\n","\n","    # softmax layer\n","    output = torch.exp(x) / torch.exp(x).sum(-1).view(B,1)\n","\n","    return output"]},{"cell_type":"markdown","id":"d17a9b4c","metadata":{"id":"d17a9b4c"},"source":["Implementing cross entropy loss:"]},{"cell_type":"code","execution_count":11,"id":"6959621c","metadata":{"id":"6959621c","executionInfo":{"status":"ok","timestamp":1686155278175,"user_tz":-210,"elapsed":4,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def cross_entropy_loss(scores, y):\n","    n = len(y)\n","    ## FILL HERE\n","    loss = 0\n","    log_scores = torch.log(scores)\n","    for i in range(n):\n","        label = y[i]\n","        loss += -log_scores[i,label]\n","    return loss"]},{"cell_type":"markdown","id":"15a589af","metadata":{"id":"15a589af"},"source":["Implementing a function for optimizing paramters and a function to zeroing out their gradients:"]},{"cell_type":"code","execution_count":20,"id":"3121c147","metadata":{"id":"3121c147","executionInfo":{"status":"ok","timestamp":1686156485092,"user_tz":-210,"elapsed":763,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n","    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n","    gradient (parameter.grad). Finally, you should zero the gradients of the parameters after updating\n","    the parameter value.'''\n","    ## FILL HERE\n","    with torch.no_grad():\n","        for param in parameters.values():\n","            param -= learning_rate * param.grad\n","            param.grad.zero_()"]},{"cell_type":"markdown","id":"e17b4cf8","metadata":{"id":"e17b4cf8"},"source":["Training functions:"]},{"cell_type":"code","execution_count":25,"id":"76c0f03b","metadata":{"id":"76c0f03b","executionInfo":{"status":"ok","timestamp":1686156582623,"user_tz":-210,"elapsed":696,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n","    ## FILL HERE\n","    \"\"\"n = len(y_pred)\n","    check = 0\n","    for i in range(n):\n","        if y_pred == y_true:\n","            check += 1\n","    acc = check / n \"\"\"\n","    acc = (y_pred==y_true).sum().item()/len(y_pred)\n","    return acc\n","\n","def train(train_loader, learning_rate=0.001, epoch=None):\n","    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n","        1- Calculate the output of the model to the given input batch\n","        2- Calculate the loss based on the model output\n","        3- Update the gradients using loss.backward() method\n","        4- Optimize the model parameters using the sgd_optimizer function defined previously\n","        5- Print the train loss (Show the epoch and batch as well)\n","        '''\n","    train_loss = 0\n","    batch_num = len(train_loader)\n","    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n","    # for calculateing the accuracy later\n","    Y = []\n","    Y_pred = []\n","    \n","    \n","    for i, (x, y) in enumerate(train_loader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        p = model(x, parameters)\n","\n","        ## FILL HERE\n","        loss = cross_entropy_loss(p,y)\n","        loss.backward()\n","        sgd_optimizer(parameters,learning_rate)\n","        \n","        print(\"epoch: \",epoch,\", batch: \",i,\", train loss: \",loss.item())\n","        train_loss += loss.item()\n","\n","        y_pred = p.argmax(dim=-1)\n","        Y.append(y.cpu().numpy())\n","        Y_pred.append(y_pred.cpu().numpy())\n","\n","    Y = np.concatenate(Y)\n","    Y_pred = np.concatenate(Y_pred)\n","    acc = accuracy(Y_pred, Y)\n","    train_loss /= batch_num\n","    print(f'Accuracy of train set: {acc}')\n","    return train_loss, acc\n","\n","\n","def validate(loader, epoch=None, set_name=None):\n","    '''This function validates the model on the test dataloader. The function goes through each batch and does\n","    the following on each batch:\n","        1- Calculate the model output\n","        2- Calculate the loss using the model output\n","        3- Print the loss for each batch and epoch\n","    \n","    Finally the function calculates the model accuracy.'''\n","    total_loss = 0\n","    N = len(loader.dataset)\n","    \n","    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n","    # for calculateing the accuracy later\n","    Y = []\n","    Y_pred = []\n","    for i, (x, y) in enumerate(loader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        p = model(x, parameters)\n","\n","        ## FILL HERE\n","        loss = cross_entropy_loss(p,y)\n","        total_loss += loss.item()\n","        print(\"epoch: \",epoch,\", batch: \",i,\", test loss: \",loss.item())\n","\n","        y_pred = p.argmax(dim=-1)\n","        Y.append(y.cpu().numpy())\n","        Y_pred.append(y_pred.cpu().numpy())\n","    Y = np.concatenate(Y)\n","    Y_pred = np.concatenate(Y_pred)\n","    total_loss /= N\n","    acc = accuracy(Y_pred, Y)\n","    print(f'Accuracy of {set_name} set: {acc}')\n","\n","    return total_loss, acc"]},{"cell_type":"code","execution_count":29,"id":"87ebb4b6","metadata":{"id":"87ebb4b6","executionInfo":{"status":"ok","timestamp":1686156680992,"user_tz":-210,"elapsed":1065,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["train_losses = []\n","test_losses = []\n","train_accuracies = []\n","test_accuracies = []"]},{"cell_type":"code","execution_count":24,"id":"28d4eb0b","metadata":{"id":"28d4eb0b","executionInfo":{"status":"ok","timestamp":1686156512177,"user_tz":-210,"elapsed":5,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}}},"outputs":[],"source":["def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n","    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n","    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n","    train_loader, test_loader = dataloaders\n","\n","    for epoch in range(num_epochs):\n","        ## FILL HERE\n","        ## You should calculate the train and test loss and accuracies for each epoch and add them to\n","        ## the lists `train_losses`, `test_losses`, `train_accuracies` and `test_accuracies`\n","        tl,ta =  train(train_loader,learning_rate,epoch)\n","        train_losses.append(tl)\n","        train_accuracies.append(ta)\n","\n","        tl,ta =  validate(test_loader,epoch,model_name)\n","        test_losses.append(tl)\n","        test_accuracies.append(ta)\n","\n","    ## plot the loss history of training and test sets \n","    ## FILL HERE\n","    epoch_array = np.arange(num_epochs)\n","    plt.figure(figsize=(12,7))\n","    plt.plot(epoch_array,train_losses,epoch_array,test_losses)\n","    plt.grid(True,'both','both')\n","    plt.legend([\"training losses\",'test losses'],fontsize=15)\n","    plt.xlabel('epoch',fontsize=18)\n","    ## plot the accuracy history of training and test sets\n","    ## FILL HERE"]},{"cell_type":"code","execution_count":30,"id":"2ec4bdd2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2ec4bdd2","executionInfo":{"status":"ok","timestamp":1686157031939,"user_tz":-210,"elapsed":348845,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}},"outputId":"cde58a70-ac45-4163-ecfa-fcd3d3ed20e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","epoch:  10 , batch:  485 , train loss:  28.991445541381836\n","epoch:  10 , batch:  486 , train loss:  18.611948013305664\n","epoch:  10 , batch:  487 , train loss:  20.092422485351562\n","epoch:  10 , batch:  488 , train loss:  10.54990005493164\n","epoch:  10 , batch:  489 , train loss:  26.199100494384766\n","epoch:  10 , batch:  490 , train loss:  18.9924259185791\n","epoch:  10 , batch:  491 , train loss:  17.459529876708984\n","epoch:  10 , batch:  492 , train loss:  19.660165786743164\n","epoch:  10 , batch:  493 , train loss:  23.68290901184082\n","epoch:  10 , batch:  494 , train loss:  14.608702659606934\n","epoch:  10 , batch:  495 , train loss:  26.84420394897461\n","epoch:  10 , batch:  496 , train loss:  23.372116088867188\n","epoch:  10 , batch:  497 , train loss:  15.817397117614746\n","epoch:  10 , batch:  498 , train loss:  19.518644332885742\n","epoch:  10 , batch:  499 , train loss:  9.875357627868652\n","epoch:  10 , batch:  500 , train loss:  13.231407165527344\n","epoch:  10 , batch:  501 , train loss:  18.860557556152344\n","epoch:  10 , batch:  502 , train loss:  12.81946849822998\n","epoch:  10 , batch:  503 , train loss:  16.858642578125\n","epoch:  10 , batch:  504 , train loss:  11.758551597595215\n","epoch:  10 , batch:  505 , train loss:  26.78111457824707\n","epoch:  10 , batch:  506 , train loss:  24.01231575012207\n","epoch:  10 , batch:  507 , train loss:  21.295270919799805\n","epoch:  10 , batch:  508 , train loss:  23.088369369506836\n","epoch:  10 , batch:  509 , train loss:  15.595056533813477\n","epoch:  10 , batch:  510 , train loss:  20.730451583862305\n","epoch:  10 , batch:  511 , train loss:  13.806692123413086\n","epoch:  10 , batch:  512 , train loss:  12.761967658996582\n","epoch:  10 , batch:  513 , train loss:  23.079612731933594\n","epoch:  10 , batch:  514 , train loss:  16.46413230895996\n","epoch:  10 , batch:  515 , train loss:  7.404480457305908\n","epoch:  10 , batch:  516 , train loss:  16.849895477294922\n","epoch:  10 , batch:  517 , train loss:  9.358476638793945\n","epoch:  10 , batch:  518 , train loss:  18.73137664794922\n","epoch:  10 , batch:  519 , train loss:  14.225369453430176\n","epoch:  10 , batch:  520 , train loss:  14.615087509155273\n","epoch:  10 , batch:  521 , train loss:  16.198091506958008\n","epoch:  10 , batch:  522 , train loss:  11.775914192199707\n","epoch:  10 , batch:  523 , train loss:  13.75434398651123\n","epoch:  10 , batch:  524 , train loss:  17.343448638916016\n","epoch:  10 , batch:  525 , train loss:  24.707847595214844\n","epoch:  10 , batch:  526 , train loss:  24.39488410949707\n","epoch:  10 , batch:  527 , train loss:  24.944730758666992\n","epoch:  10 , batch:  528 , train loss:  25.999717712402344\n","epoch:  10 , batch:  529 , train loss:  23.906984329223633\n","epoch:  10 , batch:  530 , train loss:  17.770212173461914\n","epoch:  10 , batch:  531 , train loss:  21.002418518066406\n","epoch:  10 , batch:  532 , train loss:  15.884172439575195\n","epoch:  10 , batch:  533 , train loss:  7.595963954925537\n","epoch:  10 , batch:  534 , train loss:  15.884008407592773\n","epoch:  10 , batch:  535 , train loss:  19.714067459106445\n","epoch:  10 , batch:  536 , train loss:  15.568626403808594\n","epoch:  10 , batch:  537 , train loss:  17.878520965576172\n","epoch:  10 , batch:  538 , train loss:  10.238907814025879\n","epoch:  10 , batch:  539 , train loss:  10.587621688842773\n","epoch:  10 , batch:  540 , train loss:  10.47608470916748\n","epoch:  10 , batch:  541 , train loss:  20.551206588745117\n","epoch:  10 , batch:  542 , train loss:  9.925124168395996\n","epoch:  10 , batch:  543 , train loss:  8.047464370727539\n","epoch:  10 , batch:  544 , train loss:  15.368547439575195\n","epoch:  10 , batch:  545 , train loss:  16.790584564208984\n","epoch:  10 , batch:  546 , train loss:  19.517927169799805\n","epoch:  10 , batch:  547 , train loss:  21.7689266204834\n","epoch:  10 , batch:  548 , train loss:  15.784939765930176\n","epoch:  10 , batch:  549 , train loss:  12.640410423278809\n","epoch:  10 , batch:  550 , train loss:  15.746222496032715\n","epoch:  10 , batch:  551 , train loss:  14.240675926208496\n","epoch:  10 , batch:  552 , train loss:  9.592790603637695\n","epoch:  10 , batch:  553 , train loss:  9.43606948852539\n","epoch:  10 , batch:  554 , train loss:  10.813982009887695\n","epoch:  10 , batch:  555 , train loss:  11.511329650878906\n","epoch:  10 , batch:  556 , train loss:  10.424479484558105\n","epoch:  10 , batch:  557 , train loss:  17.886091232299805\n","epoch:  10 , batch:  558 , train loss:  26.183853149414062\n","epoch:  10 , batch:  559 , train loss:  22.904027938842773\n","epoch:  10 , batch:  560 , train loss:  6.4894185066223145\n","epoch:  10 , batch:  561 , train loss:  22.26188850402832\n","epoch:  10 , batch:  562 , train loss:  14.260747909545898\n","epoch:  10 , batch:  563 , train loss:  10.81197738647461\n","epoch:  10 , batch:  564 , train loss:  16.647994995117188\n","epoch:  10 , batch:  565 , train loss:  16.00710105895996\n","epoch:  10 , batch:  566 , train loss:  19.05109977722168\n","epoch:  10 , batch:  567 , train loss:  18.429195404052734\n","epoch:  10 , batch:  568 , train loss:  7.800506591796875\n","epoch:  10 , batch:  569 , train loss:  16.651472091674805\n","epoch:  10 , batch:  570 , train loss:  10.573452949523926\n","epoch:  10 , batch:  571 , train loss:  15.625280380249023\n","epoch:  10 , batch:  572 , train loss:  9.764683723449707\n","epoch:  10 , batch:  573 , train loss:  11.562043190002441\n","epoch:  10 , batch:  574 , train loss:  8.336783409118652\n","epoch:  10 , batch:  575 , train loss:  28.068748474121094\n","epoch:  10 , batch:  576 , train loss:  14.843890190124512\n","epoch:  10 , batch:  577 , train loss:  15.961052894592285\n","epoch:  10 , batch:  578 , train loss:  19.019817352294922\n","epoch:  10 , batch:  579 , train loss:  12.273752212524414\n","epoch:  10 , batch:  580 , train loss:  19.7161865234375\n","epoch:  10 , batch:  581 , train loss:  13.123632431030273\n","epoch:  10 , batch:  582 , train loss:  18.881988525390625\n","epoch:  10 , batch:  583 , train loss:  5.905972480773926\n","epoch:  10 , batch:  584 , train loss:  24.40576171875\n","epoch:  10 , batch:  585 , train loss:  14.347270965576172\n","epoch:  10 , batch:  586 , train loss:  23.406015396118164\n","epoch:  10 , batch:  587 , train loss:  28.705974578857422\n","epoch:  10 , batch:  588 , train loss:  15.790779113769531\n","epoch:  10 , batch:  589 , train loss:  16.876901626586914\n","epoch:  10 , batch:  590 , train loss:  24.32447052001953\n","epoch:  10 , batch:  591 , train loss:  16.771202087402344\n","epoch:  10 , batch:  592 , train loss:  7.536606311798096\n","epoch:  10 , batch:  593 , train loss:  9.644763946533203\n","epoch:  10 , batch:  594 , train loss:  15.948070526123047\n","epoch:  10 , batch:  595 , train loss:  12.614579200744629\n","epoch:  10 , batch:  596 , train loss:  20.596050262451172\n","epoch:  10 , batch:  597 , train loss:  18.887409210205078\n","epoch:  10 , batch:  598 , train loss:  12.012707710266113\n","epoch:  10 , batch:  599 , train loss:  13.67055606842041\n","epoch:  10 , batch:  600 , train loss:  10.650044441223145\n","epoch:  10 , batch:  601 , train loss:  11.200916290283203\n","epoch:  10 , batch:  602 , train loss:  8.608429908752441\n","epoch:  10 , batch:  603 , train loss:  18.29058837890625\n","epoch:  10 , batch:  604 , train loss:  11.49481201171875\n","epoch:  10 , batch:  605 , train loss:  8.108918190002441\n","epoch:  10 , batch:  606 , train loss:  11.137811660766602\n","epoch:  10 , batch:  607 , train loss:  13.811514854431152\n","epoch:  10 , batch:  608 , train loss:  14.684828758239746\n","epoch:  10 , batch:  609 , train loss:  11.46492862701416\n","epoch:  10 , batch:  610 , train loss:  15.334453582763672\n","epoch:  10 , batch:  611 , train loss:  14.529003143310547\n","epoch:  10 , batch:  612 , train loss:  12.111770629882812\n","epoch:  10 , batch:  613 , train loss:  14.873469352722168\n","epoch:  10 , batch:  614 , train loss:  32.11697769165039\n","epoch:  10 , batch:  615 , train loss:  24.987945556640625\n","epoch:  10 , batch:  616 , train loss:  13.757757186889648\n","epoch:  10 , batch:  617 , train loss:  11.044541358947754\n","epoch:  10 , batch:  618 , train loss:  11.090723991394043\n","epoch:  10 , batch:  619 , train loss:  14.260774612426758\n","epoch:  10 , batch:  620 , train loss:  7.928330898284912\n","epoch:  10 , batch:  621 , train loss:  14.829830169677734\n","epoch:  10 , batch:  622 , train loss:  18.643672943115234\n","epoch:  10 , batch:  623 , train loss:  27.21383285522461\n","epoch:  10 , batch:  624 , train loss:  9.126888275146484\n","epoch:  10 , batch:  625 , train loss:  20.084270477294922\n","epoch:  10 , batch:  626 , train loss:  12.083911895751953\n","epoch:  10 , batch:  627 , train loss:  24.856950759887695\n","epoch:  10 , batch:  628 , train loss:  14.509317398071289\n","epoch:  10 , batch:  629 , train loss:  15.172629356384277\n","epoch:  10 , batch:  630 , train loss:  10.115216255187988\n","epoch:  10 , batch:  631 , train loss:  20.929088592529297\n","epoch:  10 , batch:  632 , train loss:  11.607934951782227\n","epoch:  10 , batch:  633 , train loss:  12.666586875915527\n","epoch:  10 , batch:  634 , train loss:  28.51005744934082\n","epoch:  10 , batch:  635 , train loss:  17.828458786010742\n","epoch:  10 , batch:  636 , train loss:  15.188036918640137\n","epoch:  10 , batch:  637 , train loss:  17.42774772644043\n","epoch:  10 , batch:  638 , train loss:  16.043588638305664\n","epoch:  10 , batch:  639 , train loss:  15.492986679077148\n","epoch:  10 , batch:  640 , train loss:  15.159351348876953\n","epoch:  10 , batch:  641 , train loss:  27.32846450805664\n","epoch:  10 , batch:  642 , train loss:  26.129940032958984\n","epoch:  10 , batch:  643 , train loss:  18.01905059814453\n","epoch:  10 , batch:  644 , train loss:  23.90142250061035\n","epoch:  10 , batch:  645 , train loss:  24.027372360229492\n","epoch:  10 , batch:  646 , train loss:  15.464475631713867\n","epoch:  10 , batch:  647 , train loss:  19.702674865722656\n","epoch:  10 , batch:  648 , train loss:  8.854519844055176\n","epoch:  10 , batch:  649 , train loss:  15.051148414611816\n","epoch:  10 , batch:  650 , train loss:  7.634372234344482\n","epoch:  10 , batch:  651 , train loss:  14.682650566101074\n","epoch:  10 , batch:  652 , train loss:  13.004426002502441\n","epoch:  10 , batch:  653 , train loss:  11.578391075134277\n","epoch:  10 , batch:  654 , train loss:  13.879403114318848\n","epoch:  10 , batch:  655 , train loss:  15.344348907470703\n","epoch:  10 , batch:  656 , train loss:  12.792181015014648\n","epoch:  10 , batch:  657 , train loss:  9.971048355102539\n","epoch:  10 , batch:  658 , train loss:  16.08719253540039\n","epoch:  10 , batch:  659 , train loss:  8.574079513549805\n","epoch:  10 , batch:  660 , train loss:  15.482091903686523\n","epoch:  10 , batch:  661 , train loss:  12.358987808227539\n","epoch:  10 , batch:  662 , train loss:  10.640507698059082\n","epoch:  10 , batch:  663 , train loss:  17.023073196411133\n","epoch:  10 , batch:  664 , train loss:  14.36359977722168\n","epoch:  10 , batch:  665 , train loss:  16.025875091552734\n","epoch:  10 , batch:  666 , train loss:  26.101497650146484\n","epoch:  10 , batch:  667 , train loss:  27.001903533935547\n","epoch:  10 , batch:  668 , train loss:  9.748602867126465\n","epoch:  10 , batch:  669 , train loss:  14.666268348693848\n","epoch:  10 , batch:  670 , train loss:  14.574962615966797\n","epoch:  10 , batch:  671 , train loss:  22.70225715637207\n","epoch:  10 , batch:  672 , train loss:  15.1867094039917\n","epoch:  10 , batch:  673 , train loss:  17.146867752075195\n","epoch:  10 , batch:  674 , train loss:  10.215328216552734\n","epoch:  10 , batch:  675 , train loss:  5.707635402679443\n","epoch:  10 , batch:  676 , train loss:  26.95540428161621\n","epoch:  10 , batch:  677 , train loss:  11.632511138916016\n","epoch:  10 , batch:  678 , train loss:  16.04947853088379\n","epoch:  10 , batch:  679 , train loss:  14.488929748535156\n","epoch:  10 , batch:  680 , train loss:  14.748668670654297\n","epoch:  10 , batch:  681 , train loss:  6.837162017822266\n","epoch:  10 , batch:  682 , train loss:  18.276844024658203\n","epoch:  10 , batch:  683 , train loss:  21.24360466003418\n","epoch:  10 , batch:  684 , train loss:  21.04676628112793\n","epoch:  10 , batch:  685 , train loss:  19.525636672973633\n","epoch:  10 , batch:  686 , train loss:  12.22304630279541\n","epoch:  10 , batch:  687 , train loss:  12.796083450317383\n","epoch:  10 , batch:  688 , train loss:  16.40238380432129\n","epoch:  10 , batch:  689 , train loss:  14.282081604003906\n","epoch:  10 , batch:  690 , train loss:  19.050792694091797\n","epoch:  10 , batch:  691 , train loss:  23.886764526367188\n","epoch:  10 , batch:  692 , train loss:  19.807161331176758\n","epoch:  10 , batch:  693 , train loss:  11.508099555969238\n","epoch:  10 , batch:  694 , train loss:  13.089317321777344\n","epoch:  10 , batch:  695 , train loss:  15.620048522949219\n","epoch:  10 , batch:  696 , train loss:  14.385116577148438\n","epoch:  10 , batch:  697 , train loss:  17.55658721923828\n","epoch:  10 , batch:  698 , train loss:  23.030872344970703\n","epoch:  10 , batch:  699 , train loss:  12.21322250366211\n","epoch:  10 , batch:  700 , train loss:  9.539834976196289\n","epoch:  10 , batch:  701 , train loss:  19.000442504882812\n","epoch:  10 , batch:  702 , train loss:  13.223018646240234\n","epoch:  10 , batch:  703 , train loss:  19.077787399291992\n","epoch:  10 , batch:  704 , train loss:  24.037572860717773\n","epoch:  10 , batch:  705 , train loss:  16.416301727294922\n","epoch:  10 , batch:  706 , train loss:  12.16604232788086\n","epoch:  10 , batch:  707 , train loss:  14.480762481689453\n","epoch:  10 , batch:  708 , train loss:  17.307323455810547\n","epoch:  10 , batch:  709 , train loss:  18.705232620239258\n","epoch:  10 , batch:  710 , train loss:  18.462923049926758\n","epoch:  10 , batch:  711 , train loss:  15.090310096740723\n","epoch:  10 , batch:  712 , train loss:  14.594708442687988\n","epoch:  10 , batch:  713 , train loss:  16.320743560791016\n","epoch:  10 , batch:  714 , train loss:  14.085067749023438\n","epoch:  10 , batch:  715 , train loss:  30.805877685546875\n","epoch:  10 , batch:  716 , train loss:  14.284090042114258\n","epoch:  10 , batch:  717 , train loss:  19.2545223236084\n","epoch:  10 , batch:  718 , train loss:  9.431840896606445\n","epoch:  10 , batch:  719 , train loss:  18.418869018554688\n","epoch:  10 , batch:  720 , train loss:  11.106797218322754\n","epoch:  10 , batch:  721 , train loss:  9.469915390014648\n","epoch:  10 , batch:  722 , train loss:  18.433069229125977\n","epoch:  10 , batch:  723 , train loss:  18.017139434814453\n","epoch:  10 , batch:  724 , train loss:  10.049938201904297\n","epoch:  10 , batch:  725 , train loss:  11.680017471313477\n","epoch:  10 , batch:  726 , train loss:  14.893120765686035\n","epoch:  10 , batch:  727 , train loss:  18.18075180053711\n","epoch:  10 , batch:  728 , train loss:  28.76219940185547\n","epoch:  10 , batch:  729 , train loss:  27.169292449951172\n","epoch:  10 , batch:  730 , train loss:  8.251450538635254\n","epoch:  10 , batch:  731 , train loss:  8.905426979064941\n","epoch:  10 , batch:  732 , train loss:  11.75455093383789\n","epoch:  10 , batch:  733 , train loss:  28.627199172973633\n","epoch:  10 , batch:  734 , train loss:  19.89171028137207\n","epoch:  10 , batch:  735 , train loss:  13.499062538146973\n","epoch:  10 , batch:  736 , train loss:  25.22411346435547\n","epoch:  10 , batch:  737 , train loss:  15.39426326751709\n","epoch:  10 , batch:  738 , train loss:  12.517720222473145\n","epoch:  10 , batch:  739 , train loss:  7.095244407653809\n","epoch:  10 , batch:  740 , train loss:  23.02739143371582\n","epoch:  10 , batch:  741 , train loss:  7.977602005004883\n","epoch:  10 , batch:  742 , train loss:  22.929561614990234\n","epoch:  10 , batch:  743 , train loss:  14.466134071350098\n","epoch:  10 , batch:  744 , train loss:  22.826963424682617\n","epoch:  10 , batch:  745 , train loss:  11.213582992553711\n","epoch:  10 , batch:  746 , train loss:  26.514019012451172\n","epoch:  10 , batch:  747 , train loss:  25.52582359313965\n","epoch:  10 , batch:  748 , train loss:  27.642240524291992\n","epoch:  10 , batch:  749 , train loss:  12.685256004333496\n","epoch:  10 , batch:  750 , train loss:  11.531625747680664\n","epoch:  10 , batch:  751 , train loss:  23.795183181762695\n","epoch:  10 , batch:  752 , train loss:  21.950429916381836\n","epoch:  10 , batch:  753 , train loss:  22.3723087310791\n","epoch:  10 , batch:  754 , train loss:  20.79374122619629\n","epoch:  10 , batch:  755 , train loss:  18.460371017456055\n","epoch:  10 , batch:  756 , train loss:  12.985773086547852\n","epoch:  10 , batch:  757 , train loss:  17.358936309814453\n","epoch:  10 , batch:  758 , train loss:  12.71169662475586\n","epoch:  10 , batch:  759 , train loss:  15.181535720825195\n","epoch:  10 , batch:  760 , train loss:  13.914351463317871\n","epoch:  10 , batch:  761 , train loss:  13.993958473205566\n","epoch:  10 , batch:  762 , train loss:  12.18838882446289\n","epoch:  10 , batch:  763 , train loss:  8.051107406616211\n","epoch:  10 , batch:  764 , train loss:  9.564566612243652\n","epoch:  10 , batch:  765 , train loss:  11.982584953308105\n","epoch:  10 , batch:  766 , train loss:  10.283390998840332\n","epoch:  10 , batch:  767 , train loss:  20.808616638183594\n","epoch:  10 , batch:  768 , train loss:  23.950546264648438\n","epoch:  10 , batch:  769 , train loss:  20.590784072875977\n","epoch:  10 , batch:  770 , train loss:  17.566669464111328\n","epoch:  10 , batch:  771 , train loss:  7.828948020935059\n","epoch:  10 , batch:  772 , train loss:  15.694311141967773\n","epoch:  10 , batch:  773 , train loss:  15.951032638549805\n","epoch:  10 , batch:  774 , train loss:  9.469043731689453\n","epoch:  10 , batch:  775 , train loss:  13.349346160888672\n","epoch:  10 , batch:  776 , train loss:  18.5667667388916\n","epoch:  10 , batch:  777 , train loss:  10.813996315002441\n","epoch:  10 , batch:  778 , train loss:  11.295671463012695\n","epoch:  10 , batch:  779 , train loss:  7.786027908325195\n","epoch:  10 , batch:  780 , train loss:  8.75357723236084\n","epoch:  10 , batch:  781 , train loss:  21.34693145751953\n","epoch:  10 , batch:  782 , train loss:  20.277542114257812\n","epoch:  10 , batch:  783 , train loss:  13.06322193145752\n","epoch:  10 , batch:  784 , train loss:  19.277048110961914\n","epoch:  10 , batch:  785 , train loss:  15.599855422973633\n","epoch:  10 , batch:  786 , train loss:  15.5202054977417\n","epoch:  10 , batch:  787 , train loss:  15.602618217468262\n","epoch:  10 , batch:  788 , train loss:  15.057403564453125\n","epoch:  10 , batch:  789 , train loss:  23.645082473754883\n","epoch:  10 , batch:  790 , train loss:  16.668319702148438\n","epoch:  10 , batch:  791 , train loss:  20.323705673217773\n","epoch:  10 , batch:  792 , train loss:  9.288681030273438\n","epoch:  10 , batch:  793 , train loss:  14.114771842956543\n","epoch:  10 , batch:  794 , train loss:  9.51648235321045\n","epoch:  10 , batch:  795 , train loss:  15.740020751953125\n","epoch:  10 , batch:  796 , train loss:  17.939523696899414\n","epoch:  10 , batch:  797 , train loss:  20.928665161132812\n","epoch:  10 , batch:  798 , train loss:  22.302061080932617\n","epoch:  10 , batch:  799 , train loss:  22.30863380432129\n","epoch:  10 , batch:  800 , train loss:  15.621424674987793\n","epoch:  10 , batch:  801 , train loss:  15.707504272460938\n","epoch:  10 , batch:  802 , train loss:  14.639453887939453\n","epoch:  10 , batch:  803 , train loss:  13.976767539978027\n","epoch:  10 , batch:  804 , train loss:  12.755966186523438\n","epoch:  10 , batch:  805 , train loss:  16.380863189697266\n","epoch:  10 , batch:  806 , train loss:  15.252074241638184\n","epoch:  10 , batch:  807 , train loss:  12.928916931152344\n","epoch:  10 , batch:  808 , train loss:  11.997381210327148\n","epoch:  10 , batch:  809 , train loss:  6.583966255187988\n","epoch:  10 , batch:  810 , train loss:  25.571224212646484\n","epoch:  10 , batch:  811 , train loss:  16.826387405395508\n","epoch:  10 , batch:  812 , train loss:  17.501449584960938\n","epoch:  10 , batch:  813 , train loss:  17.71338653564453\n","epoch:  10 , batch:  814 , train loss:  20.358552932739258\n","epoch:  10 , batch:  815 , train loss:  18.971426010131836\n","epoch:  10 , batch:  816 , train loss:  33.63077163696289\n","epoch:  10 , batch:  817 , train loss:  18.481069564819336\n","epoch:  10 , batch:  818 , train loss:  10.750341415405273\n","epoch:  10 , batch:  819 , train loss:  12.581317901611328\n","epoch:  10 , batch:  820 , train loss:  20.829801559448242\n","epoch:  10 , batch:  821 , train loss:  11.147391319274902\n","epoch:  10 , batch:  822 , train loss:  11.902661323547363\n","epoch:  10 , batch:  823 , train loss:  12.564848899841309\n","epoch:  10 , batch:  824 , train loss:  12.159344673156738\n","epoch:  10 , batch:  825 , train loss:  10.88352108001709\n","epoch:  10 , batch:  826 , train loss:  21.67981719970703\n","epoch:  10 , batch:  827 , train loss:  20.59299659729004\n","epoch:  10 , batch:  828 , train loss:  11.2308988571167\n","epoch:  10 , batch:  829 , train loss:  16.680269241333008\n","epoch:  10 , batch:  830 , train loss:  12.043806076049805\n","epoch:  10 , batch:  831 , train loss:  12.816274642944336\n","epoch:  10 , batch:  832 , train loss:  21.5935001373291\n","epoch:  10 , batch:  833 , train loss:  9.81868839263916\n","epoch:  10 , batch:  834 , train loss:  15.395952224731445\n","epoch:  10 , batch:  835 , train loss:  19.862171173095703\n","epoch:  10 , batch:  836 , train loss:  13.080018043518066\n","epoch:  10 , batch:  837 , train loss:  11.96190071105957\n","epoch:  10 , batch:  838 , train loss:  13.607760429382324\n","epoch:  10 , batch:  839 , train loss:  22.280588150024414\n","epoch:  10 , batch:  840 , train loss:  15.155332565307617\n","epoch:  10 , batch:  841 , train loss:  21.1376895904541\n","epoch:  10 , batch:  842 , train loss:  9.91474723815918\n","epoch:  10 , batch:  843 , train loss:  24.151412963867188\n","epoch:  10 , batch:  844 , train loss:  19.368173599243164\n","epoch:  10 , batch:  845 , train loss:  22.28337860107422\n","epoch:  10 , batch:  846 , train loss:  9.006826400756836\n","epoch:  10 , batch:  847 , train loss:  15.482718467712402\n","epoch:  10 , batch:  848 , train loss:  18.916423797607422\n","epoch:  10 , batch:  849 , train loss:  9.478713989257812\n","epoch:  10 , batch:  850 , train loss:  14.082770347595215\n","epoch:  10 , batch:  851 , train loss:  21.125200271606445\n","epoch:  10 , batch:  852 , train loss:  14.50696849822998\n","epoch:  10 , batch:  853 , train loss:  24.554637908935547\n","epoch:  10 , batch:  854 , train loss:  15.213153839111328\n","epoch:  10 , batch:  855 , train loss:  15.560903549194336\n","epoch:  10 , batch:  856 , train loss:  14.08319091796875\n","epoch:  10 , batch:  857 , train loss:  16.045148849487305\n","epoch:  10 , batch:  858 , train loss:  26.417524337768555\n","epoch:  10 , batch:  859 , train loss:  15.929952621459961\n","epoch:  10 , batch:  860 , train loss:  21.566349029541016\n","epoch:  10 , batch:  861 , train loss:  17.421342849731445\n","epoch:  10 , batch:  862 , train loss:  12.789697647094727\n","epoch:  10 , batch:  863 , train loss:  26.23738670349121\n","epoch:  10 , batch:  864 , train loss:  17.50063133239746\n","epoch:  10 , batch:  865 , train loss:  12.456870079040527\n","epoch:  10 , batch:  866 , train loss:  7.98621129989624\n","epoch:  10 , batch:  867 , train loss:  18.109758377075195\n","epoch:  10 , batch:  868 , train loss:  11.519368171691895\n","epoch:  10 , batch:  869 , train loss:  24.32745361328125\n","epoch:  10 , batch:  870 , train loss:  15.501130104064941\n","epoch:  10 , batch:  871 , train loss:  21.139062881469727\n","epoch:  10 , batch:  872 , train loss:  14.590808868408203\n","epoch:  10 , batch:  873 , train loss:  11.913110733032227\n","epoch:  10 , batch:  874 , train loss:  14.332647323608398\n","epoch:  10 , batch:  875 , train loss:  18.289167404174805\n","epoch:  10 , batch:  876 , train loss:  20.495813369750977\n","epoch:  10 , batch:  877 , train loss:  17.889379501342773\n","epoch:  10 , batch:  878 , train loss:  24.159881591796875\n","epoch:  10 , batch:  879 , train loss:  13.12717342376709\n","epoch:  10 , batch:  880 , train loss:  13.058006286621094\n","epoch:  10 , batch:  881 , train loss:  12.876368522644043\n","epoch:  10 , batch:  882 , train loss:  9.15279769897461\n","epoch:  10 , batch:  883 , train loss:  22.364370346069336\n","epoch:  10 , batch:  884 , train loss:  22.072315216064453\n","epoch:  10 , batch:  885 , train loss:  23.409658432006836\n","epoch:  10 , batch:  886 , train loss:  14.125616073608398\n","epoch:  10 , batch:  887 , train loss:  23.34714698791504\n","epoch:  10 , batch:  888 , train loss:  15.696455955505371\n","epoch:  10 , batch:  889 , train loss:  11.87895679473877\n","epoch:  10 , batch:  890 , train loss:  21.478628158569336\n","epoch:  10 , batch:  891 , train loss:  12.697218894958496\n","epoch:  10 , batch:  892 , train loss:  32.827510833740234\n","epoch:  10 , batch:  893 , train loss:  16.788375854492188\n","epoch:  10 , batch:  894 , train loss:  15.422250747680664\n","epoch:  10 , batch:  895 , train loss:  11.37716293334961\n","epoch:  10 , batch:  896 , train loss:  18.258460998535156\n","epoch:  10 , batch:  897 , train loss:  22.951251983642578\n","epoch:  10 , batch:  898 , train loss:  11.500267028808594\n","epoch:  10 , batch:  899 , train loss:  22.57473373413086\n","epoch:  10 , batch:  900 , train loss:  18.402128219604492\n","epoch:  10 , batch:  901 , train loss:  12.68118953704834\n","epoch:  10 , batch:  902 , train loss:  7.632264614105225\n","epoch:  10 , batch:  903 , train loss:  20.45796012878418\n","epoch:  10 , batch:  904 , train loss:  14.060983657836914\n","epoch:  10 , batch:  905 , train loss:  13.664629936218262\n","epoch:  10 , batch:  906 , train loss:  11.740559577941895\n","epoch:  10 , batch:  907 , train loss:  20.794607162475586\n","epoch:  10 , batch:  908 , train loss:  12.667645454406738\n","epoch:  10 , batch:  909 , train loss:  19.276674270629883\n","epoch:  10 , batch:  910 , train loss:  15.249731063842773\n","epoch:  10 , batch:  911 , train loss:  14.836553573608398\n","epoch:  10 , batch:  912 , train loss:  17.99985694885254\n","epoch:  10 , batch:  913 , train loss:  24.738401412963867\n","epoch:  10 , batch:  914 , train loss:  20.816638946533203\n","epoch:  10 , batch:  915 , train loss:  13.351761817932129\n","epoch:  10 , batch:  916 , train loss:  13.93343734741211\n","epoch:  10 , batch:  917 , train loss:  13.48250961303711\n","epoch:  10 , batch:  918 , train loss:  27.806257247924805\n","epoch:  10 , batch:  919 , train loss:  13.512774467468262\n","epoch:  10 , batch:  920 , train loss:  9.153936386108398\n","epoch:  10 , batch:  921 , train loss:  12.823535919189453\n","epoch:  10 , batch:  922 , train loss:  17.406522750854492\n","epoch:  10 , batch:  923 , train loss:  15.597498893737793\n","epoch:  10 , batch:  924 , train loss:  21.502031326293945\n","epoch:  10 , batch:  925 , train loss:  15.959481239318848\n","epoch:  10 , batch:  926 , train loss:  23.821975708007812\n","epoch:  10 , batch:  927 , train loss:  15.13962173461914\n","epoch:  10 , batch:  928 , train loss:  11.414393424987793\n","epoch:  10 , batch:  929 , train loss:  15.17322063446045\n","epoch:  10 , batch:  930 , train loss:  8.329780578613281\n","epoch:  10 , batch:  931 , train loss:  19.495573043823242\n","epoch:  10 , batch:  932 , train loss:  15.214530944824219\n","epoch:  10 , batch:  933 , train loss:  19.364768981933594\n","epoch:  10 , batch:  934 , train loss:  14.629342079162598\n","epoch:  10 , batch:  935 , train loss:  19.79330062866211\n","epoch:  10 , batch:  936 , train loss:  12.750099182128906\n","epoch:  10 , batch:  937 , train loss:  4.041323661804199\n","Accuracy of train set: 0.90315\n","epoch:  10 , batch:  0 , test loss:  17.186176300048828\n","epoch:  10 , batch:  1 , test loss:  20.157245635986328\n","epoch:  10 , batch:  2 , test loss:  12.37120246887207\n","epoch:  10 , batch:  3 , test loss:  13.476903915405273\n","epoch:  10 , batch:  4 , test loss:  33.61591339111328\n","epoch:  10 , batch:  5 , test loss:  48.16181945800781\n","epoch:  10 , batch:  6 , test loss:  23.430469512939453\n","epoch:  10 , batch:  7 , test loss:  18.552274703979492\n","epoch:  10 , batch:  8 , test loss:  19.46077537536621\n","epoch:  10 , batch:  9 , test loss:  24.327381134033203\n","epoch:  10 , batch:  10 , test loss:  45.04936599731445\n","epoch:  10 , batch:  11 , test loss:  19.245044708251953\n","epoch:  10 , batch:  12 , test loss:  17.08451271057129\n","epoch:  10 , batch:  13 , test loss:  41.012386322021484\n","epoch:  10 , batch:  14 , test loss:  28.610265731811523\n","epoch:  10 , batch:  15 , test loss:  16.81233787536621\n","epoch:  10 , batch:  16 , test loss:  24.84920310974121\n","epoch:  10 , batch:  17 , test loss:  13.622681617736816\n","epoch:  10 , batch:  18 , test loss:  22.651020050048828\n","epoch:  10 , batch:  19 , test loss:  19.202117919921875\n","epoch:  10 , batch:  20 , test loss:  17.47592544555664\n","epoch:  10 , batch:  21 , test loss:  16.97610092163086\n","epoch:  10 , batch:  22 , test loss:  10.122074127197266\n","epoch:  10 , batch:  23 , test loss:  18.01880645751953\n","epoch:  10 , batch:  24 , test loss:  19.73722267150879\n","epoch:  10 , batch:  25 , test loss:  13.58597469329834\n","epoch:  10 , batch:  26 , test loss:  40.891544342041016\n","epoch:  10 , batch:  27 , test loss:  32.620975494384766\n","epoch:  10 , batch:  28 , test loss:  19.067331314086914\n","epoch:  10 , batch:  29 , test loss:  33.359832763671875\n","epoch:  10 , batch:  30 , test loss:  23.332801818847656\n","epoch:  10 , batch:  31 , test loss:  26.787912368774414\n","epoch:  10 , batch:  32 , test loss:  23.239633560180664\n","epoch:  10 , batch:  33 , test loss:  23.650806427001953\n","epoch:  10 , batch:  34 , test loss:  18.035715103149414\n","epoch:  10 , batch:  35 , test loss:  21.530603408813477\n","epoch:  10 , batch:  36 , test loss:  19.505210876464844\n","epoch:  10 , batch:  37 , test loss:  29.194374084472656\n","epoch:  10 , batch:  38 , test loss:  15.866134643554688\n","epoch:  10 , batch:  39 , test loss:  9.973408699035645\n","epoch:  10 , batch:  40 , test loss:  17.84090232849121\n","epoch:  10 , batch:  41 , test loss:  15.408339500427246\n","epoch:  10 , batch:  42 , test loss:  15.300076484680176\n","epoch:  10 , batch:  43 , test loss:  25.017107009887695\n","epoch:  10 , batch:  44 , test loss:  18.985153198242188\n","epoch:  10 , batch:  45 , test loss:  17.265369415283203\n","epoch:  10 , batch:  46 , test loss:  24.490928649902344\n","epoch:  10 , batch:  47 , test loss:  27.398818969726562\n","epoch:  10 , batch:  48 , test loss:  15.370129585266113\n","epoch:  10 , batch:  49 , test loss:  18.178287506103516\n","epoch:  10 , batch:  50 , test loss:  22.82716178894043\n","epoch:  10 , batch:  51 , test loss:  9.800253868103027\n","epoch:  10 , batch:  52 , test loss:  30.17083168029785\n","epoch:  10 , batch:  53 , test loss:  16.587923049926758\n","epoch:  10 , batch:  54 , test loss:  26.1312198638916\n","epoch:  10 , batch:  55 , test loss:  32.81824493408203\n","epoch:  10 , batch:  56 , test loss:  16.518415451049805\n","epoch:  10 , batch:  57 , test loss:  16.229108810424805\n","epoch:  10 , batch:  58 , test loss:  20.385892868041992\n","epoch:  10 , batch:  59 , test loss:  23.991025924682617\n","epoch:  10 , batch:  60 , test loss:  20.79094886779785\n","epoch:  10 , batch:  61 , test loss:  24.686487197875977\n","epoch:  10 , batch:  62 , test loss:  20.060867309570312\n","epoch:  10 , batch:  63 , test loss:  15.351444244384766\n","epoch:  10 , batch:  64 , test loss:  25.775583267211914\n","epoch:  10 , batch:  65 , test loss:  16.539321899414062\n","epoch:  10 , batch:  66 , test loss:  15.029844284057617\n","epoch:  10 , batch:  67 , test loss:  21.903013229370117\n","epoch:  10 , batch:  68 , test loss:  18.951833724975586\n","epoch:  10 , batch:  69 , test loss:  31.13182830810547\n","epoch:  10 , batch:  70 , test loss:  19.667030334472656\n","epoch:  10 , batch:  71 , test loss:  29.674068450927734\n","epoch:  10 , batch:  72 , test loss:  23.79884910583496\n","epoch:  10 , batch:  73 , test loss:  20.102645874023438\n","epoch:  10 , batch:  74 , test loss:  21.180803298950195\n","epoch:  10 , batch:  75 , test loss:  28.781343460083008\n","epoch:  10 , batch:  76 , test loss:  12.150375366210938\n","epoch:  10 , batch:  77 , test loss:  25.03188705444336\n","epoch:  10 , batch:  78 , test loss:  13.660690307617188\n","epoch:  10 , batch:  79 , test loss:  30.21800994873047\n","epoch:  10 , batch:  80 , test loss:  24.945629119873047\n","epoch:  10 , batch:  81 , test loss:  9.267155647277832\n","epoch:  10 , batch:  82 , test loss:  24.36661720275879\n","epoch:  10 , batch:  83 , test loss:  12.287378311157227\n","epoch:  10 , batch:  84 , test loss:  39.05861282348633\n","epoch:  10 , batch:  85 , test loss:  20.10076332092285\n","epoch:  10 , batch:  86 , test loss:  23.708433151245117\n","epoch:  10 , batch:  87 , test loss:  27.239957809448242\n","epoch:  10 , batch:  88 , test loss:  21.17562484741211\n","epoch:  10 , batch:  89 , test loss:  16.591764450073242\n","epoch:  10 , batch:  90 , test loss:  18.0876522064209\n","epoch:  10 , batch:  91 , test loss:  26.502683639526367\n","epoch:  10 , batch:  92 , test loss:  20.73107147216797\n","epoch:  10 , batch:  93 , test loss:  28.948244094848633\n","epoch:  10 , batch:  94 , test loss:  15.171377182006836\n","epoch:  10 , batch:  95 , test loss:  24.632051467895508\n","epoch:  10 , batch:  96 , test loss:  12.071501731872559\n","epoch:  10 , batch:  97 , test loss:  19.654634475708008\n","epoch:  10 , batch:  98 , test loss:  39.27277374267578\n","epoch:  10 , batch:  99 , test loss:  21.67288589477539\n","epoch:  10 , batch:  100 , test loss:  23.825157165527344\n","epoch:  10 , batch:  101 , test loss:  16.56691551208496\n","epoch:  10 , batch:  102 , test loss:  26.669082641601562\n","epoch:  10 , batch:  103 , test loss:  26.57784080505371\n","epoch:  10 , batch:  104 , test loss:  22.443836212158203\n","epoch:  10 , batch:  105 , test loss:  41.68531036376953\n","epoch:  10 , batch:  106 , test loss:  23.878629684448242\n","epoch:  10 , batch:  107 , test loss:  31.75977897644043\n","epoch:  10 , batch:  108 , test loss:  13.95956802368164\n","epoch:  10 , batch:  109 , test loss:  22.457000732421875\n","epoch:  10 , batch:  110 , test loss:  25.25707244873047\n","epoch:  10 , batch:  111 , test loss:  30.359533309936523\n","epoch:  10 , batch:  112 , test loss:  23.47327995300293\n","epoch:  10 , batch:  113 , test loss:  12.997452735900879\n","epoch:  10 , batch:  114 , test loss:  13.181172370910645\n","epoch:  10 , batch:  115 , test loss:  17.224956512451172\n","epoch:  10 , batch:  116 , test loss:  14.733654975891113\n","epoch:  10 , batch:  117 , test loss:  14.867332458496094\n","epoch:  10 , batch:  118 , test loss:  48.38383865356445\n","epoch:  10 , batch:  119 , test loss:  17.165504455566406\n","epoch:  10 , batch:  120 , test loss:  28.63821029663086\n","epoch:  10 , batch:  121 , test loss:  14.884645462036133\n","epoch:  10 , batch:  122 , test loss:  26.68630599975586\n","epoch:  10 , batch:  123 , test loss:  27.588747024536133\n","epoch:  10 , batch:  124 , test loss:  23.254697799682617\n","epoch:  10 , batch:  125 , test loss:  12.360628128051758\n","epoch:  10 , batch:  126 , test loss:  18.689002990722656\n","epoch:  10 , batch:  127 , test loss:  15.485332489013672\n","epoch:  10 , batch:  128 , test loss:  22.26363182067871\n","epoch:  10 , batch:  129 , test loss:  11.091436386108398\n","epoch:  10 , batch:  130 , test loss:  11.437093734741211\n","epoch:  10 , batch:  131 , test loss:  24.504064559936523\n","epoch:  10 , batch:  132 , test loss:  17.3927001953125\n","epoch:  10 , batch:  133 , test loss:  13.376457214355469\n","epoch:  10 , batch:  134 , test loss:  28.505502700805664\n","epoch:  10 , batch:  135 , test loss:  27.056095123291016\n","epoch:  10 , batch:  136 , test loss:  22.664480209350586\n","epoch:  10 , batch:  137 , test loss:  10.807125091552734\n","epoch:  10 , batch:  138 , test loss:  24.18964195251465\n","epoch:  10 , batch:  139 , test loss:  23.969457626342773\n","epoch:  10 , batch:  140 , test loss:  17.360380172729492\n","epoch:  10 , batch:  141 , test loss:  20.559856414794922\n","epoch:  10 , batch:  142 , test loss:  18.963586807250977\n","epoch:  10 , batch:  143 , test loss:  22.85824966430664\n","epoch:  10 , batch:  144 , test loss:  8.081592559814453\n","epoch:  10 , batch:  145 , test loss:  22.44602394104004\n","epoch:  10 , batch:  146 , test loss:  30.175569534301758\n","epoch:  10 , batch:  147 , test loss:  19.45625877380371\n","epoch:  10 , batch:  148 , test loss:  35.86991500854492\n","epoch:  10 , batch:  149 , test loss:  29.692241668701172\n","epoch:  10 , batch:  150 , test loss:  22.167766571044922\n","epoch:  10 , batch:  151 , test loss:  14.030586242675781\n","epoch:  10 , batch:  152 , test loss:  21.968551635742188\n","epoch:  10 , batch:  153 , test loss:  31.61058807373047\n","epoch:  10 , batch:  154 , test loss:  18.38623046875\n","epoch:  10 , batch:  155 , test loss:  21.678869247436523\n","epoch:  10 , batch:  156 , test loss:  5.155681610107422\n","Accuracy of pytorch_model set: 0.8785\n","epoch:  11 , batch:  0 , train loss:  22.37675666809082\n","epoch:  11 , batch:  1 , train loss:  17.178651809692383\n","epoch:  11 , batch:  2 , train loss:  18.70692253112793\n","epoch:  11 , batch:  3 , train loss:  11.346405029296875\n","epoch:  11 , batch:  4 , train loss:  12.146472930908203\n","epoch:  11 , batch:  5 , train loss:  13.92060375213623\n","epoch:  11 , batch:  6 , train loss:  10.95232105255127\n","epoch:  11 , batch:  7 , train loss:  25.08486557006836\n","epoch:  11 , batch:  8 , train loss:  20.14730453491211\n","epoch:  11 , batch:  9 , train loss:  18.415273666381836\n","epoch:  11 , batch:  10 , train loss:  13.396419525146484\n","epoch:  11 , batch:  11 , train loss:  12.42129135131836\n","epoch:  11 , batch:  12 , train loss:  15.12981128692627\n","epoch:  11 , batch:  13 , train loss:  8.170758247375488\n","epoch:  11 , batch:  14 , train loss:  18.706501007080078\n","epoch:  11 , batch:  15 , train loss:  18.562788009643555\n","epoch:  11 , batch:  16 , train loss:  8.41934585571289\n","epoch:  11 , batch:  17 , train loss:  23.1591854095459\n","epoch:  11 , batch:  18 , train loss:  16.849390029907227\n","epoch:  11 , batch:  19 , train loss:  15.502309799194336\n","epoch:  11 , batch:  20 , train loss:  15.123153686523438\n","epoch:  11 , batch:  21 , train loss:  12.071528434753418\n","epoch:  11 , batch:  22 , train loss:  9.539169311523438\n","epoch:  11 , batch:  23 , train loss:  8.600895881652832\n","epoch:  11 , batch:  24 , train loss:  19.15247344970703\n","epoch:  11 , batch:  25 , train loss:  15.782439231872559\n","epoch:  11 , batch:  26 , train loss:  6.653733253479004\n","epoch:  11 , batch:  27 , train loss:  11.329222679138184\n","epoch:  11 , batch:  28 , train loss:  17.620248794555664\n","epoch:  11 , batch:  29 , train loss:  13.46064567565918\n","epoch:  11 , batch:  30 , train loss:  16.082286834716797\n","epoch:  11 , batch:  31 , train loss:  15.309969902038574\n","epoch:  11 , batch:  32 , train loss:  8.743803977966309\n","epoch:  11 , batch:  33 , train loss:  16.127361297607422\n","epoch:  11 , batch:  34 , train loss:  11.479108810424805\n","epoch:  11 , batch:  35 , train loss:  24.716716766357422\n","epoch:  11 , batch:  36 , train loss:  8.988954544067383\n","epoch:  11 , batch:  37 , train loss:  16.514705657958984\n","epoch:  11 , batch:  38 , train loss:  19.81850814819336\n","epoch:  11 , batch:  39 , train loss:  11.380509376525879\n","epoch:  11 , batch:  40 , train loss:  14.754794120788574\n","epoch:  11 , batch:  41 , train loss:  17.774076461791992\n","epoch:  11 , batch:  42 , train loss:  8.357364654541016\n","epoch:  11 , batch:  43 , train loss:  20.584102630615234\n","epoch:  11 , batch:  44 , train loss:  16.58732795715332\n","epoch:  11 , batch:  45 , train loss:  20.67600440979004\n","epoch:  11 , batch:  46 , train loss:  25.39010238647461\n","epoch:  11 , batch:  47 , train loss:  20.368513107299805\n","epoch:  11 , batch:  48 , train loss:  24.378271102905273\n","epoch:  11 , batch:  49 , train loss:  13.905265808105469\n","epoch:  11 , batch:  50 , train loss:  14.875182151794434\n","epoch:  11 , batch:  51 , train loss:  15.634027481079102\n","epoch:  11 , batch:  52 , train loss:  31.94087791442871\n","epoch:  11 , batch:  53 , train loss:  10.558427810668945\n","epoch:  11 , batch:  54 , train loss:  16.654315948486328\n","epoch:  11 , batch:  55 , train loss:  6.8714375495910645\n","epoch:  11 , batch:  56 , train loss:  12.222164154052734\n","epoch:  11 , batch:  57 , train loss:  23.79302406311035\n","epoch:  11 , batch:  58 , train loss:  11.550354957580566\n","epoch:  11 , batch:  59 , train loss:  16.16816520690918\n","epoch:  11 , batch:  60 , train loss:  10.640974044799805\n","epoch:  11 , batch:  61 , train loss:  18.215707778930664\n","epoch:  11 , batch:  62 , train loss:  12.577594757080078\n","epoch:  11 , batch:  63 , train loss:  15.43539810180664\n","epoch:  11 , batch:  64 , train loss:  9.370271682739258\n","epoch:  11 , batch:  65 , train loss:  16.644237518310547\n","epoch:  11 , batch:  66 , train loss:  12.573936462402344\n","epoch:  11 , batch:  67 , train loss:  19.921655654907227\n","epoch:  11 , batch:  68 , train loss:  14.641627311706543\n","epoch:  11 , batch:  69 , train loss:  12.726655960083008\n","epoch:  11 , batch:  70 , train loss:  18.004438400268555\n","epoch:  11 , batch:  71 , train loss:  4.174405097961426\n","epoch:  11 , batch:  72 , train loss:  16.568378448486328\n","epoch:  11 , batch:  73 , train loss:  24.068614959716797\n","epoch:  11 , batch:  74 , train loss:  24.89029884338379\n","epoch:  11 , batch:  75 , train loss:  16.254125595092773\n","epoch:  11 , batch:  76 , train loss:  13.44427490234375\n","epoch:  11 , batch:  77 , train loss:  17.130456924438477\n","epoch:  11 , batch:  78 , train loss:  11.495316505432129\n","epoch:  11 , batch:  79 , train loss:  15.42415714263916\n","epoch:  11 , batch:  80 , train loss:  13.088213920593262\n","epoch:  11 , batch:  81 , train loss:  8.958003997802734\n","epoch:  11 , batch:  82 , train loss:  24.77716636657715\n","epoch:  11 , batch:  83 , train loss:  15.992021560668945\n","epoch:  11 , batch:  84 , train loss:  11.542285919189453\n","epoch:  11 , batch:  85 , train loss:  8.79348373413086\n","epoch:  11 , batch:  86 , train loss:  8.826900482177734\n","epoch:  11 , batch:  87 , train loss:  17.39573860168457\n","epoch:  11 , batch:  88 , train loss:  14.721973419189453\n","epoch:  11 , batch:  89 , train loss:  7.036993503570557\n","epoch:  11 , batch:  90 , train loss:  27.960670471191406\n","epoch:  11 , batch:  91 , train loss:  42.90224838256836\n","epoch:  11 , batch:  92 , train loss:  35.00088882446289\n","epoch:  11 , batch:  93 , train loss:  21.05829620361328\n","epoch:  11 , batch:  94 , train loss:  18.715341567993164\n","epoch:  11 , batch:  95 , train loss:  18.747114181518555\n","epoch:  11 , batch:  96 , train loss:  14.671579360961914\n","epoch:  11 , batch:  97 , train loss:  22.23691177368164\n","epoch:  11 , batch:  98 , train loss:  21.55328941345215\n","epoch:  11 , batch:  99 , train loss:  14.523296356201172\n","epoch:  11 , batch:  100 , train loss:  29.275049209594727\n","epoch:  11 , batch:  101 , train loss:  16.551546096801758\n","epoch:  11 , batch:  102 , train loss:  27.90311050415039\n","epoch:  11 , batch:  103 , train loss:  15.185356140136719\n","epoch:  11 , batch:  104 , train loss:  9.907703399658203\n","epoch:  11 , batch:  105 , train loss:  18.278362274169922\n","epoch:  11 , batch:  106 , train loss:  13.23179817199707\n","epoch:  11 , batch:  107 , train loss:  19.025997161865234\n","epoch:  11 , batch:  108 , train loss:  21.401487350463867\n","epoch:  11 , batch:  109 , train loss:  18.383647918701172\n","epoch:  11 , batch:  110 , train loss:  11.496394157409668\n","epoch:  11 , batch:  111 , train loss:  14.696882247924805\n","epoch:  11 , batch:  112 , train loss:  7.6145339012146\n","epoch:  11 , batch:  113 , train loss:  19.502723693847656\n","epoch:  11 , batch:  114 , train loss:  22.06229019165039\n","epoch:  11 , batch:  115 , train loss:  19.882802963256836\n","epoch:  11 , batch:  116 , train loss:  25.19672393798828\n","epoch:  11 , batch:  117 , train loss:  13.217585563659668\n","epoch:  11 , batch:  118 , train loss:  12.743921279907227\n","epoch:  11 , batch:  119 , train loss:  21.67912483215332\n","epoch:  11 , batch:  120 , train loss:  17.800798416137695\n","epoch:  11 , batch:  121 , train loss:  11.255143165588379\n","epoch:  11 , batch:  122 , train loss:  16.323516845703125\n","epoch:  11 , batch:  123 , train loss:  24.377605438232422\n","epoch:  11 , batch:  124 , train loss:  17.106332778930664\n","epoch:  11 , batch:  125 , train loss:  12.484027862548828\n","epoch:  11 , batch:  126 , train loss:  10.691849708557129\n","epoch:  11 , batch:  127 , train loss:  14.498741149902344\n","epoch:  11 , batch:  128 , train loss:  9.223922729492188\n","epoch:  11 , batch:  129 , train loss:  10.009176254272461\n","epoch:  11 , batch:  130 , train loss:  18.227781295776367\n","epoch:  11 , batch:  131 , train loss:  11.89592170715332\n","epoch:  11 , batch:  132 , train loss:  15.991585731506348\n","epoch:  11 , batch:  133 , train loss:  19.088459014892578\n","epoch:  11 , batch:  134 , train loss:  4.294899940490723\n","epoch:  11 , batch:  135 , train loss:  22.93344497680664\n","epoch:  11 , batch:  136 , train loss:  6.566980361938477\n","epoch:  11 , batch:  137 , train loss:  7.839585304260254\n","epoch:  11 , batch:  138 , train loss:  6.897619247436523\n","epoch:  11 , batch:  139 , train loss:  14.704002380371094\n","epoch:  11 , batch:  140 , train loss:  18.168485641479492\n","epoch:  11 , batch:  141 , train loss:  15.648344039916992\n","epoch:  11 , batch:  142 , train loss:  11.057640075683594\n","epoch:  11 , batch:  143 , train loss:  13.890896797180176\n","epoch:  11 , batch:  144 , train loss:  14.51876449584961\n","epoch:  11 , batch:  145 , train loss:  24.495670318603516\n","epoch:  11 , batch:  146 , train loss:  7.847832202911377\n","epoch:  11 , batch:  147 , train loss:  14.432594299316406\n","epoch:  11 , batch:  148 , train loss:  11.673727035522461\n","epoch:  11 , batch:  149 , train loss:  12.446836471557617\n","epoch:  11 , batch:  150 , train loss:  10.739141464233398\n","epoch:  11 , batch:  151 , train loss:  16.216144561767578\n","epoch:  11 , batch:  152 , train loss:  14.590332984924316\n","epoch:  11 , batch:  153 , train loss:  10.559680938720703\n","epoch:  11 , batch:  154 , train loss:  15.083901405334473\n","epoch:  11 , batch:  155 , train loss:  12.471059799194336\n","epoch:  11 , batch:  156 , train loss:  28.403053283691406\n","epoch:  11 , batch:  157 , train loss:  20.697141647338867\n","epoch:  11 , batch:  158 , train loss:  20.530672073364258\n","epoch:  11 , batch:  159 , train loss:  10.454302787780762\n","epoch:  11 , batch:  160 , train loss:  19.738910675048828\n","epoch:  11 , batch:  161 , train loss:  29.679973602294922\n","epoch:  11 , batch:  162 , train loss:  20.013490676879883\n","epoch:  11 , batch:  163 , train loss:  17.52403450012207\n","epoch:  11 , batch:  164 , train loss:  16.10894012451172\n","epoch:  11 , batch:  165 , train loss:  17.735401153564453\n","epoch:  11 , batch:  166 , train loss:  17.23535919189453\n","epoch:  11 , batch:  167 , train loss:  20.12651252746582\n","epoch:  11 , batch:  168 , train loss:  17.93905258178711\n","epoch:  11 , batch:  169 , train loss:  21.440244674682617\n","epoch:  11 , batch:  170 , train loss:  14.121066093444824\n","epoch:  11 , batch:  171 , train loss:  16.681842803955078\n","epoch:  11 , batch:  172 , train loss:  15.758438110351562\n","epoch:  11 , batch:  173 , train loss:  18.959854125976562\n","epoch:  11 , batch:  174 , train loss:  13.580751419067383\n","epoch:  11 , batch:  175 , train loss:  11.437052726745605\n","epoch:  11 , batch:  176 , train loss:  13.002946853637695\n","epoch:  11 , batch:  177 , train loss:  23.946842193603516\n","epoch:  11 , batch:  178 , train loss:  26.589282989501953\n","epoch:  11 , batch:  179 , train loss:  17.435161590576172\n","epoch:  11 , batch:  180 , train loss:  27.018430709838867\n","epoch:  11 , batch:  181 , train loss:  12.556715965270996\n","epoch:  11 , batch:  182 , train loss:  15.613432884216309\n","epoch:  11 , batch:  183 , train loss:  12.19987678527832\n","epoch:  11 , batch:  184 , train loss:  13.06004810333252\n","epoch:  11 , batch:  185 , train loss:  14.217513084411621\n","epoch:  11 , batch:  186 , train loss:  8.299456596374512\n","epoch:  11 , batch:  187 , train loss:  13.82494831085205\n","epoch:  11 , batch:  188 , train loss:  16.256610870361328\n","epoch:  11 , batch:  189 , train loss:  16.241107940673828\n","epoch:  11 , batch:  190 , train loss:  9.849373817443848\n","epoch:  11 , batch:  191 , train loss:  12.018887519836426\n","epoch:  11 , batch:  192 , train loss:  22.477096557617188\n","epoch:  11 , batch:  193 , train loss:  12.571538925170898\n","epoch:  11 , batch:  194 , train loss:  9.868398666381836\n","epoch:  11 , batch:  195 , train loss:  19.060626983642578\n","epoch:  11 , batch:  196 , train loss:  24.861934661865234\n","epoch:  11 , batch:  197 , train loss:  10.884066581726074\n","epoch:  11 , batch:  198 , train loss:  10.947925567626953\n","epoch:  11 , batch:  199 , train loss:  14.515121459960938\n","epoch:  11 , batch:  200 , train loss:  10.895085334777832\n","epoch:  11 , batch:  201 , train loss:  16.145864486694336\n","epoch:  11 , batch:  202 , train loss:  14.047880172729492\n","epoch:  11 , batch:  203 , train loss:  25.338472366333008\n","epoch:  11 , batch:  204 , train loss:  12.472543716430664\n","epoch:  11 , batch:  205 , train loss:  15.768219947814941\n","epoch:  11 , batch:  206 , train loss:  19.44061279296875\n","epoch:  11 , batch:  207 , train loss:  18.939973831176758\n","epoch:  11 , batch:  208 , train loss:  31.87468719482422\n","epoch:  11 , batch:  209 , train loss:  16.579692840576172\n","epoch:  11 , batch:  210 , train loss:  19.38422966003418\n","epoch:  11 , batch:  211 , train loss:  16.133228302001953\n","epoch:  11 , batch:  212 , train loss:  26.524194717407227\n","epoch:  11 , batch:  213 , train loss:  17.54275131225586\n","epoch:  11 , batch:  214 , train loss:  15.156846046447754\n","epoch:  11 , batch:  215 , train loss:  16.85074806213379\n","epoch:  11 , batch:  216 , train loss:  9.409574508666992\n","epoch:  11 , batch:  217 , train loss:  15.941754341125488\n","epoch:  11 , batch:  218 , train loss:  15.025771141052246\n","epoch:  11 , batch:  219 , train loss:  14.111234664916992\n","epoch:  11 , batch:  220 , train loss:  25.363765716552734\n","epoch:  11 , batch:  221 , train loss:  12.469385147094727\n","epoch:  11 , batch:  222 , train loss:  17.576812744140625\n","epoch:  11 , batch:  223 , train loss:  22.247543334960938\n","epoch:  11 , batch:  224 , train loss:  9.897468566894531\n","epoch:  11 , batch:  225 , train loss:  25.29676055908203\n","epoch:  11 , batch:  226 , train loss:  11.267749786376953\n","epoch:  11 , batch:  227 , train loss:  17.58383560180664\n","epoch:  11 , batch:  228 , train loss:  11.05495548248291\n","epoch:  11 , batch:  229 , train loss:  9.977683067321777\n","epoch:  11 , batch:  230 , train loss:  19.025087356567383\n","epoch:  11 , batch:  231 , train loss:  7.601287841796875\n","epoch:  11 , batch:  232 , train loss:  8.743277549743652\n","epoch:  11 , batch:  233 , train loss:  12.06778335571289\n","epoch:  11 , batch:  234 , train loss:  13.67681884765625\n","epoch:  11 , batch:  235 , train loss:  14.493826866149902\n","epoch:  11 , batch:  236 , train loss:  18.121089935302734\n","epoch:  11 , batch:  237 , train loss:  23.61674690246582\n","epoch:  11 , batch:  238 , train loss:  9.429447174072266\n","epoch:  11 , batch:  239 , train loss:  23.03738784790039\n","epoch:  11 , batch:  240 , train loss:  10.852692604064941\n","epoch:  11 , batch:  241 , train loss:  14.08099365234375\n","epoch:  11 , batch:  242 , train loss:  17.400463104248047\n","epoch:  11 , batch:  243 , train loss:  11.311644554138184\n","epoch:  11 , batch:  244 , train loss:  14.557778358459473\n","epoch:  11 , batch:  245 , train loss:  20.423051834106445\n","epoch:  11 , batch:  246 , train loss:  7.832520008087158\n","epoch:  11 , batch:  247 , train loss:  19.675106048583984\n","epoch:  11 , batch:  248 , train loss:  5.721360206604004\n","epoch:  11 , batch:  249 , train loss:  9.221796989440918\n","epoch:  11 , batch:  250 , train loss:  9.141805648803711\n","epoch:  11 , batch:  251 , train loss:  6.2280354499816895\n","epoch:  11 , batch:  252 , train loss:  21.673839569091797\n","epoch:  11 , batch:  253 , train loss:  9.909748077392578\n","epoch:  11 , batch:  254 , train loss:  15.408658027648926\n","epoch:  11 , batch:  255 , train loss:  9.571117401123047\n","epoch:  11 , batch:  256 , train loss:  13.26302719116211\n","epoch:  11 , batch:  257 , train loss:  7.981617450714111\n","epoch:  11 , batch:  258 , train loss:  21.635229110717773\n","epoch:  11 , batch:  259 , train loss:  16.240768432617188\n","epoch:  11 , batch:  260 , train loss:  12.234092712402344\n","epoch:  11 , batch:  261 , train loss:  11.011099815368652\n","epoch:  11 , batch:  262 , train loss:  8.893431663513184\n","epoch:  11 , batch:  263 , train loss:  16.979888916015625\n","epoch:  11 , batch:  264 , train loss:  10.458706855773926\n","epoch:  11 , batch:  265 , train loss:  14.966390609741211\n","epoch:  11 , batch:  266 , train loss:  10.903079986572266\n","epoch:  11 , batch:  267 , train loss:  11.051801681518555\n","epoch:  11 , batch:  268 , train loss:  13.30630111694336\n","epoch:  11 , batch:  269 , train loss:  8.233221054077148\n","epoch:  11 , batch:  270 , train loss:  15.11651611328125\n","epoch:  11 , batch:  271 , train loss:  18.845199584960938\n","epoch:  11 , batch:  272 , train loss:  19.39217185974121\n","epoch:  11 , batch:  273 , train loss:  13.606764793395996\n","epoch:  11 , batch:  274 , train loss:  11.353909492492676\n","epoch:  11 , batch:  275 , train loss:  11.403472900390625\n","epoch:  11 , batch:  276 , train loss:  15.377237319946289\n","epoch:  11 , batch:  277 , train loss:  11.014379501342773\n","epoch:  11 , batch:  278 , train loss:  10.817020416259766\n","epoch:  11 , batch:  279 , train loss:  14.591341018676758\n","epoch:  11 , batch:  280 , train loss:  19.692535400390625\n","epoch:  11 , batch:  281 , train loss:  11.505790710449219\n","epoch:  11 , batch:  282 , train loss:  20.67618179321289\n","epoch:  11 , batch:  283 , train loss:  11.292901039123535\n","epoch:  11 , batch:  284 , train loss:  11.270597457885742\n","epoch:  11 , batch:  285 , train loss:  12.102078437805176\n","epoch:  11 , batch:  286 , train loss:  17.801733016967773\n","epoch:  11 , batch:  287 , train loss:  22.26272201538086\n","epoch:  11 , batch:  288 , train loss:  9.670324325561523\n","epoch:  11 , batch:  289 , train loss:  19.107868194580078\n","epoch:  11 , batch:  290 , train loss:  17.709989547729492\n","epoch:  11 , batch:  291 , train loss:  15.00688648223877\n","epoch:  11 , batch:  292 , train loss:  21.02450942993164\n","epoch:  11 , batch:  293 , train loss:  7.496079921722412\n","epoch:  11 , batch:  294 , train loss:  12.529339790344238\n","epoch:  11 , batch:  295 , train loss:  20.739362716674805\n","epoch:  11 , batch:  296 , train loss:  15.548940658569336\n","epoch:  11 , batch:  297 , train loss:  17.071182250976562\n","epoch:  11 , batch:  298 , train loss:  15.10046672821045\n","epoch:  11 , batch:  299 , train loss:  8.303184509277344\n","epoch:  11 , batch:  300 , train loss:  12.060162544250488\n","epoch:  11 , batch:  301 , train loss:  18.042177200317383\n","epoch:  11 , batch:  302 , train loss:  17.590787887573242\n","epoch:  11 , batch:  303 , train loss:  23.56964111328125\n","epoch:  11 , batch:  304 , train loss:  20.77032470703125\n","epoch:  11 , batch:  305 , train loss:  17.58349609375\n","epoch:  11 , batch:  306 , train loss:  15.67637825012207\n","epoch:  11 , batch:  307 , train loss:  17.593961715698242\n","epoch:  11 , batch:  308 , train loss:  8.24843978881836\n","epoch:  11 , batch:  309 , train loss:  11.91469669342041\n","epoch:  11 , batch:  310 , train loss:  17.040138244628906\n","epoch:  11 , batch:  311 , train loss:  12.927590370178223\n","epoch:  11 , batch:  312 , train loss:  23.709129333496094\n","epoch:  11 , batch:  313 , train loss:  14.087945938110352\n","epoch:  11 , batch:  314 , train loss:  20.949331283569336\n","epoch:  11 , batch:  315 , train loss:  7.827420711517334\n","epoch:  11 , batch:  316 , train loss:  12.902240753173828\n","epoch:  11 , batch:  317 , train loss:  12.80533504486084\n","epoch:  11 , batch:  318 , train loss:  15.778569221496582\n","epoch:  11 , batch:  319 , train loss:  21.850988388061523\n","epoch:  11 , batch:  320 , train loss:  17.168134689331055\n","epoch:  11 , batch:  321 , train loss:  15.5029296875\n","epoch:  11 , batch:  322 , train loss:  16.264446258544922\n","epoch:  11 , batch:  323 , train loss:  14.666309356689453\n","epoch:  11 , batch:  324 , train loss:  14.917167663574219\n","epoch:  11 , batch:  325 , train loss:  13.594599723815918\n","epoch:  11 , batch:  326 , train loss:  30.303972244262695\n","epoch:  11 , batch:  327 , train loss:  17.297771453857422\n","epoch:  11 , batch:  328 , train loss:  9.107481956481934\n","epoch:  11 , batch:  329 , train loss:  13.04692268371582\n","epoch:  11 , batch:  330 , train loss:  12.54773998260498\n","epoch:  11 , batch:  331 , train loss:  19.479915618896484\n","epoch:  11 , batch:  332 , train loss:  17.798418045043945\n","epoch:  11 , batch:  333 , train loss:  13.761162757873535\n","epoch:  11 , batch:  334 , train loss:  16.909990310668945\n","epoch:  11 , batch:  335 , train loss:  12.408463478088379\n","epoch:  11 , batch:  336 , train loss:  13.740534782409668\n","epoch:  11 , batch:  337 , train loss:  9.482345581054688\n","epoch:  11 , batch:  338 , train loss:  17.620939254760742\n","epoch:  11 , batch:  339 , train loss:  13.253790855407715\n","epoch:  11 , batch:  340 , train loss:  10.740124702453613\n","epoch:  11 , batch:  341 , train loss:  15.678191184997559\n","epoch:  11 , batch:  342 , train loss:  9.60329532623291\n","epoch:  11 , batch:  343 , train loss:  12.483990669250488\n","epoch:  11 , batch:  344 , train loss:  16.451513290405273\n","epoch:  11 , batch:  345 , train loss:  17.972835540771484\n","epoch:  11 , batch:  346 , train loss:  15.65949821472168\n","epoch:  11 , batch:  347 , train loss:  8.6166410446167\n","epoch:  11 , batch:  348 , train loss:  12.049283981323242\n","epoch:  11 , batch:  349 , train loss:  13.415648460388184\n","epoch:  11 , batch:  350 , train loss:  20.943771362304688\n","epoch:  11 , batch:  351 , train loss:  21.39413070678711\n","epoch:  11 , batch:  352 , train loss:  16.240720748901367\n","epoch:  11 , batch:  353 , train loss:  17.50001335144043\n","epoch:  11 , batch:  354 , train loss:  13.945371627807617\n","epoch:  11 , batch:  355 , train loss:  16.623035430908203\n","epoch:  11 , batch:  356 , train loss:  14.516115188598633\n","epoch:  11 , batch:  357 , train loss:  15.465755462646484\n","epoch:  11 , batch:  358 , train loss:  21.2907772064209\n","epoch:  11 , batch:  359 , train loss:  24.309101104736328\n","epoch:  11 , batch:  360 , train loss:  20.116928100585938\n","epoch:  11 , batch:  361 , train loss:  27.496078491210938\n","epoch:  11 , batch:  362 , train loss:  17.181108474731445\n","epoch:  11 , batch:  363 , train loss:  19.61656951904297\n","epoch:  11 , batch:  364 , train loss:  4.518097400665283\n","epoch:  11 , batch:  365 , train loss:  7.081569671630859\n","epoch:  11 , batch:  366 , train loss:  16.416303634643555\n","epoch:  11 , batch:  367 , train loss:  9.125123977661133\n","epoch:  11 , batch:  368 , train loss:  17.75145721435547\n","epoch:  11 , batch:  369 , train loss:  14.417557716369629\n","epoch:  11 , batch:  370 , train loss:  11.42729377746582\n","epoch:  11 , batch:  371 , train loss:  23.921140670776367\n","epoch:  11 , batch:  372 , train loss:  15.204702377319336\n","epoch:  11 , batch:  373 , train loss:  12.225446701049805\n","epoch:  11 , batch:  374 , train loss:  21.525203704833984\n","epoch:  11 , batch:  375 , train loss:  11.77891731262207\n","epoch:  11 , batch:  376 , train loss:  16.1368465423584\n","epoch:  11 , batch:  377 , train loss:  16.85715103149414\n","epoch:  11 , batch:  378 , train loss:  9.284661293029785\n","epoch:  11 , batch:  379 , train loss:  14.033442497253418\n","epoch:  11 , batch:  380 , train loss:  7.953333377838135\n","epoch:  11 , batch:  381 , train loss:  23.054399490356445\n","epoch:  11 , batch:  382 , train loss:  21.14724349975586\n","epoch:  11 , batch:  383 , train loss:  17.775026321411133\n","epoch:  11 , batch:  384 , train loss:  13.656155586242676\n","epoch:  11 , batch:  385 , train loss:  11.075016975402832\n","epoch:  11 , batch:  386 , train loss:  17.522748947143555\n","epoch:  11 , batch:  387 , train loss:  7.709955215454102\n","epoch:  11 , batch:  388 , train loss:  15.806431770324707\n","epoch:  11 , batch:  389 , train loss:  16.351884841918945\n","epoch:  11 , batch:  390 , train loss:  28.323455810546875\n","epoch:  11 , batch:  391 , train loss:  15.442882537841797\n","epoch:  11 , batch:  392 , train loss:  20.357589721679688\n","epoch:  11 , batch:  393 , train loss:  19.132408142089844\n","epoch:  11 , batch:  394 , train loss:  13.270650863647461\n","epoch:  11 , batch:  395 , train loss:  9.482357025146484\n","epoch:  11 , batch:  396 , train loss:  8.154866218566895\n","epoch:  11 , batch:  397 , train loss:  17.97357749938965\n","epoch:  11 , batch:  398 , train loss:  31.24290657043457\n","epoch:  11 , batch:  399 , train loss:  17.123964309692383\n","epoch:  11 , batch:  400 , train loss:  16.071393966674805\n","epoch:  11 , batch:  401 , train loss:  13.716337203979492\n","epoch:  11 , batch:  402 , train loss:  17.295116424560547\n","epoch:  11 , batch:  403 , train loss:  19.148040771484375\n","epoch:  11 , batch:  404 , train loss:  15.506769180297852\n","epoch:  11 , batch:  405 , train loss:  15.1116304397583\n","epoch:  11 , batch:  406 , train loss:  17.516077041625977\n","epoch:  11 , batch:  407 , train loss:  12.281495094299316\n","epoch:  11 , batch:  408 , train loss:  15.673807144165039\n","epoch:  11 , batch:  409 , train loss:  16.264755249023438\n","epoch:  11 , batch:  410 , train loss:  13.716084480285645\n","epoch:  11 , batch:  411 , train loss:  14.525684356689453\n","epoch:  11 , batch:  412 , train loss:  10.21015453338623\n","epoch:  11 , batch:  413 , train loss:  16.780536651611328\n","epoch:  11 , batch:  414 , train loss:  20.756248474121094\n","epoch:  11 , batch:  415 , train loss:  12.72826099395752\n","epoch:  11 , batch:  416 , train loss:  15.122261047363281\n","epoch:  11 , batch:  417 , train loss:  14.758866310119629\n","epoch:  11 , batch:  418 , train loss:  7.261605262756348\n","epoch:  11 , batch:  419 , train loss:  20.590919494628906\n","epoch:  11 , batch:  420 , train loss:  19.311397552490234\n","epoch:  11 , batch:  421 , train loss:  22.63018798828125\n","epoch:  11 , batch:  422 , train loss:  16.20335578918457\n","epoch:  11 , batch:  423 , train loss:  24.552257537841797\n","epoch:  11 , batch:  424 , train loss:  7.73956298828125\n","epoch:  11 , batch:  425 , train loss:  9.45846176147461\n","epoch:  11 , batch:  426 , train loss:  12.761239051818848\n","epoch:  11 , batch:  427 , train loss:  24.49936866760254\n","epoch:  11 , batch:  428 , train loss:  13.134143829345703\n","epoch:  11 , batch:  429 , train loss:  11.451217651367188\n","epoch:  11 , batch:  430 , train loss:  14.734329223632812\n","epoch:  11 , batch:  431 , train loss:  13.751980781555176\n","epoch:  11 , batch:  432 , train loss:  9.903447151184082\n","epoch:  11 , batch:  433 , train loss:  13.54294204711914\n","epoch:  11 , batch:  434 , train loss:  15.653616905212402\n","epoch:  11 , batch:  435 , train loss:  15.057442665100098\n","epoch:  11 , batch:  436 , train loss:  27.8199462890625\n","epoch:  11 , batch:  437 , train loss:  10.958148002624512\n","epoch:  11 , batch:  438 , train loss:  25.956281661987305\n","epoch:  11 , batch:  439 , train loss:  8.25721263885498\n","epoch:  11 , batch:  440 , train loss:  21.3429012298584\n","epoch:  11 , batch:  441 , train loss:  9.188165664672852\n","epoch:  11 , batch:  442 , train loss:  14.956582069396973\n","epoch:  11 , batch:  443 , train loss:  15.983466148376465\n","epoch:  11 , batch:  444 , train loss:  15.355005264282227\n","epoch:  11 , batch:  445 , train loss:  8.480652809143066\n","epoch:  11 , batch:  446 , train loss:  17.10360336303711\n","epoch:  11 , batch:  447 , train loss:  13.075682640075684\n","epoch:  11 , batch:  448 , train loss:  12.427886962890625\n","epoch:  11 , batch:  449 , train loss:  20.018779754638672\n","epoch:  11 , batch:  450 , train loss:  14.57902717590332\n","epoch:  11 , batch:  451 , train loss:  19.335880279541016\n","epoch:  11 , batch:  452 , train loss:  11.870705604553223\n","epoch:  11 , batch:  453 , train loss:  16.620830535888672\n","epoch:  11 , batch:  454 , train loss:  13.64389419555664\n","epoch:  11 , batch:  455 , train loss:  19.518800735473633\n","epoch:  11 , batch:  456 , train loss:  13.860701560974121\n","epoch:  11 , batch:  457 , train loss:  16.58301544189453\n","epoch:  11 , batch:  458 , train loss:  21.29657554626465\n","epoch:  11 , batch:  459 , train loss:  18.531055450439453\n","epoch:  11 , batch:  460 , train loss:  11.888039588928223\n","epoch:  11 , batch:  461 , train loss:  15.556853294372559\n","epoch:  11 , batch:  462 , train loss:  7.45405387878418\n","epoch:  11 , batch:  463 , train loss:  22.774730682373047\n","epoch:  11 , batch:  464 , train loss:  17.763198852539062\n","epoch:  11 , batch:  465 , train loss:  25.710969924926758\n","epoch:  11 , batch:  466 , train loss:  14.095672607421875\n","epoch:  11 , batch:  467 , train loss:  14.622581481933594\n","epoch:  11 , batch:  468 , train loss:  16.498523712158203\n","epoch:  11 , batch:  469 , train loss:  30.85175132751465\n","epoch:  11 , batch:  470 , train loss:  19.84916877746582\n","epoch:  11 , batch:  471 , train loss:  13.222066879272461\n","epoch:  11 , batch:  472 , train loss:  13.013805389404297\n","epoch:  11 , batch:  473 , train loss:  18.005033493041992\n","epoch:  11 , batch:  474 , train loss:  17.628381729125977\n","epoch:  11 , batch:  475 , train loss:  12.62093448638916\n","epoch:  11 , batch:  476 , train loss:  20.606348037719727\n","epoch:  11 , batch:  477 , train loss:  21.631488800048828\n","epoch:  11 , batch:  478 , train loss:  18.2830810546875\n","epoch:  11 , batch:  479 , train loss:  18.536619186401367\n","epoch:  11 , batch:  480 , train loss:  19.87311553955078\n","epoch:  11 , batch:  481 , train loss:  21.63412857055664\n","epoch:  11 , batch:  482 , train loss:  16.127416610717773\n","epoch:  11 , batch:  483 , train loss:  14.77342700958252\n","epoch:  11 , batch:  484 , train loss:  19.040042877197266\n","epoch:  11 , batch:  485 , train loss:  15.522348403930664\n","epoch:  11 , batch:  486 , train loss:  10.771001815795898\n","epoch:  11 , batch:  487 , train loss:  22.25146484375\n","epoch:  11 , batch:  488 , train loss:  13.295433044433594\n","epoch:  11 , batch:  489 , train loss:  15.949591636657715\n","epoch:  11 , batch:  490 , train loss:  14.937734603881836\n","epoch:  11 , batch:  491 , train loss:  22.464052200317383\n","epoch:  11 , batch:  492 , train loss:  24.52182960510254\n","epoch:  11 , batch:  493 , train loss:  18.689054489135742\n","epoch:  11 , batch:  494 , train loss:  12.73865795135498\n","epoch:  11 , batch:  495 , train loss:  11.252519607543945\n","epoch:  11 , batch:  496 , train loss:  25.085922241210938\n","epoch:  11 , batch:  497 , train loss:  16.833444595336914\n","epoch:  11 , batch:  498 , train loss:  12.841047286987305\n","epoch:  11 , batch:  499 , train loss:  13.368706703186035\n","epoch:  11 , batch:  500 , train loss:  11.845566749572754\n","epoch:  11 , batch:  501 , train loss:  31.20263671875\n","epoch:  11 , batch:  502 , train loss:  13.418599128723145\n","epoch:  11 , batch:  503 , train loss:  23.67217254638672\n","epoch:  11 , batch:  504 , train loss:  12.47989273071289\n","epoch:  11 , batch:  505 , train loss:  19.995737075805664\n","epoch:  11 , batch:  506 , train loss:  21.7850284576416\n","epoch:  11 , batch:  507 , train loss:  27.239866256713867\n","epoch:  11 , batch:  508 , train loss:  14.148528099060059\n","epoch:  11 , batch:  509 , train loss:  12.634671211242676\n","epoch:  11 , batch:  510 , train loss:  13.161443710327148\n","epoch:  11 , batch:  511 , train loss:  14.924178123474121\n","epoch:  11 , batch:  512 , train loss:  19.842884063720703\n","epoch:  11 , batch:  513 , train loss:  13.128111839294434\n","epoch:  11 , batch:  514 , train loss:  14.242539405822754\n","epoch:  11 , batch:  515 , train loss:  13.457219123840332\n","epoch:  11 , batch:  516 , train loss:  15.483566284179688\n","epoch:  11 , batch:  517 , train loss:  13.231795310974121\n","epoch:  11 , batch:  518 , train loss:  9.27202033996582\n","epoch:  11 , batch:  519 , train loss:  22.862939834594727\n","epoch:  11 , batch:  520 , train loss:  16.202518463134766\n","epoch:  11 , batch:  521 , train loss:  12.633135795593262\n","epoch:  11 , batch:  522 , train loss:  12.964781761169434\n","epoch:  11 , batch:  523 , train loss:  9.578410148620605\n","epoch:  11 , batch:  524 , train loss:  13.091680526733398\n","epoch:  11 , batch:  525 , train loss:  7.7888078689575195\n","epoch:  11 , batch:  526 , train loss:  16.388639450073242\n","epoch:  11 , batch:  527 , train loss:  11.736385345458984\n","epoch:  11 , batch:  528 , train loss:  17.36790657043457\n","epoch:  11 , batch:  529 , train loss:  13.926985740661621\n","epoch:  11 , batch:  530 , train loss:  17.97954559326172\n","epoch:  11 , batch:  531 , train loss:  26.81552505493164\n","epoch:  11 , batch:  532 , train loss:  12.975786209106445\n","epoch:  11 , batch:  533 , train loss:  20.6894474029541\n","epoch:  11 , batch:  534 , train loss:  14.327370643615723\n","epoch:  11 , batch:  535 , train loss:  20.989543914794922\n","epoch:  11 , batch:  536 , train loss:  20.592851638793945\n","epoch:  11 , batch:  537 , train loss:  11.730515480041504\n","epoch:  11 , batch:  538 , train loss:  12.144434928894043\n","epoch:  11 , batch:  539 , train loss:  14.215126037597656\n","epoch:  11 , batch:  540 , train loss:  24.22254180908203\n","epoch:  11 , batch:  541 , train loss:  22.82320213317871\n","epoch:  11 , batch:  542 , train loss:  14.721976280212402\n","epoch:  11 , batch:  543 , train loss:  7.572048187255859\n","epoch:  11 , batch:  544 , train loss:  14.742593765258789\n","epoch:  11 , batch:  545 , train loss:  17.526708602905273\n","epoch:  11 , batch:  546 , train loss:  11.920282363891602\n","epoch:  11 , batch:  547 , train loss:  17.133420944213867\n","epoch:  11 , batch:  548 , train loss:  8.033148765563965\n","epoch:  11 , batch:  549 , train loss:  13.907777786254883\n","epoch:  11 , batch:  550 , train loss:  15.90117073059082\n","epoch:  11 , batch:  551 , train loss:  16.615745544433594\n","epoch:  11 , batch:  552 , train loss:  17.246675491333008\n","epoch:  11 , batch:  553 , train loss:  14.091282844543457\n","epoch:  11 , batch:  554 , train loss:  13.888733863830566\n","epoch:  11 , batch:  555 , train loss:  18.1025390625\n","epoch:  11 , batch:  556 , train loss:  15.749319076538086\n","epoch:  11 , batch:  557 , train loss:  24.90469741821289\n","epoch:  11 , batch:  558 , train loss:  15.515952110290527\n","epoch:  11 , batch:  559 , train loss:  11.664056777954102\n","epoch:  11 , batch:  560 , train loss:  21.258405685424805\n","epoch:  11 , batch:  561 , train loss:  13.525063514709473\n","epoch:  11 , batch:  562 , train loss:  18.041122436523438\n","epoch:  11 , batch:  563 , train loss:  21.394325256347656\n","epoch:  11 , batch:  564 , train loss:  11.087756156921387\n","epoch:  11 , batch:  565 , train loss:  25.776464462280273\n","epoch:  11 , batch:  566 , train loss:  12.25658893585205\n","epoch:  11 , batch:  567 , train loss:  13.322662353515625\n","epoch:  11 , batch:  568 , train loss:  12.459266662597656\n","epoch:  11 , batch:  569 , train loss:  11.29763412475586\n","epoch:  11 , batch:  570 , train loss:  25.012874603271484\n","epoch:  11 , batch:  571 , train loss:  21.29450225830078\n","epoch:  11 , batch:  572 , train loss:  9.330121040344238\n","epoch:  11 , batch:  573 , train loss:  13.75970458984375\n","epoch:  11 , batch:  574 , train loss:  21.2926082611084\n","epoch:  11 , batch:  575 , train loss:  16.864961624145508\n","epoch:  11 , batch:  576 , train loss:  6.941898345947266\n","epoch:  11 , batch:  577 , train loss:  10.966628074645996\n","epoch:  11 , batch:  578 , train loss:  18.203916549682617\n","epoch:  11 , batch:  579 , train loss:  10.040230751037598\n","epoch:  11 , batch:  580 , train loss:  19.44252586364746\n","epoch:  11 , batch:  581 , train loss:  12.313651084899902\n","epoch:  11 , batch:  582 , train loss:  20.463809967041016\n","epoch:  11 , batch:  583 , train loss:  17.71238899230957\n","epoch:  11 , batch:  584 , train loss:  20.35232925415039\n","epoch:  11 , batch:  585 , train loss:  20.32694435119629\n","epoch:  11 , batch:  586 , train loss:  11.39615535736084\n","epoch:  11 , batch:  587 , train loss:  17.632490158081055\n","epoch:  11 , batch:  588 , train loss:  18.654518127441406\n","epoch:  11 , batch:  589 , train loss:  18.639541625976562\n","epoch:  11 , batch:  590 , train loss:  17.802621841430664\n","epoch:  11 , batch:  591 , train loss:  9.280619621276855\n","epoch:  11 , batch:  592 , train loss:  23.20006561279297\n","epoch:  11 , batch:  593 , train loss:  24.229326248168945\n","epoch:  11 , batch:  594 , train loss:  14.973002433776855\n","epoch:  11 , batch:  595 , train loss:  17.309701919555664\n","epoch:  11 , batch:  596 , train loss:  20.115543365478516\n","epoch:  11 , batch:  597 , train loss:  11.96550464630127\n","epoch:  11 , batch:  598 , train loss:  16.9497127532959\n","epoch:  11 , batch:  599 , train loss:  21.685523986816406\n","epoch:  11 , batch:  600 , train loss:  20.328874588012695\n","epoch:  11 , batch:  601 , train loss:  15.855308532714844\n","epoch:  11 , batch:  602 , train loss:  10.025603294372559\n","epoch:  11 , batch:  603 , train loss:  13.407637596130371\n","epoch:  11 , batch:  604 , train loss:  9.33312702178955\n","epoch:  11 , batch:  605 , train loss:  10.542323112487793\n","epoch:  11 , batch:  606 , train loss:  16.452646255493164\n","epoch:  11 , batch:  607 , train loss:  13.862533569335938\n","epoch:  11 , batch:  608 , train loss:  8.263412475585938\n","epoch:  11 , batch:  609 , train loss:  17.92890167236328\n","epoch:  11 , batch:  610 , train loss:  13.086992263793945\n","epoch:  11 , batch:  611 , train loss:  20.683860778808594\n","epoch:  11 , batch:  612 , train loss:  14.12288761138916\n","epoch:  11 , batch:  613 , train loss:  19.07044219970703\n","epoch:  11 , batch:  614 , train loss:  10.979125022888184\n","epoch:  11 , batch:  615 , train loss:  5.253093719482422\n","epoch:  11 , batch:  616 , train loss:  23.420429229736328\n","epoch:  11 , batch:  617 , train loss:  11.870503425598145\n","epoch:  11 , batch:  618 , train loss:  19.206134796142578\n","epoch:  11 , batch:  619 , train loss:  13.335594177246094\n","epoch:  11 , batch:  620 , train loss:  13.28267765045166\n","epoch:  11 , batch:  621 , train loss:  9.395806312561035\n","epoch:  11 , batch:  622 , train loss:  18.18460464477539\n","epoch:  11 , batch:  623 , train loss:  17.850168228149414\n","epoch:  11 , batch:  624 , train loss:  15.94342041015625\n","epoch:  11 , batch:  625 , train loss:  16.32286262512207\n","epoch:  11 , batch:  626 , train loss:  16.66829490661621\n","epoch:  11 , batch:  627 , train loss:  22.424816131591797\n","epoch:  11 , batch:  628 , train loss:  24.24419403076172\n","epoch:  11 , batch:  629 , train loss:  17.347515106201172\n","epoch:  11 , batch:  630 , train loss:  17.93041229248047\n","epoch:  11 , batch:  631 , train loss:  10.710524559020996\n","epoch:  11 , batch:  632 , train loss:  15.454625129699707\n","epoch:  11 , batch:  633 , train loss:  19.474393844604492\n","epoch:  11 , batch:  634 , train loss:  13.774433135986328\n","epoch:  11 , batch:  635 , train loss:  20.768836975097656\n","epoch:  11 , batch:  636 , train loss:  13.727447509765625\n","epoch:  11 , batch:  637 , train loss:  10.60006332397461\n","epoch:  11 , batch:  638 , train loss:  17.261844635009766\n","epoch:  11 , batch:  639 , train loss:  9.367928504943848\n","epoch:  11 , batch:  640 , train loss:  19.183303833007812\n","epoch:  11 , batch:  641 , train loss:  18.331979751586914\n","epoch:  11 , batch:  642 , train loss:  11.532585144042969\n","epoch:  11 , batch:  643 , train loss:  22.1700496673584\n","epoch:  11 , batch:  644 , train loss:  10.59592056274414\n","epoch:  11 , batch:  645 , train loss:  21.345691680908203\n","epoch:  11 , batch:  646 , train loss:  17.138214111328125\n","epoch:  11 , batch:  647 , train loss:  25.38841438293457\n","epoch:  11 , batch:  648 , train loss:  12.24108600616455\n","epoch:  11 , batch:  649 , train loss:  12.518974304199219\n","epoch:  11 , batch:  650 , train loss:  8.680831909179688\n","epoch:  11 , batch:  651 , train loss:  13.33205509185791\n","epoch:  11 , batch:  652 , train loss:  18.072694778442383\n","epoch:  11 , batch:  653 , train loss:  6.706325531005859\n","epoch:  11 , batch:  654 , train loss:  17.837038040161133\n","epoch:  11 , batch:  655 , train loss:  25.877233505249023\n","epoch:  11 , batch:  656 , train loss:  16.689741134643555\n","epoch:  11 , batch:  657 , train loss:  20.136703491210938\n","epoch:  11 , batch:  658 , train loss:  8.403420448303223\n","epoch:  11 , batch:  659 , train loss:  14.979826927185059\n","epoch:  11 , batch:  660 , train loss:  26.367294311523438\n","epoch:  11 , batch:  661 , train loss:  25.25736427307129\n","epoch:  11 , batch:  662 , train loss:  22.32247543334961\n","epoch:  11 , batch:  663 , train loss:  12.646693229675293\n","epoch:  11 , batch:  664 , train loss:  21.158605575561523\n","epoch:  11 , batch:  665 , train loss:  21.478273391723633\n","epoch:  11 , batch:  666 , train loss:  11.263040542602539\n","epoch:  11 , batch:  667 , train loss:  30.05910301208496\n","epoch:  11 , batch:  668 , train loss:  18.150665283203125\n","epoch:  11 , batch:  669 , train loss:  16.504430770874023\n","epoch:  11 , batch:  670 , train loss:  15.9368314743042\n","epoch:  11 , batch:  671 , train loss:  28.485532760620117\n","epoch:  11 , batch:  672 , train loss:  13.522899627685547\n","epoch:  11 , batch:  673 , train loss:  16.118366241455078\n","epoch:  11 , batch:  674 , train loss:  12.771055221557617\n","epoch:  11 , batch:  675 , train loss:  11.434440612792969\n","epoch:  11 , batch:  676 , train loss:  22.64227867126465\n","epoch:  11 , batch:  677 , train loss:  16.42313575744629\n","epoch:  11 , batch:  678 , train loss:  15.437207221984863\n","epoch:  11 , batch:  679 , train loss:  18.342260360717773\n","epoch:  11 , batch:  680 , train loss:  15.201423645019531\n","epoch:  11 , batch:  681 , train loss:  16.676158905029297\n","epoch:  11 , batch:  682 , train loss:  16.813037872314453\n","epoch:  11 , batch:  683 , train loss:  17.017423629760742\n","epoch:  11 , batch:  684 , train loss:  18.322406768798828\n","epoch:  11 , batch:  685 , train loss:  8.4353609085083\n","epoch:  11 , batch:  686 , train loss:  18.87774085998535\n","epoch:  11 , batch:  687 , train loss:  22.290279388427734\n","epoch:  11 , batch:  688 , train loss:  15.813124656677246\n","epoch:  11 , batch:  689 , train loss:  16.98118782043457\n","epoch:  11 , batch:  690 , train loss:  16.70555305480957\n","epoch:  11 , batch:  691 , train loss:  20.022157669067383\n","epoch:  11 , batch:  692 , train loss:  15.087892532348633\n","epoch:  11 , batch:  693 , train loss:  12.85070514678955\n","epoch:  11 , batch:  694 , train loss:  18.619253158569336\n","epoch:  11 , batch:  695 , train loss:  9.827709197998047\n","epoch:  11 , batch:  696 , train loss:  24.659761428833008\n","epoch:  11 , batch:  697 , train loss:  9.438166618347168\n","epoch:  11 , batch:  698 , train loss:  10.102459907531738\n","epoch:  11 , batch:  699 , train loss:  10.196372032165527\n","epoch:  11 , batch:  700 , train loss:  8.370277404785156\n","epoch:  11 , batch:  701 , train loss:  12.862295150756836\n","epoch:  11 , batch:  702 , train loss:  14.365880966186523\n","epoch:  11 , batch:  703 , train loss:  12.164324760437012\n","epoch:  11 , batch:  704 , train loss:  9.835675239562988\n","epoch:  11 , batch:  705 , train loss:  10.968191146850586\n","epoch:  11 , batch:  706 , train loss:  14.804048538208008\n","epoch:  11 , batch:  707 , train loss:  38.29332733154297\n","epoch:  11 , batch:  708 , train loss:  18.228845596313477\n","epoch:  11 , batch:  709 , train loss:  32.9942741394043\n","epoch:  11 , batch:  710 , train loss:  27.824495315551758\n","epoch:  11 , batch:  711 , train loss:  11.347332000732422\n","epoch:  11 , batch:  712 , train loss:  19.663471221923828\n","epoch:  11 , batch:  713 , train loss:  10.243410110473633\n","epoch:  11 , batch:  714 , train loss:  19.910367965698242\n","epoch:  11 , batch:  715 , train loss:  15.536161422729492\n","epoch:  11 , batch:  716 , train loss:  15.873397827148438\n","epoch:  11 , batch:  717 , train loss:  21.65358543395996\n","epoch:  11 , batch:  718 , train loss:  11.78443431854248\n","epoch:  11 , batch:  719 , train loss:  22.638469696044922\n","epoch:  11 , batch:  720 , train loss:  24.147891998291016\n","epoch:  11 , batch:  721 , train loss:  12.359816551208496\n","epoch:  11 , batch:  722 , train loss:  13.71855354309082\n","epoch:  11 , batch:  723 , train loss:  15.59598445892334\n","epoch:  11 , batch:  724 , train loss:  17.491870880126953\n","epoch:  11 , batch:  725 , train loss:  13.667977333068848\n","epoch:  11 , batch:  726 , train loss:  16.953479766845703\n","epoch:  11 , batch:  727 , train loss:  13.255936622619629\n","epoch:  11 , batch:  728 , train loss:  19.12204360961914\n","epoch:  11 , batch:  729 , train loss:  13.033910751342773\n","epoch:  11 , batch:  730 , train loss:  12.411565780639648\n","epoch:  11 , batch:  731 , train loss:  24.793243408203125\n","epoch:  11 , batch:  732 , train loss:  13.867661476135254\n","epoch:  11 , batch:  733 , train loss:  11.353203773498535\n","epoch:  11 , batch:  734 , train loss:  21.293508529663086\n","epoch:  11 , batch:  735 , train loss:  15.596009254455566\n","epoch:  11 , batch:  736 , train loss:  14.565953254699707\n","epoch:  11 , batch:  737 , train loss:  19.03533935546875\n","epoch:  11 , batch:  738 , train loss:  25.343219757080078\n","epoch:  11 , batch:  739 , train loss:  15.191459655761719\n","epoch:  11 , batch:  740 , train loss:  23.72233009338379\n","epoch:  11 , batch:  741 , train loss:  19.7861270904541\n","epoch:  11 , batch:  742 , train loss:  24.248865127563477\n","epoch:  11 , batch:  743 , train loss:  15.272292137145996\n","epoch:  11 , batch:  744 , train loss:  7.913636207580566\n","epoch:  11 , batch:  745 , train loss:  8.19747543334961\n","epoch:  11 , batch:  746 , train loss:  31.83007049560547\n","epoch:  11 , batch:  747 , train loss:  19.0959415435791\n","epoch:  11 , batch:  748 , train loss:  14.001108169555664\n","epoch:  11 , batch:  749 , train loss:  20.99915313720703\n","epoch:  11 , batch:  750 , train loss:  19.622655868530273\n","epoch:  11 , batch:  751 , train loss:  21.608686447143555\n","epoch:  11 , batch:  752 , train loss:  12.912062644958496\n","epoch:  11 , batch:  753 , train loss:  18.047178268432617\n","epoch:  11 , batch:  754 , train loss:  27.236841201782227\n","epoch:  11 , batch:  755 , train loss:  14.219782829284668\n","epoch:  11 , batch:  756 , train loss:  15.274590492248535\n","epoch:  11 , batch:  757 , train loss:  16.126323699951172\n","epoch:  11 , batch:  758 , train loss:  15.524938583374023\n","epoch:  11 , batch:  759 , train loss:  10.492475509643555\n","epoch:  11 , batch:  760 , train loss:  7.289402008056641\n","epoch:  11 , batch:  761 , train loss:  18.0424747467041\n","epoch:  11 , batch:  762 , train loss:  17.277050018310547\n","epoch:  11 , batch:  763 , train loss:  12.902174949645996\n","epoch:  11 , batch:  764 , train loss:  12.180272102355957\n","epoch:  11 , batch:  765 , train loss:  19.559072494506836\n","epoch:  11 , batch:  766 , train loss:  23.893903732299805\n","epoch:  11 , batch:  767 , train loss:  17.05014991760254\n","epoch:  11 , batch:  768 , train loss:  10.786280632019043\n","epoch:  11 , batch:  769 , train loss:  16.84316635131836\n","epoch:  11 , batch:  770 , train loss:  24.837692260742188\n","epoch:  11 , batch:  771 , train loss:  23.09389877319336\n","epoch:  11 , batch:  772 , train loss:  12.974298477172852\n","epoch:  11 , batch:  773 , train loss:  18.6007022857666\n","epoch:  11 , batch:  774 , train loss:  12.881707191467285\n","epoch:  11 , batch:  775 , train loss:  14.367862701416016\n","epoch:  11 , batch:  776 , train loss:  18.23973846435547\n","epoch:  11 , batch:  777 , train loss:  8.802661895751953\n","epoch:  11 , batch:  778 , train loss:  12.321189880371094\n","epoch:  11 , batch:  779 , train loss:  12.41201400756836\n","epoch:  11 , batch:  780 , train loss:  10.036200523376465\n","epoch:  11 , batch:  781 , train loss:  14.349567413330078\n","epoch:  11 , batch:  782 , train loss:  8.298213958740234\n","epoch:  11 , batch:  783 , train loss:  13.159080505371094\n","epoch:  11 , batch:  784 , train loss:  17.765941619873047\n","epoch:  11 , batch:  785 , train loss:  10.135601997375488\n","epoch:  11 , batch:  786 , train loss:  15.846397399902344\n","epoch:  11 , batch:  787 , train loss:  9.055302619934082\n","epoch:  11 , batch:  788 , train loss:  11.814911842346191\n","epoch:  11 , batch:  789 , train loss:  15.135048866271973\n","epoch:  11 , batch:  790 , train loss:  25.027172088623047\n","epoch:  11 , batch:  791 , train loss:  8.344792366027832\n","epoch:  11 , batch:  792 , train loss:  18.995132446289062\n","epoch:  11 , batch:  793 , train loss:  20.15205955505371\n","epoch:  11 , batch:  794 , train loss:  22.10036849975586\n","epoch:  11 , batch:  795 , train loss:  25.240833282470703\n","epoch:  11 , batch:  796 , train loss:  20.581628799438477\n","epoch:  11 , batch:  797 , train loss:  12.610353469848633\n","epoch:  11 , batch:  798 , train loss:  8.9147367477417\n","epoch:  11 , batch:  799 , train loss:  14.8101167678833\n","epoch:  11 , batch:  800 , train loss:  22.3464298248291\n","epoch:  11 , batch:  801 , train loss:  16.108261108398438\n","epoch:  11 , batch:  802 , train loss:  28.51774787902832\n","epoch:  11 , batch:  803 , train loss:  16.05279541015625\n","epoch:  11 , batch:  804 , train loss:  10.167091369628906\n","epoch:  11 , batch:  805 , train loss:  7.568434238433838\n","epoch:  11 , batch:  806 , train loss:  17.51551055908203\n","epoch:  11 , batch:  807 , train loss:  26.414113998413086\n","epoch:  11 , batch:  808 , train loss:  21.341825485229492\n","epoch:  11 , batch:  809 , train loss:  21.601030349731445\n","epoch:  11 , batch:  810 , train loss:  10.658785820007324\n","epoch:  11 , batch:  811 , train loss:  20.434404373168945\n","epoch:  11 , batch:  812 , train loss:  16.94519805908203\n","epoch:  11 , batch:  813 , train loss:  9.612257957458496\n","epoch:  11 , batch:  814 , train loss:  14.055035591125488\n","epoch:  11 , batch:  815 , train loss:  12.453170776367188\n","epoch:  11 , batch:  816 , train loss:  8.580198287963867\n","epoch:  11 , batch:  817 , train loss:  7.644824028015137\n","epoch:  11 , batch:  818 , train loss:  18.767601013183594\n","epoch:  11 , batch:  819 , train loss:  21.086769104003906\n","epoch:  11 , batch:  820 , train loss:  15.067508697509766\n","epoch:  11 , batch:  821 , train loss:  9.618730545043945\n","epoch:  11 , batch:  822 , train loss:  17.789350509643555\n","epoch:  11 , batch:  823 , train loss:  22.985410690307617\n","epoch:  11 , batch:  824 , train loss:  26.20818328857422\n","epoch:  11 , batch:  825 , train loss:  15.722164154052734\n","epoch:  11 , batch:  826 , train loss:  28.6737060546875\n","epoch:  11 , batch:  827 , train loss:  17.449726104736328\n","epoch:  11 , batch:  828 , train loss:  24.07546043395996\n","epoch:  11 , batch:  829 , train loss:  18.282272338867188\n","epoch:  11 , batch:  830 , train loss:  8.468986511230469\n","epoch:  11 , batch:  831 , train loss:  13.76988410949707\n","epoch:  11 , batch:  832 , train loss:  8.714343070983887\n","epoch:  11 , batch:  833 , train loss:  7.394959449768066\n","epoch:  11 , batch:  834 , train loss:  12.63233470916748\n","epoch:  11 , batch:  835 , train loss:  19.765066146850586\n","epoch:  11 , batch:  836 , train loss:  23.44036102294922\n","epoch:  11 , batch:  837 , train loss:  16.93675994873047\n","epoch:  11 , batch:  838 , train loss:  15.485849380493164\n","epoch:  11 , batch:  839 , train loss:  14.726410865783691\n","epoch:  11 , batch:  840 , train loss:  11.352428436279297\n","epoch:  11 , batch:  841 , train loss:  10.872519493103027\n","epoch:  11 , batch:  842 , train loss:  20.425739288330078\n","epoch:  11 , batch:  843 , train loss:  10.166275978088379\n","epoch:  11 , batch:  844 , train loss:  10.188078880310059\n","epoch:  11 , batch:  845 , train loss:  21.916316986083984\n","epoch:  11 , batch:  846 , train loss:  19.527070999145508\n","epoch:  11 , batch:  847 , train loss:  8.787727355957031\n","epoch:  11 , batch:  848 , train loss:  13.15588092803955\n","epoch:  11 , batch:  849 , train loss:  15.685911178588867\n","epoch:  11 , batch:  850 , train loss:  12.713093757629395\n","epoch:  11 , batch:  851 , train loss:  9.977921485900879\n","epoch:  11 , batch:  852 , train loss:  19.971616744995117\n","epoch:  11 , batch:  853 , train loss:  22.83835220336914\n","epoch:  11 , batch:  854 , train loss:  17.938417434692383\n","epoch:  11 , batch:  855 , train loss:  20.78818702697754\n","epoch:  11 , batch:  856 , train loss:  5.654626846313477\n","epoch:  11 , batch:  857 , train loss:  15.020994186401367\n","epoch:  11 , batch:  858 , train loss:  14.782193183898926\n","epoch:  11 , batch:  859 , train loss:  14.78100872039795\n","epoch:  11 , batch:  860 , train loss:  24.376665115356445\n","epoch:  11 , batch:  861 , train loss:  22.539430618286133\n","epoch:  11 , batch:  862 , train loss:  15.529523849487305\n","epoch:  11 , batch:  863 , train loss:  13.68766975402832\n","epoch:  11 , batch:  864 , train loss:  12.076007843017578\n","epoch:  11 , batch:  865 , train loss:  9.350022315979004\n","epoch:  11 , batch:  866 , train loss:  12.159903526306152\n","epoch:  11 , batch:  867 , train loss:  16.09760856628418\n","epoch:  11 , batch:  868 , train loss:  18.37980842590332\n","epoch:  11 , batch:  869 , train loss:  15.957988739013672\n","epoch:  11 , batch:  870 , train loss:  20.943761825561523\n","epoch:  11 , batch:  871 , train loss:  17.09848403930664\n","epoch:  11 , batch:  872 , train loss:  16.596956253051758\n","epoch:  11 , batch:  873 , train loss:  17.517499923706055\n","epoch:  11 , batch:  874 , train loss:  14.756572723388672\n","epoch:  11 , batch:  875 , train loss:  15.248270034790039\n","epoch:  11 , batch:  876 , train loss:  18.869298934936523\n","epoch:  11 , batch:  877 , train loss:  19.97184944152832\n","epoch:  11 , batch:  878 , train loss:  12.041807174682617\n","epoch:  11 , batch:  879 , train loss:  20.383146286010742\n","epoch:  11 , batch:  880 , train loss:  18.457731246948242\n","epoch:  11 , batch:  881 , train loss:  19.465408325195312\n","epoch:  11 , batch:  882 , train loss:  22.188892364501953\n","epoch:  11 , batch:  883 , train loss:  12.690019607543945\n","epoch:  11 , batch:  884 , train loss:  11.730557441711426\n","epoch:  11 , batch:  885 , train loss:  23.619678497314453\n","epoch:  11 , batch:  886 , train loss:  18.34231185913086\n","epoch:  11 , batch:  887 , train loss:  21.87671661376953\n","epoch:  11 , batch:  888 , train loss:  9.506636619567871\n","epoch:  11 , batch:  889 , train loss:  14.601134300231934\n","epoch:  11 , batch:  890 , train loss:  3.9704816341400146\n","epoch:  11 , batch:  891 , train loss:  23.50572395324707\n","epoch:  11 , batch:  892 , train loss:  8.052478790283203\n","epoch:  11 , batch:  893 , train loss:  10.756421089172363\n","epoch:  11 , batch:  894 , train loss:  9.153155326843262\n","epoch:  11 , batch:  895 , train loss:  11.788931846618652\n","epoch:  11 , batch:  896 , train loss:  18.508705139160156\n","epoch:  11 , batch:  897 , train loss:  19.35553550720215\n","epoch:  11 , batch:  898 , train loss:  19.98084831237793\n","epoch:  11 , batch:  899 , train loss:  10.613931655883789\n","epoch:  11 , batch:  900 , train loss:  14.334972381591797\n","epoch:  11 , batch:  901 , train loss:  13.372970581054688\n","epoch:  11 , batch:  902 , train loss:  21.095396041870117\n","epoch:  11 , batch:  903 , train loss:  19.013975143432617\n","epoch:  11 , batch:  904 , train loss:  17.31039810180664\n","epoch:  11 , batch:  905 , train loss:  14.426724433898926\n","epoch:  11 , batch:  906 , train loss:  15.934243202209473\n","epoch:  11 , batch:  907 , train loss:  20.8604679107666\n","epoch:  11 , batch:  908 , train loss:  15.362308502197266\n","epoch:  11 , batch:  909 , train loss:  13.979723930358887\n","epoch:  11 , batch:  910 , train loss:  14.416729927062988\n","epoch:  11 , batch:  911 , train loss:  10.322440147399902\n","epoch:  11 , batch:  912 , train loss:  24.551603317260742\n","epoch:  11 , batch:  913 , train loss:  17.945262908935547\n","epoch:  11 , batch:  914 , train loss:  18.310400009155273\n","epoch:  11 , batch:  915 , train loss:  6.628216743469238\n","epoch:  11 , batch:  916 , train loss:  23.136768341064453\n","epoch:  11 , batch:  917 , train loss:  15.61775016784668\n","epoch:  11 , batch:  918 , train loss:  25.17794418334961\n","epoch:  11 , batch:  919 , train loss:  13.442486763000488\n","epoch:  11 , batch:  920 , train loss:  18.82976722717285\n","epoch:  11 , batch:  921 , train loss:  18.642837524414062\n","epoch:  11 , batch:  922 , train loss:  15.121691703796387\n","epoch:  11 , batch:  923 , train loss:  9.762353897094727\n","epoch:  11 , batch:  924 , train loss:  22.22846221923828\n","epoch:  11 , batch:  925 , train loss:  7.973663806915283\n","epoch:  11 , batch:  926 , train loss:  12.55777645111084\n","epoch:  11 , batch:  927 , train loss:  16.97750473022461\n","epoch:  11 , batch:  928 , train loss:  13.039301872253418\n","epoch:  11 , batch:  929 , train loss:  14.028915405273438\n","epoch:  11 , batch:  930 , train loss:  10.046701431274414\n","epoch:  11 , batch:  931 , train loss:  13.365296363830566\n","epoch:  11 , batch:  932 , train loss:  8.982202529907227\n","epoch:  11 , batch:  933 , train loss:  17.344587326049805\n","epoch:  11 , batch:  934 , train loss:  10.918699264526367\n","epoch:  11 , batch:  935 , train loss:  24.781667709350586\n","epoch:  11 , batch:  936 , train loss:  10.40607738494873\n","epoch:  11 , batch:  937 , train loss:  11.860084533691406\n","Accuracy of train set: 0.9062166666666667\n","epoch:  11 , batch:  0 , test loss:  36.821678161621094\n","epoch:  11 , batch:  1 , test loss:  20.25619125366211\n","epoch:  11 , batch:  2 , test loss:  33.10786819458008\n","epoch:  11 , batch:  3 , test loss:  22.457544326782227\n","epoch:  11 , batch:  4 , test loss:  26.51236915588379\n","epoch:  11 , batch:  5 , test loss:  33.16022872924805\n","epoch:  11 , batch:  6 , test loss:  21.983455657958984\n","epoch:  11 , batch:  7 , test loss:  21.90529441833496\n","epoch:  11 , batch:  8 , test loss:  21.10671043395996\n","epoch:  11 , batch:  9 , test loss:  14.500429153442383\n","epoch:  11 , batch:  10 , test loss:  14.543791770935059\n","epoch:  11 , batch:  11 , test loss:  26.807214736938477\n","epoch:  11 , batch:  12 , test loss:  25.769426345825195\n","epoch:  11 , batch:  13 , test loss:  35.70853042602539\n","epoch:  11 , batch:  14 , test loss:  18.185245513916016\n","epoch:  11 , batch:  15 , test loss:  18.75934600830078\n","epoch:  11 , batch:  16 , test loss:  26.620004653930664\n","epoch:  11 , batch:  17 , test loss:  16.466394424438477\n","epoch:  11 , batch:  18 , test loss:  30.394977569580078\n","epoch:  11 , batch:  19 , test loss:  25.57822608947754\n","epoch:  11 , batch:  20 , test loss:  28.90825653076172\n","epoch:  11 , batch:  21 , test loss:  32.08603286743164\n","epoch:  11 , batch:  22 , test loss:  28.93630599975586\n","epoch:  11 , batch:  23 , test loss:  22.640600204467773\n","epoch:  11 , batch:  24 , test loss:  20.94021987915039\n","epoch:  11 , batch:  25 , test loss:  26.47247314453125\n","epoch:  11 , batch:  26 , test loss:  26.452848434448242\n","epoch:  11 , batch:  27 , test loss:  15.045334815979004\n","epoch:  11 , batch:  28 , test loss:  14.856657981872559\n","epoch:  11 , batch:  29 , test loss:  15.224370002746582\n","epoch:  11 , batch:  30 , test loss:  14.81717300415039\n","epoch:  11 , batch:  31 , test loss:  21.284025192260742\n","epoch:  11 , batch:  32 , test loss:  27.044754028320312\n","epoch:  11 , batch:  33 , test loss:  29.257427215576172\n","epoch:  11 , batch:  34 , test loss:  20.660770416259766\n","epoch:  11 , batch:  35 , test loss:  25.158720016479492\n","epoch:  11 , batch:  36 , test loss:  15.695440292358398\n","epoch:  11 , batch:  37 , test loss:  21.880197525024414\n","epoch:  11 , batch:  38 , test loss:  13.296656608581543\n","epoch:  11 , batch:  39 , test loss:  13.337515830993652\n","epoch:  11 , batch:  40 , test loss:  14.228317260742188\n","epoch:  11 , batch:  41 , test loss:  22.48128890991211\n","epoch:  11 , batch:  42 , test loss:  16.164798736572266\n","epoch:  11 , batch:  43 , test loss:  13.17595386505127\n","epoch:  11 , batch:  44 , test loss:  16.28111457824707\n","epoch:  11 , batch:  45 , test loss:  30.917470932006836\n","epoch:  11 , batch:  46 , test loss:  25.503936767578125\n","epoch:  11 , batch:  47 , test loss:  28.843660354614258\n","epoch:  11 , batch:  48 , test loss:  20.541168212890625\n","epoch:  11 , batch:  49 , test loss:  28.03339385986328\n","epoch:  11 , batch:  50 , test loss:  25.2392578125\n","epoch:  11 , batch:  51 , test loss:  12.938995361328125\n","epoch:  11 , batch:  52 , test loss:  25.67027473449707\n","epoch:  11 , batch:  53 , test loss:  20.206647872924805\n","epoch:  11 , batch:  54 , test loss:  35.86835479736328\n","epoch:  11 , batch:  55 , test loss:  16.985166549682617\n","epoch:  11 , batch:  56 , test loss:  39.845848083496094\n","epoch:  11 , batch:  57 , test loss:  17.877317428588867\n","epoch:  11 , batch:  58 , test loss:  14.558541297912598\n","epoch:  11 , batch:  59 , test loss:  32.59146499633789\n","epoch:  11 , batch:  60 , test loss:  15.523653984069824\n","epoch:  11 , batch:  61 , test loss:  15.638675689697266\n","epoch:  11 , batch:  62 , test loss:  24.43425941467285\n","epoch:  11 , batch:  63 , test loss:  19.88768768310547\n","epoch:  11 , batch:  64 , test loss:  16.774538040161133\n","epoch:  11 , batch:  65 , test loss:  19.40961456298828\n","epoch:  11 , batch:  66 , test loss:  20.953054428100586\n","epoch:  11 , batch:  67 , test loss:  15.128494262695312\n","epoch:  11 , batch:  68 , test loss:  27.101009368896484\n","epoch:  11 , batch:  69 , test loss:  19.409292221069336\n","epoch:  11 , batch:  70 , test loss:  16.515958786010742\n","epoch:  11 , batch:  71 , test loss:  21.072452545166016\n","epoch:  11 , batch:  72 , test loss:  27.949600219726562\n","epoch:  11 , batch:  73 , test loss:  23.539636611938477\n","epoch:  11 , batch:  74 , test loss:  26.6400203704834\n","epoch:  11 , batch:  75 , test loss:  32.214237213134766\n","epoch:  11 , batch:  76 , test loss:  33.39632034301758\n","epoch:  11 , batch:  77 , test loss:  9.1233549118042\n","epoch:  11 , batch:  78 , test loss:  22.947086334228516\n","epoch:  11 , batch:  79 , test loss:  15.745694160461426\n","epoch:  11 , batch:  80 , test loss:  21.499847412109375\n","epoch:  11 , batch:  81 , test loss:  16.214725494384766\n","epoch:  11 , batch:  82 , test loss:  31.11549186706543\n","epoch:  11 , batch:  83 , test loss:  32.96238708496094\n","epoch:  11 , batch:  84 , test loss:  42.445709228515625\n","epoch:  11 , batch:  85 , test loss:  18.640792846679688\n","epoch:  11 , batch:  86 , test loss:  19.85806655883789\n","epoch:  11 , batch:  87 , test loss:  25.027114868164062\n","epoch:  11 , batch:  88 , test loss:  23.248733520507812\n","epoch:  11 , batch:  89 , test loss:  18.29723358154297\n","epoch:  11 , batch:  90 , test loss:  31.45954132080078\n","epoch:  11 , batch:  91 , test loss:  23.999561309814453\n","epoch:  11 , batch:  92 , test loss:  14.149898529052734\n","epoch:  11 , batch:  93 , test loss:  18.9688777923584\n","epoch:  11 , batch:  94 , test loss:  31.712581634521484\n","epoch:  11 , batch:  95 , test loss:  22.52168083190918\n","epoch:  11 , batch:  96 , test loss:  29.796302795410156\n","epoch:  11 , batch:  97 , test loss:  22.211389541625977\n","epoch:  11 , batch:  98 , test loss:  20.0293025970459\n","epoch:  11 , batch:  99 , test loss:  23.734167098999023\n","epoch:  11 , batch:  100 , test loss:  28.62023162841797\n","epoch:  11 , batch:  101 , test loss:  24.587923049926758\n","epoch:  11 , batch:  102 , test loss:  20.207237243652344\n","epoch:  11 , batch:  103 , test loss:  28.43694496154785\n","epoch:  11 , batch:  104 , test loss:  23.859405517578125\n","epoch:  11 , batch:  105 , test loss:  17.448223114013672\n","epoch:  11 , batch:  106 , test loss:  16.31629180908203\n","epoch:  11 , batch:  107 , test loss:  16.63808250427246\n","epoch:  11 , batch:  108 , test loss:  15.555384635925293\n","epoch:  11 , batch:  109 , test loss:  12.741154670715332\n","epoch:  11 , batch:  110 , test loss:  25.771501541137695\n","epoch:  11 , batch:  111 , test loss:  23.713525772094727\n","epoch:  11 , batch:  112 , test loss:  25.433908462524414\n","epoch:  11 , batch:  113 , test loss:  19.076557159423828\n","epoch:  11 , batch:  114 , test loss:  17.736780166625977\n","epoch:  11 , batch:  115 , test loss:  20.46187400817871\n","epoch:  11 , batch:  116 , test loss:  12.983908653259277\n","epoch:  11 , batch:  117 , test loss:  16.82373809814453\n","epoch:  11 , batch:  118 , test loss:  21.226322174072266\n","epoch:  11 , batch:  119 , test loss:  12.368996620178223\n","epoch:  11 , batch:  120 , test loss:  23.308137893676758\n","epoch:  11 , batch:  121 , test loss:  44.38337707519531\n","epoch:  11 , batch:  122 , test loss:  14.449385643005371\n","epoch:  11 , batch:  123 , test loss:  32.5598030090332\n","epoch:  11 , batch:  124 , test loss:  32.8507080078125\n","epoch:  11 , batch:  125 , test loss:  26.314071655273438\n","epoch:  11 , batch:  126 , test loss:  20.888795852661133\n","epoch:  11 , batch:  127 , test loss:  19.579750061035156\n","epoch:  11 , batch:  128 , test loss:  22.12129020690918\n","epoch:  11 , batch:  129 , test loss:  42.27048873901367\n","epoch:  11 , batch:  130 , test loss:  15.25070858001709\n","epoch:  11 , batch:  131 , test loss:  16.150285720825195\n","epoch:  11 , batch:  132 , test loss:  13.698725700378418\n","epoch:  11 , batch:  133 , test loss:  36.38069152832031\n","epoch:  11 , batch:  134 , test loss:  21.534425735473633\n","epoch:  11 , batch:  135 , test loss:  22.120988845825195\n","epoch:  11 , batch:  136 , test loss:  19.476160049438477\n","epoch:  11 , batch:  137 , test loss:  12.94731616973877\n","epoch:  11 , batch:  138 , test loss:  16.768259048461914\n","epoch:  11 , batch:  139 , test loss:  31.527061462402344\n","epoch:  11 , batch:  140 , test loss:  13.309248924255371\n","epoch:  11 , batch:  141 , test loss:  25.66599464416504\n","epoch:  11 , batch:  142 , test loss:  23.808313369750977\n","epoch:  11 , batch:  143 , test loss:  26.715517044067383\n","epoch:  11 , batch:  144 , test loss:  22.072254180908203\n","epoch:  11 , batch:  145 , test loss:  17.427797317504883\n","epoch:  11 , batch:  146 , test loss:  38.52351379394531\n","epoch:  11 , batch:  147 , test loss:  32.4233512878418\n","epoch:  11 , batch:  148 , test loss:  25.24113655090332\n","epoch:  11 , batch:  149 , test loss:  28.17323875427246\n","epoch:  11 , batch:  150 , test loss:  32.645626068115234\n","epoch:  11 , batch:  151 , test loss:  17.27741813659668\n","epoch:  11 , batch:  152 , test loss:  20.434505462646484\n","epoch:  11 , batch:  153 , test loss:  25.812484741210938\n","epoch:  11 , batch:  154 , test loss:  15.59708309173584\n","epoch:  11 , batch:  155 , test loss:  24.05324363708496\n","epoch:  11 , batch:  156 , test loss:  2.3423051834106445\n","Accuracy of pytorch_model set: 0.8714\n","epoch:  12 , batch:  0 , train loss:  14.957449913024902\n","epoch:  12 , batch:  1 , train loss:  17.169496536254883\n","epoch:  12 , batch:  2 , train loss:  13.131338119506836\n","epoch:  12 , batch:  3 , train loss:  12.968207359313965\n","epoch:  12 , batch:  4 , train loss:  9.066740989685059\n","epoch:  12 , batch:  5 , train loss:  20.576852798461914\n","epoch:  12 , batch:  6 , train loss:  9.902185440063477\n","epoch:  12 , batch:  7 , train loss:  26.867584228515625\n","epoch:  12 , batch:  8 , train loss:  12.835020065307617\n","epoch:  12 , batch:  9 , train loss:  13.558097839355469\n","epoch:  12 , batch:  10 , train loss:  17.165973663330078\n","epoch:  12 , batch:  11 , train loss:  16.236099243164062\n","epoch:  12 , batch:  12 , train loss:  19.790258407592773\n","epoch:  12 , batch:  13 , train loss:  9.571857452392578\n","epoch:  12 , batch:  14 , train loss:  24.569900512695312\n","epoch:  12 , batch:  15 , train loss:  16.11063575744629\n","epoch:  12 , batch:  16 , train loss:  10.48168659210205\n","epoch:  12 , batch:  17 , train loss:  10.125393867492676\n","epoch:  12 , batch:  18 , train loss:  15.900925636291504\n","epoch:  12 , batch:  19 , train loss:  17.92864227294922\n","epoch:  12 , batch:  20 , train loss:  15.638906478881836\n","epoch:  12 , batch:  21 , train loss:  7.604259490966797\n","epoch:  12 , batch:  22 , train loss:  20.278667449951172\n","epoch:  12 , batch:  23 , train loss:  10.495987892150879\n","epoch:  12 , batch:  24 , train loss:  10.799346923828125\n","epoch:  12 , batch:  25 , train loss:  22.827903747558594\n","epoch:  12 , batch:  26 , train loss:  7.529080390930176\n","epoch:  12 , batch:  27 , train loss:  13.656510353088379\n","epoch:  12 , batch:  28 , train loss:  13.131725311279297\n","epoch:  12 , batch:  29 , train loss:  16.672740936279297\n","epoch:  12 , batch:  30 , train loss:  19.63294792175293\n","epoch:  12 , batch:  31 , train loss:  16.805177688598633\n","epoch:  12 , batch:  32 , train loss:  17.687761306762695\n","epoch:  12 , batch:  33 , train loss:  27.450363159179688\n","epoch:  12 , batch:  34 , train loss:  11.267602920532227\n","epoch:  12 , batch:  35 , train loss:  15.663244247436523\n","epoch:  12 , batch:  36 , train loss:  13.736225128173828\n","epoch:  12 , batch:  37 , train loss:  11.8662109375\n","epoch:  12 , batch:  38 , train loss:  7.600525379180908\n","epoch:  12 , batch:  39 , train loss:  18.13772201538086\n","epoch:  12 , batch:  40 , train loss:  18.54560661315918\n","epoch:  12 , batch:  41 , train loss:  12.651413917541504\n","epoch:  12 , batch:  42 , train loss:  15.762486457824707\n","epoch:  12 , batch:  43 , train loss:  13.78997802734375\n","epoch:  12 , batch:  44 , train loss:  12.245448112487793\n","epoch:  12 , batch:  45 , train loss:  14.413217544555664\n","epoch:  12 , batch:  46 , train loss:  9.35002613067627\n","epoch:  12 , batch:  47 , train loss:  18.98651885986328\n","epoch:  12 , batch:  48 , train loss:  12.474922180175781\n","epoch:  12 , batch:  49 , train loss:  30.287485122680664\n","epoch:  12 , batch:  50 , train loss:  8.193933486938477\n","epoch:  12 , batch:  51 , train loss:  24.56792640686035\n","epoch:  12 , batch:  52 , train loss:  12.971946716308594\n","epoch:  12 , batch:  53 , train loss:  12.959630966186523\n","epoch:  12 , batch:  54 , train loss:  11.323507308959961\n","epoch:  12 , batch:  55 , train loss:  17.76915168762207\n","epoch:  12 , batch:  56 , train loss:  16.08409309387207\n","epoch:  12 , batch:  57 , train loss:  17.224353790283203\n","epoch:  12 , batch:  58 , train loss:  17.016786575317383\n","epoch:  12 , batch:  59 , train loss:  13.796661376953125\n","epoch:  12 , batch:  60 , train loss:  8.993718147277832\n","epoch:  12 , batch:  61 , train loss:  13.45976448059082\n","epoch:  12 , batch:  62 , train loss:  16.68338394165039\n","epoch:  12 , batch:  63 , train loss:  16.83121681213379\n","epoch:  12 , batch:  64 , train loss:  8.237531661987305\n","epoch:  12 , batch:  65 , train loss:  17.897098541259766\n","epoch:  12 , batch:  66 , train loss:  15.478327751159668\n","epoch:  12 , batch:  67 , train loss:  20.395484924316406\n","epoch:  12 , batch:  68 , train loss:  14.19934368133545\n","epoch:  12 , batch:  69 , train loss:  7.743768215179443\n","epoch:  12 , batch:  70 , train loss:  14.74377155303955\n","epoch:  12 , batch:  71 , train loss:  6.866210460662842\n","epoch:  12 , batch:  72 , train loss:  13.72195816040039\n","epoch:  12 , batch:  73 , train loss:  8.200019836425781\n","epoch:  12 , batch:  74 , train loss:  7.52962589263916\n","epoch:  12 , batch:  75 , train loss:  11.283726692199707\n","epoch:  12 , batch:  76 , train loss:  9.124942779541016\n","epoch:  12 , batch:  77 , train loss:  17.10790252685547\n","epoch:  12 , batch:  78 , train loss:  14.141985893249512\n","epoch:  12 , batch:  79 , train loss:  15.682021141052246\n","epoch:  12 , batch:  80 , train loss:  13.415055274963379\n","epoch:  12 , batch:  81 , train loss:  7.341206073760986\n","epoch:  12 , batch:  82 , train loss:  13.89675235748291\n","epoch:  12 , batch:  83 , train loss:  32.84213638305664\n","epoch:  12 , batch:  84 , train loss:  14.100086212158203\n","epoch:  12 , batch:  85 , train loss:  18.30868911743164\n","epoch:  12 , batch:  86 , train loss:  11.521782875061035\n","epoch:  12 , batch:  87 , train loss:  15.493901252746582\n","epoch:  12 , batch:  88 , train loss:  20.577478408813477\n","epoch:  12 , batch:  89 , train loss:  21.045536041259766\n","epoch:  12 , batch:  90 , train loss:  15.839412689208984\n","epoch:  12 , batch:  91 , train loss:  6.540863990783691\n","epoch:  12 , batch:  92 , train loss:  16.371145248413086\n","epoch:  12 , batch:  93 , train loss:  12.4719877243042\n","epoch:  12 , batch:  94 , train loss:  28.656917572021484\n","epoch:  12 , batch:  95 , train loss:  25.124107360839844\n","epoch:  12 , batch:  96 , train loss:  18.378555297851562\n","epoch:  12 , batch:  97 , train loss:  13.686755180358887\n","epoch:  12 , batch:  98 , train loss:  17.446537017822266\n","epoch:  12 , batch:  99 , train loss:  20.400012969970703\n","epoch:  12 , batch:  100 , train loss:  15.582771301269531\n","epoch:  12 , batch:  101 , train loss:  14.888315200805664\n","epoch:  12 , batch:  102 , train loss:  21.37397003173828\n","epoch:  12 , batch:  103 , train loss:  17.894346237182617\n","epoch:  12 , batch:  104 , train loss:  25.667091369628906\n","epoch:  12 , batch:  105 , train loss:  25.13747787475586\n","epoch:  12 , batch:  106 , train loss:  16.50637435913086\n","epoch:  12 , batch:  107 , train loss:  18.070886611938477\n","epoch:  12 , batch:  108 , train loss:  25.508886337280273\n","epoch:  12 , batch:  109 , train loss:  11.67041015625\n","epoch:  12 , batch:  110 , train loss:  27.65164566040039\n","epoch:  12 , batch:  111 , train loss:  12.983795166015625\n","epoch:  12 , batch:  112 , train loss:  9.103442192077637\n","epoch:  12 , batch:  113 , train loss:  17.355510711669922\n","epoch:  12 , batch:  114 , train loss:  9.54244613647461\n","epoch:  12 , batch:  115 , train loss:  18.524810791015625\n","epoch:  12 , batch:  116 , train loss:  12.456932067871094\n","epoch:  12 , batch:  117 , train loss:  13.316530227661133\n","epoch:  12 , batch:  118 , train loss:  10.62088394165039\n","epoch:  12 , batch:  119 , train loss:  14.75151538848877\n","epoch:  12 , batch:  120 , train loss:  7.156471252441406\n","epoch:  12 , batch:  121 , train loss:  11.353536605834961\n","epoch:  12 , batch:  122 , train loss:  16.72958755493164\n","epoch:  12 , batch:  123 , train loss:  12.94430923461914\n","epoch:  12 , batch:  124 , train loss:  11.761744499206543\n","epoch:  12 , batch:  125 , train loss:  11.232037544250488\n","epoch:  12 , batch:  126 , train loss:  10.972814559936523\n","epoch:  12 , batch:  127 , train loss:  24.33888053894043\n","epoch:  12 , batch:  128 , train loss:  5.28587532043457\n","epoch:  12 , batch:  129 , train loss:  21.207843780517578\n","epoch:  12 , batch:  130 , train loss:  9.630257606506348\n","epoch:  12 , batch:  131 , train loss:  19.101224899291992\n","epoch:  12 , batch:  132 , train loss:  23.407411575317383\n","epoch:  12 , batch:  133 , train loss:  12.424771308898926\n","epoch:  12 , batch:  134 , train loss:  10.734150886535645\n","epoch:  12 , batch:  135 , train loss:  9.032830238342285\n","epoch:  12 , batch:  136 , train loss:  6.99426794052124\n","epoch:  12 , batch:  137 , train loss:  18.2869815826416\n","epoch:  12 , batch:  138 , train loss:  13.0081787109375\n","epoch:  12 , batch:  139 , train loss:  11.02383041381836\n","epoch:  12 , batch:  140 , train loss:  15.586084365844727\n","epoch:  12 , batch:  141 , train loss:  5.750428199768066\n","epoch:  12 , batch:  142 , train loss:  18.853519439697266\n","epoch:  12 , batch:  143 , train loss:  14.759821891784668\n","epoch:  12 , batch:  144 , train loss:  10.533622741699219\n","epoch:  12 , batch:  145 , train loss:  13.917364120483398\n","epoch:  12 , batch:  146 , train loss:  30.602998733520508\n","epoch:  12 , batch:  147 , train loss:  17.469316482543945\n","epoch:  12 , batch:  148 , train loss:  10.943166732788086\n","epoch:  12 , batch:  149 , train loss:  19.421138763427734\n","epoch:  12 , batch:  150 , train loss:  12.003192901611328\n","epoch:  12 , batch:  151 , train loss:  25.068775177001953\n","epoch:  12 , batch:  152 , train loss:  27.91115951538086\n","epoch:  12 , batch:  153 , train loss:  19.594343185424805\n","epoch:  12 , batch:  154 , train loss:  15.187051773071289\n","epoch:  12 , batch:  155 , train loss:  28.14425277709961\n","epoch:  12 , batch:  156 , train loss:  21.66168785095215\n","epoch:  12 , batch:  157 , train loss:  10.420145988464355\n","epoch:  12 , batch:  158 , train loss:  20.280241012573242\n","epoch:  12 , batch:  159 , train loss:  7.486420154571533\n","epoch:  12 , batch:  160 , train loss:  12.178128242492676\n","epoch:  12 , batch:  161 , train loss:  8.319025993347168\n","epoch:  12 , batch:  162 , train loss:  15.798569679260254\n","epoch:  12 , batch:  163 , train loss:  12.303483009338379\n","epoch:  12 , batch:  164 , train loss:  5.553699493408203\n","epoch:  12 , batch:  165 , train loss:  24.97467803955078\n","epoch:  12 , batch:  166 , train loss:  16.293560028076172\n","epoch:  12 , batch:  167 , train loss:  22.74517822265625\n","epoch:  12 , batch:  168 , train loss:  21.249900817871094\n","epoch:  12 , batch:  169 , train loss:  11.701430320739746\n","epoch:  12 , batch:  170 , train loss:  15.584752082824707\n","epoch:  12 , batch:  171 , train loss:  13.286099433898926\n","epoch:  12 , batch:  172 , train loss:  8.82243824005127\n","epoch:  12 , batch:  173 , train loss:  20.12497329711914\n","epoch:  12 , batch:  174 , train loss:  14.969602584838867\n","epoch:  12 , batch:  175 , train loss:  18.571735382080078\n","epoch:  12 , batch:  176 , train loss:  32.11732482910156\n","epoch:  12 , batch:  177 , train loss:  8.267048835754395\n","epoch:  12 , batch:  178 , train loss:  12.280582427978516\n","epoch:  12 , batch:  179 , train loss:  26.263221740722656\n","epoch:  12 , batch:  180 , train loss:  17.654268264770508\n","epoch:  12 , batch:  181 , train loss:  13.302566528320312\n","epoch:  12 , batch:  182 , train loss:  12.547100067138672\n","epoch:  12 , batch:  183 , train loss:  10.698933601379395\n","epoch:  12 , batch:  184 , train loss:  15.543797492980957\n","epoch:  12 , batch:  185 , train loss:  23.2429141998291\n","epoch:  12 , batch:  186 , train loss:  16.399171829223633\n","epoch:  12 , batch:  187 , train loss:  10.005495071411133\n","epoch:  12 , batch:  188 , train loss:  16.051481246948242\n","epoch:  12 , batch:  189 , train loss:  18.893245697021484\n","epoch:  12 , batch:  190 , train loss:  21.53124237060547\n","epoch:  12 , batch:  191 , train loss:  27.2930850982666\n","epoch:  12 , batch:  192 , train loss:  21.016597747802734\n","epoch:  12 , batch:  193 , train loss:  16.75127410888672\n","epoch:  12 , batch:  194 , train loss:  20.36704444885254\n","epoch:  12 , batch:  195 , train loss:  15.147507667541504\n","epoch:  12 , batch:  196 , train loss:  13.891600608825684\n","epoch:  12 , batch:  197 , train loss:  23.521099090576172\n","epoch:  12 , batch:  198 , train loss:  13.855300903320312\n","epoch:  12 , batch:  199 , train loss:  13.877570152282715\n","epoch:  12 , batch:  200 , train loss:  18.523540496826172\n","epoch:  12 , batch:  201 , train loss:  20.69692039489746\n","epoch:  12 , batch:  202 , train loss:  13.56478214263916\n","epoch:  12 , batch:  203 , train loss:  9.845516204833984\n","epoch:  12 , batch:  204 , train loss:  11.192689895629883\n","epoch:  12 , batch:  205 , train loss:  13.090182304382324\n","epoch:  12 , batch:  206 , train loss:  10.87097454071045\n","epoch:  12 , batch:  207 , train loss:  12.315241813659668\n","epoch:  12 , batch:  208 , train loss:  16.64877700805664\n","epoch:  12 , batch:  209 , train loss:  19.13207244873047\n","epoch:  12 , batch:  210 , train loss:  12.488186836242676\n","epoch:  12 , batch:  211 , train loss:  12.380387306213379\n","epoch:  12 , batch:  212 , train loss:  20.838459014892578\n","epoch:  12 , batch:  213 , train loss:  24.164325714111328\n","epoch:  12 , batch:  214 , train loss:  17.097021102905273\n","epoch:  12 , batch:  215 , train loss:  10.435003280639648\n","epoch:  12 , batch:  216 , train loss:  15.289538383483887\n","epoch:  12 , batch:  217 , train loss:  11.252585411071777\n","epoch:  12 , batch:  218 , train loss:  17.389793395996094\n","epoch:  12 , batch:  219 , train loss:  14.004549026489258\n","epoch:  12 , batch:  220 , train loss:  11.884195327758789\n","epoch:  12 , batch:  221 , train loss:  6.489837646484375\n","epoch:  12 , batch:  222 , train loss:  22.872161865234375\n","epoch:  12 , batch:  223 , train loss:  11.357866287231445\n","epoch:  12 , batch:  224 , train loss:  10.229260444641113\n","epoch:  12 , batch:  225 , train loss:  16.135114669799805\n","epoch:  12 , batch:  226 , train loss:  9.415735244750977\n","epoch:  12 , batch:  227 , train loss:  24.494041442871094\n","epoch:  12 , batch:  228 , train loss:  23.24382781982422\n","epoch:  12 , batch:  229 , train loss:  9.871747016906738\n","epoch:  12 , batch:  230 , train loss:  16.384248733520508\n","epoch:  12 , batch:  231 , train loss:  11.158134460449219\n","epoch:  12 , batch:  232 , train loss:  17.15793800354004\n","epoch:  12 , batch:  233 , train loss:  31.259075164794922\n","epoch:  12 , batch:  234 , train loss:  17.600059509277344\n","epoch:  12 , batch:  235 , train loss:  12.74523639678955\n","epoch:  12 , batch:  236 , train loss:  26.917613983154297\n","epoch:  12 , batch:  237 , train loss:  34.81856155395508\n","epoch:  12 , batch:  238 , train loss:  67.94200897216797\n","epoch:  12 , batch:  239 , train loss:  26.029052734375\n","epoch:  12 , batch:  240 , train loss:  10.938864707946777\n","epoch:  12 , batch:  241 , train loss:  19.168384552001953\n","epoch:  12 , batch:  242 , train loss:  14.684443473815918\n","epoch:  12 , batch:  243 , train loss:  6.4443745613098145\n","epoch:  12 , batch:  244 , train loss:  22.359331130981445\n","epoch:  12 , batch:  245 , train loss:  10.087533950805664\n","epoch:  12 , batch:  246 , train loss:  13.477922439575195\n","epoch:  12 , batch:  247 , train loss:  21.797069549560547\n","epoch:  12 , batch:  248 , train loss:  20.621965408325195\n","epoch:  12 , batch:  249 , train loss:  11.245748519897461\n","epoch:  12 , batch:  250 , train loss:  14.002412796020508\n","epoch:  12 , batch:  251 , train loss:  17.755456924438477\n","epoch:  12 , batch:  252 , train loss:  14.68282699584961\n","epoch:  12 , batch:  253 , train loss:  21.703689575195312\n","epoch:  12 , batch:  254 , train loss:  13.975931167602539\n","epoch:  12 , batch:  255 , train loss:  11.183610916137695\n","epoch:  12 , batch:  256 , train loss:  18.172163009643555\n","epoch:  12 , batch:  257 , train loss:  14.786172866821289\n","epoch:  12 , batch:  258 , train loss:  14.497612953186035\n","epoch:  12 , batch:  259 , train loss:  17.5004940032959\n","epoch:  12 , batch:  260 , train loss:  26.020700454711914\n","epoch:  12 , batch:  261 , train loss:  17.617568969726562\n","epoch:  12 , batch:  262 , train loss:  15.362298011779785\n","epoch:  12 , batch:  263 , train loss:  18.51967430114746\n","epoch:  12 , batch:  264 , train loss:  12.278778076171875\n","epoch:  12 , batch:  265 , train loss:  13.621794700622559\n","epoch:  12 , batch:  266 , train loss:  12.549846649169922\n","epoch:  12 , batch:  267 , train loss:  19.792715072631836\n","epoch:  12 , batch:  268 , train loss:  9.917410850524902\n","epoch:  12 , batch:  269 , train loss:  8.41683292388916\n","epoch:  12 , batch:  270 , train loss:  16.490068435668945\n","epoch:  12 , batch:  271 , train loss:  17.948331832885742\n","epoch:  12 , batch:  272 , train loss:  18.19363784790039\n","epoch:  12 , batch:  273 , train loss:  12.793692588806152\n","epoch:  12 , batch:  274 , train loss:  16.903825759887695\n","epoch:  12 , batch:  275 , train loss:  12.411493301391602\n","epoch:  12 , batch:  276 , train loss:  18.58603286743164\n","epoch:  12 , batch:  277 , train loss:  21.390310287475586\n","epoch:  12 , batch:  278 , train loss:  14.728757858276367\n","epoch:  12 , batch:  279 , train loss:  15.12845230102539\n","epoch:  12 , batch:  280 , train loss:  15.842947959899902\n","epoch:  12 , batch:  281 , train loss:  16.08816146850586\n","epoch:  12 , batch:  282 , train loss:  12.260934829711914\n","epoch:  12 , batch:  283 , train loss:  12.30214786529541\n","epoch:  12 , batch:  284 , train loss:  12.9091157913208\n","epoch:  12 , batch:  285 , train loss:  17.311508178710938\n","epoch:  12 , batch:  286 , train loss:  15.655350685119629\n","epoch:  12 , batch:  287 , train loss:  19.328632354736328\n","epoch:  12 , batch:  288 , train loss:  13.434133529663086\n","epoch:  12 , batch:  289 , train loss:  14.700151443481445\n","epoch:  12 , batch:  290 , train loss:  10.424564361572266\n","epoch:  12 , batch:  291 , train loss:  13.660947799682617\n","epoch:  12 , batch:  292 , train loss:  8.821756362915039\n","epoch:  12 , batch:  293 , train loss:  16.89154052734375\n","epoch:  12 , batch:  294 , train loss:  13.248161315917969\n","epoch:  12 , batch:  295 , train loss:  9.53535270690918\n","epoch:  12 , batch:  296 , train loss:  12.214530944824219\n","epoch:  12 , batch:  297 , train loss:  9.859980583190918\n","epoch:  12 , batch:  298 , train loss:  16.55377960205078\n","epoch:  12 , batch:  299 , train loss:  9.425210952758789\n","epoch:  12 , batch:  300 , train loss:  16.909931182861328\n","epoch:  12 , batch:  301 , train loss:  18.34212303161621\n","epoch:  12 , batch:  302 , train loss:  10.453577041625977\n","epoch:  12 , batch:  303 , train loss:  14.430870056152344\n","epoch:  12 , batch:  304 , train loss:  31.389698028564453\n","epoch:  12 , batch:  305 , train loss:  22.048006057739258\n","epoch:  12 , batch:  306 , train loss:  19.827924728393555\n","epoch:  12 , batch:  307 , train loss:  20.1578426361084\n","epoch:  12 , batch:  308 , train loss:  20.45293617248535\n","epoch:  12 , batch:  309 , train loss:  6.414783477783203\n","epoch:  12 , batch:  310 , train loss:  13.494760513305664\n","epoch:  12 , batch:  311 , train loss:  12.818838119506836\n","epoch:  12 , batch:  312 , train loss:  21.07619857788086\n","epoch:  12 , batch:  313 , train loss:  13.698482513427734\n","epoch:  12 , batch:  314 , train loss:  19.33052635192871\n","epoch:  12 , batch:  315 , train loss:  12.298196792602539\n","epoch:  12 , batch:  316 , train loss:  17.27101707458496\n","epoch:  12 , batch:  317 , train loss:  25.606008529663086\n","epoch:  12 , batch:  318 , train loss:  17.645078659057617\n","epoch:  12 , batch:  319 , train loss:  15.212608337402344\n","epoch:  12 , batch:  320 , train loss:  18.69563865661621\n","epoch:  12 , batch:  321 , train loss:  10.430667877197266\n","epoch:  12 , batch:  322 , train loss:  13.561928749084473\n","epoch:  12 , batch:  323 , train loss:  13.030363082885742\n","epoch:  12 , batch:  324 , train loss:  8.980040550231934\n","epoch:  12 , batch:  325 , train loss:  17.295085906982422\n","epoch:  12 , batch:  326 , train loss:  16.31195831298828\n","epoch:  12 , batch:  327 , train loss:  11.03501033782959\n","epoch:  12 , batch:  328 , train loss:  10.35341739654541\n","epoch:  12 , batch:  329 , train loss:  6.738409996032715\n","epoch:  12 , batch:  330 , train loss:  17.233793258666992\n","epoch:  12 , batch:  331 , train loss:  20.512699127197266\n","epoch:  12 , batch:  332 , train loss:  16.88045883178711\n","epoch:  12 , batch:  333 , train loss:  12.519225120544434\n","epoch:  12 , batch:  334 , train loss:  12.145605087280273\n","epoch:  12 , batch:  335 , train loss:  10.358985900878906\n","epoch:  12 , batch:  336 , train loss:  16.66130256652832\n","epoch:  12 , batch:  337 , train loss:  16.653059005737305\n","epoch:  12 , batch:  338 , train loss:  11.935037612915039\n","epoch:  12 , batch:  339 , train loss:  20.780494689941406\n","epoch:  12 , batch:  340 , train loss:  10.668190956115723\n","epoch:  12 , batch:  341 , train loss:  26.211027145385742\n","epoch:  12 , batch:  342 , train loss:  11.567146301269531\n","epoch:  12 , batch:  343 , train loss:  10.525007247924805\n","epoch:  12 , batch:  344 , train loss:  18.694549560546875\n","epoch:  12 , batch:  345 , train loss:  17.304058074951172\n","epoch:  12 , batch:  346 , train loss:  12.383234024047852\n","epoch:  12 , batch:  347 , train loss:  20.1466007232666\n","epoch:  12 , batch:  348 , train loss:  23.41802406311035\n","epoch:  12 , batch:  349 , train loss:  17.984285354614258\n","epoch:  12 , batch:  350 , train loss:  24.400386810302734\n","epoch:  12 , batch:  351 , train loss:  21.7669620513916\n","epoch:  12 , batch:  352 , train loss:  15.633801460266113\n","epoch:  12 , batch:  353 , train loss:  15.462566375732422\n","epoch:  12 , batch:  354 , train loss:  11.544779777526855\n","epoch:  12 , batch:  355 , train loss:  11.449870109558105\n","epoch:  12 , batch:  356 , train loss:  27.13081932067871\n","epoch:  12 , batch:  357 , train loss:  13.2953462600708\n","epoch:  12 , batch:  358 , train loss:  14.884171485900879\n","epoch:  12 , batch:  359 , train loss:  14.457216262817383\n","epoch:  12 , batch:  360 , train loss:  18.57537269592285\n","epoch:  12 , batch:  361 , train loss:  11.56190013885498\n","epoch:  12 , batch:  362 , train loss:  10.248605728149414\n","epoch:  12 , batch:  363 , train loss:  14.316917419433594\n","epoch:  12 , batch:  364 , train loss:  12.039862632751465\n","epoch:  12 , batch:  365 , train loss:  16.448379516601562\n","epoch:  12 , batch:  366 , train loss:  7.469587326049805\n","epoch:  12 , batch:  367 , train loss:  14.02375316619873\n","epoch:  12 , batch:  368 , train loss:  26.391441345214844\n","epoch:  12 , batch:  369 , train loss:  13.5630521774292\n","epoch:  12 , batch:  370 , train loss:  15.655718803405762\n","epoch:  12 , batch:  371 , train loss:  21.743120193481445\n","epoch:  12 , batch:  372 , train loss:  20.082914352416992\n","epoch:  12 , batch:  373 , train loss:  15.831993103027344\n","epoch:  12 , batch:  374 , train loss:  15.716349601745605\n","epoch:  12 , batch:  375 , train loss:  17.957849502563477\n","epoch:  12 , batch:  376 , train loss:  19.24656105041504\n","epoch:  12 , batch:  377 , train loss:  17.542543411254883\n","epoch:  12 , batch:  378 , train loss:  13.381686210632324\n","epoch:  12 , batch:  379 , train loss:  15.39207649230957\n","epoch:  12 , batch:  380 , train loss:  13.54377269744873\n","epoch:  12 , batch:  381 , train loss:  10.771830558776855\n","epoch:  12 , batch:  382 , train loss:  10.420594215393066\n","epoch:  12 , batch:  383 , train loss:  19.491430282592773\n","epoch:  12 , batch:  384 , train loss:  14.206652641296387\n","epoch:  12 , batch:  385 , train loss:  11.889595031738281\n","epoch:  12 , batch:  386 , train loss:  10.898178100585938\n","epoch:  12 , batch:  387 , train loss:  10.677342414855957\n","epoch:  12 , batch:  388 , train loss:  15.006023406982422\n","epoch:  12 , batch:  389 , train loss:  16.98687171936035\n","epoch:  12 , batch:  390 , train loss:  16.288150787353516\n","epoch:  12 , batch:  391 , train loss:  9.173028945922852\n","epoch:  12 , batch:  392 , train loss:  17.66849708557129\n","epoch:  12 , batch:  393 , train loss:  17.530969619750977\n","epoch:  12 , batch:  394 , train loss:  14.6195650100708\n","epoch:  12 , batch:  395 , train loss:  10.823104858398438\n","epoch:  12 , batch:  396 , train loss:  13.325576782226562\n","epoch:  12 , batch:  397 , train loss:  6.577202796936035\n","epoch:  12 , batch:  398 , train loss:  18.52892303466797\n","epoch:  12 , batch:  399 , train loss:  15.285017967224121\n","epoch:  12 , batch:  400 , train loss:  13.376385688781738\n","epoch:  12 , batch:  401 , train loss:  11.993334770202637\n","epoch:  12 , batch:  402 , train loss:  14.034422874450684\n","epoch:  12 , batch:  403 , train loss:  16.237323760986328\n","epoch:  12 , batch:  404 , train loss:  11.488759994506836\n","epoch:  12 , batch:  405 , train loss:  14.840128898620605\n","epoch:  12 , batch:  406 , train loss:  11.776314735412598\n","epoch:  12 , batch:  407 , train loss:  8.402288436889648\n","epoch:  12 , batch:  408 , train loss:  10.26165771484375\n","epoch:  12 , batch:  409 , train loss:  5.3862833976745605\n","epoch:  12 , batch:  410 , train loss:  10.471490859985352\n","epoch:  12 , batch:  411 , train loss:  8.673312187194824\n","epoch:  12 , batch:  412 , train loss:  9.400385856628418\n","epoch:  12 , batch:  413 , train loss:  17.950660705566406\n","epoch:  12 , batch:  414 , train loss:  9.633901596069336\n","epoch:  12 , batch:  415 , train loss:  19.26736068725586\n","epoch:  12 , batch:  416 , train loss:  26.765188217163086\n","epoch:  12 , batch:  417 , train loss:  12.03225040435791\n","epoch:  12 , batch:  418 , train loss:  8.966644287109375\n","epoch:  12 , batch:  419 , train loss:  13.443029403686523\n","epoch:  12 , batch:  420 , train loss:  8.759571075439453\n","epoch:  12 , batch:  421 , train loss:  13.599102973937988\n","epoch:  12 , batch:  422 , train loss:  24.049711227416992\n","epoch:  12 , batch:  423 , train loss:  27.214340209960938\n","epoch:  12 , batch:  424 , train loss:  12.59611701965332\n","epoch:  12 , batch:  425 , train loss:  10.21158504486084\n","epoch:  12 , batch:  426 , train loss:  24.008865356445312\n","epoch:  12 , batch:  427 , train loss:  9.192228317260742\n","epoch:  12 , batch:  428 , train loss:  23.56858253479004\n","epoch:  12 , batch:  429 , train loss:  11.1118803024292\n","epoch:  12 , batch:  430 , train loss:  11.248846054077148\n","epoch:  12 , batch:  431 , train loss:  12.273441314697266\n","epoch:  12 , batch:  432 , train loss:  20.28752326965332\n","epoch:  12 , batch:  433 , train loss:  18.17517852783203\n","epoch:  12 , batch:  434 , train loss:  13.601512908935547\n","epoch:  12 , batch:  435 , train loss:  24.152421951293945\n","epoch:  12 , batch:  436 , train loss:  15.731192588806152\n","epoch:  12 , batch:  437 , train loss:  16.933757781982422\n","epoch:  12 , batch:  438 , train loss:  10.297392845153809\n","epoch:  12 , batch:  439 , train loss:  10.25531005859375\n","epoch:  12 , batch:  440 , train loss:  7.368658065795898\n","epoch:  12 , batch:  441 , train loss:  16.187742233276367\n","epoch:  12 , batch:  442 , train loss:  21.74452781677246\n","epoch:  12 , batch:  443 , train loss:  25.48714828491211\n","epoch:  12 , batch:  444 , train loss:  13.099502563476562\n","epoch:  12 , batch:  445 , train loss:  11.488265037536621\n","epoch:  12 , batch:  446 , train loss:  20.102855682373047\n","epoch:  12 , batch:  447 , train loss:  21.629619598388672\n","epoch:  12 , batch:  448 , train loss:  19.688255310058594\n","epoch:  12 , batch:  449 , train loss:  14.241372108459473\n","epoch:  12 , batch:  450 , train loss:  16.32567024230957\n","epoch:  12 , batch:  451 , train loss:  16.155532836914062\n","epoch:  12 , batch:  452 , train loss:  7.761115550994873\n","epoch:  12 , batch:  453 , train loss:  15.31387710571289\n","epoch:  12 , batch:  454 , train loss:  10.94221305847168\n","epoch:  12 , batch:  455 , train loss:  17.297868728637695\n","epoch:  12 , batch:  456 , train loss:  18.29903793334961\n","epoch:  12 , batch:  457 , train loss:  15.009571075439453\n","epoch:  12 , batch:  458 , train loss:  14.732019424438477\n","epoch:  12 , batch:  459 , train loss:  10.776198387145996\n","epoch:  12 , batch:  460 , train loss:  12.002745628356934\n","epoch:  12 , batch:  461 , train loss:  25.939111709594727\n","epoch:  12 , batch:  462 , train loss:  24.104331970214844\n","epoch:  12 , batch:  463 , train loss:  16.500442504882812\n","epoch:  12 , batch:  464 , train loss:  12.830780029296875\n","epoch:  12 , batch:  465 , train loss:  9.083553314208984\n","epoch:  12 , batch:  466 , train loss:  19.21503257751465\n","epoch:  12 , batch:  467 , train loss:  12.241045951843262\n","epoch:  12 , batch:  468 , train loss:  16.030210494995117\n","epoch:  12 , batch:  469 , train loss:  11.326895713806152\n","epoch:  12 , batch:  470 , train loss:  16.716175079345703\n","epoch:  12 , batch:  471 , train loss:  12.101662635803223\n","epoch:  12 , batch:  472 , train loss:  13.248876571655273\n","epoch:  12 , batch:  473 , train loss:  10.787751197814941\n","epoch:  12 , batch:  474 , train loss:  7.613658905029297\n","epoch:  12 , batch:  475 , train loss:  21.81549644470215\n","epoch:  12 , batch:  476 , train loss:  13.080734252929688\n","epoch:  12 , batch:  477 , train loss:  10.509773254394531\n","epoch:  12 , batch:  478 , train loss:  23.827680587768555\n","epoch:  12 , batch:  479 , train loss:  10.457558631896973\n","epoch:  12 , batch:  480 , train loss:  16.292362213134766\n","epoch:  12 , batch:  481 , train loss:  24.508808135986328\n","epoch:  12 , batch:  482 , train loss:  12.143366813659668\n","epoch:  12 , batch:  483 , train loss:  18.9943790435791\n","epoch:  12 , batch:  484 , train loss:  12.824416160583496\n","epoch:  12 , batch:  485 , train loss:  10.88766860961914\n","epoch:  12 , batch:  486 , train loss:  16.935359954833984\n","epoch:  12 , batch:  487 , train loss:  24.05205535888672\n","epoch:  12 , batch:  488 , train loss:  17.559144973754883\n","epoch:  12 , batch:  489 , train loss:  25.438770294189453\n","epoch:  12 , batch:  490 , train loss:  8.889533996582031\n","epoch:  12 , batch:  491 , train loss:  17.367279052734375\n","epoch:  12 , batch:  492 , train loss:  15.452465057373047\n","epoch:  12 , batch:  493 , train loss:  11.504494667053223\n","epoch:  12 , batch:  494 , train loss:  29.581066131591797\n","epoch:  12 , batch:  495 , train loss:  16.500391006469727\n","epoch:  12 , batch:  496 , train loss:  10.047635078430176\n","epoch:  12 , batch:  497 , train loss:  10.331863403320312\n","epoch:  12 , batch:  498 , train loss:  17.16189956665039\n","epoch:  12 , batch:  499 , train loss:  16.66798973083496\n","epoch:  12 , batch:  500 , train loss:  27.641016006469727\n","epoch:  12 , batch:  501 , train loss:  11.702887535095215\n","epoch:  12 , batch:  502 , train loss:  16.1495418548584\n","epoch:  12 , batch:  503 , train loss:  15.76895809173584\n","epoch:  12 , batch:  504 , train loss:  11.558988571166992\n","epoch:  12 , batch:  505 , train loss:  23.386934280395508\n","epoch:  12 , batch:  506 , train loss:  19.982173919677734\n","epoch:  12 , batch:  507 , train loss:  9.413505554199219\n","epoch:  12 , batch:  508 , train loss:  9.15204906463623\n","epoch:  12 , batch:  509 , train loss:  12.951935768127441\n","epoch:  12 , batch:  510 , train loss:  8.683843612670898\n","epoch:  12 , batch:  511 , train loss:  16.0502986907959\n","epoch:  12 , batch:  512 , train loss:  19.695077896118164\n","epoch:  12 , batch:  513 , train loss:  12.076850891113281\n","epoch:  12 , batch:  514 , train loss:  12.563295364379883\n","epoch:  12 , batch:  515 , train loss:  15.12099552154541\n","epoch:  12 , batch:  516 , train loss:  13.477320671081543\n","epoch:  12 , batch:  517 , train loss:  15.670726776123047\n","epoch:  12 , batch:  518 , train loss:  14.283759117126465\n","epoch:  12 , batch:  519 , train loss:  17.103939056396484\n","epoch:  12 , batch:  520 , train loss:  17.269834518432617\n","epoch:  12 , batch:  521 , train loss:  13.19559383392334\n","epoch:  12 , batch:  522 , train loss:  12.385089874267578\n","epoch:  12 , batch:  523 , train loss:  7.686307430267334\n","epoch:  12 , batch:  524 , train loss:  9.858813285827637\n","epoch:  12 , batch:  525 , train loss:  22.274700164794922\n","epoch:  12 , batch:  526 , train loss:  11.844963073730469\n","epoch:  12 , batch:  527 , train loss:  12.399106979370117\n","epoch:  12 , batch:  528 , train loss:  17.332521438598633\n","epoch:  12 , batch:  529 , train loss:  19.46812629699707\n","epoch:  12 , batch:  530 , train loss:  17.424423217773438\n","epoch:  12 , batch:  531 , train loss:  17.839616775512695\n","epoch:  12 , batch:  532 , train loss:  14.406265258789062\n","epoch:  12 , batch:  533 , train loss:  15.47727108001709\n","epoch:  12 , batch:  534 , train loss:  10.78731918334961\n","epoch:  12 , batch:  535 , train loss:  6.369155406951904\n","epoch:  12 , batch:  536 , train loss:  12.050580978393555\n","epoch:  12 , batch:  537 , train loss:  17.58368682861328\n","epoch:  12 , batch:  538 , train loss:  12.922319412231445\n","epoch:  12 , batch:  539 , train loss:  13.969141960144043\n","epoch:  12 , batch:  540 , train loss:  17.486087799072266\n","epoch:  12 , batch:  541 , train loss:  21.22265625\n","epoch:  12 , batch:  542 , train loss:  13.200345993041992\n","epoch:  12 , batch:  543 , train loss:  21.68332862854004\n","epoch:  12 , batch:  544 , train loss:  19.461889266967773\n","epoch:  12 , batch:  545 , train loss:  13.468376159667969\n","epoch:  12 , batch:  546 , train loss:  16.737577438354492\n","epoch:  12 , batch:  547 , train loss:  12.494903564453125\n","epoch:  12 , batch:  548 , train loss:  14.0086030960083\n","epoch:  12 , batch:  549 , train loss:  17.789575576782227\n","epoch:  12 , batch:  550 , train loss:  21.21210479736328\n","epoch:  12 , batch:  551 , train loss:  13.83680248260498\n","epoch:  12 , batch:  552 , train loss:  13.4366455078125\n","epoch:  12 , batch:  553 , train loss:  10.158899307250977\n","epoch:  12 , batch:  554 , train loss:  23.677419662475586\n","epoch:  12 , batch:  555 , train loss:  14.022839546203613\n","epoch:  12 , batch:  556 , train loss:  21.733598709106445\n","epoch:  12 , batch:  557 , train loss:  16.736682891845703\n","epoch:  12 , batch:  558 , train loss:  30.1436710357666\n","epoch:  12 , batch:  559 , train loss:  22.313024520874023\n","epoch:  12 , batch:  560 , train loss:  10.556147575378418\n","epoch:  12 , batch:  561 , train loss:  22.347476959228516\n","epoch:  12 , batch:  562 , train loss:  15.471715927124023\n","epoch:  12 , batch:  563 , train loss:  18.724933624267578\n","epoch:  12 , batch:  564 , train loss:  17.00342559814453\n","epoch:  12 , batch:  565 , train loss:  10.31484317779541\n","epoch:  12 , batch:  566 , train loss:  12.59374713897705\n","epoch:  12 , batch:  567 , train loss:  18.636585235595703\n","epoch:  12 , batch:  568 , train loss:  19.169708251953125\n","epoch:  12 , batch:  569 , train loss:  26.975826263427734\n","epoch:  12 , batch:  570 , train loss:  14.636756896972656\n","epoch:  12 , batch:  571 , train loss:  22.24310302734375\n","epoch:  12 , batch:  572 , train loss:  11.074065208435059\n","epoch:  12 , batch:  573 , train loss:  9.573200225830078\n","epoch:  12 , batch:  574 , train loss:  17.02370834350586\n","epoch:  12 , batch:  575 , train loss:  12.420928001403809\n","epoch:  12 , batch:  576 , train loss:  19.140522003173828\n","epoch:  12 , batch:  577 , train loss:  18.571304321289062\n","epoch:  12 , batch:  578 , train loss:  9.343950271606445\n","epoch:  12 , batch:  579 , train loss:  18.572769165039062\n","epoch:  12 , batch:  580 , train loss:  11.475427627563477\n","epoch:  12 , batch:  581 , train loss:  15.824295997619629\n","epoch:  12 , batch:  582 , train loss:  14.084060668945312\n","epoch:  12 , batch:  583 , train loss:  19.09852409362793\n","epoch:  12 , batch:  584 , train loss:  20.25306510925293\n","epoch:  12 , batch:  585 , train loss:  17.319049835205078\n","epoch:  12 , batch:  586 , train loss:  13.689720153808594\n","epoch:  12 , batch:  587 , train loss:  13.699316024780273\n","epoch:  12 , batch:  588 , train loss:  15.071154594421387\n","epoch:  12 , batch:  589 , train loss:  8.68122386932373\n","epoch:  12 , batch:  590 , train loss:  12.819748878479004\n","epoch:  12 , batch:  591 , train loss:  19.9072322845459\n","epoch:  12 , batch:  592 , train loss:  34.75214385986328\n","epoch:  12 , batch:  593 , train loss:  26.07196044921875\n","epoch:  12 , batch:  594 , train loss:  9.82556438446045\n","epoch:  12 , batch:  595 , train loss:  15.27902603149414\n","epoch:  12 , batch:  596 , train loss:  16.053829193115234\n","epoch:  12 , batch:  597 , train loss:  15.779380798339844\n","epoch:  12 , batch:  598 , train loss:  19.533279418945312\n","epoch:  12 , batch:  599 , train loss:  24.043746948242188\n","epoch:  12 , batch:  600 , train loss:  14.633902549743652\n","epoch:  12 , batch:  601 , train loss:  15.748205184936523\n","epoch:  12 , batch:  602 , train loss:  13.317060470581055\n","epoch:  12 , batch:  603 , train loss:  19.669187545776367\n","epoch:  12 , batch:  604 , train loss:  13.769709587097168\n","epoch:  12 , batch:  605 , train loss:  10.180587768554688\n","epoch:  12 , batch:  606 , train loss:  26.322296142578125\n","epoch:  12 , batch:  607 , train loss:  20.96677017211914\n","epoch:  12 , batch:  608 , train loss:  19.235769271850586\n","epoch:  12 , batch:  609 , train loss:  11.456449508666992\n","epoch:  12 , batch:  610 , train loss:  8.433908462524414\n","epoch:  12 , batch:  611 , train loss:  11.154932975769043\n","epoch:  12 , batch:  612 , train loss:  11.017544746398926\n","epoch:  12 , batch:  613 , train loss:  14.280098915100098\n","epoch:  12 , batch:  614 , train loss:  25.91666030883789\n","epoch:  12 , batch:  615 , train loss:  11.150748252868652\n","epoch:  12 , batch:  616 , train loss:  18.49431800842285\n","epoch:  12 , batch:  617 , train loss:  10.359463691711426\n","epoch:  12 , batch:  618 , train loss:  17.87428092956543\n","epoch:  12 , batch:  619 , train loss:  11.449959754943848\n","epoch:  12 , batch:  620 , train loss:  11.90394401550293\n","epoch:  12 , batch:  621 , train loss:  25.25505256652832\n","epoch:  12 , batch:  622 , train loss:  18.529605865478516\n","epoch:  12 , batch:  623 , train loss:  11.699904441833496\n","epoch:  12 , batch:  624 , train loss:  16.314990997314453\n","epoch:  12 , batch:  625 , train loss:  15.967798233032227\n","epoch:  12 , batch:  626 , train loss:  26.794099807739258\n","epoch:  12 , batch:  627 , train loss:  16.40863037109375\n","epoch:  12 , batch:  628 , train loss:  12.100704193115234\n","epoch:  12 , batch:  629 , train loss:  29.72532081604004\n","epoch:  12 , batch:  630 , train loss:  21.68083953857422\n","epoch:  12 , batch:  631 , train loss:  13.870089530944824\n","epoch:  12 , batch:  632 , train loss:  13.650347709655762\n","epoch:  12 , batch:  633 , train loss:  17.95794105529785\n","epoch:  12 , batch:  634 , train loss:  13.941247940063477\n","epoch:  12 , batch:  635 , train loss:  10.364448547363281\n","epoch:  12 , batch:  636 , train loss:  14.687143325805664\n","epoch:  12 , batch:  637 , train loss:  12.999765396118164\n","epoch:  12 , batch:  638 , train loss:  14.171974182128906\n","epoch:  12 , batch:  639 , train loss:  18.717500686645508\n","epoch:  12 , batch:  640 , train loss:  12.601369857788086\n","epoch:  12 , batch:  641 , train loss:  12.640863418579102\n","epoch:  12 , batch:  642 , train loss:  19.17000389099121\n","epoch:  12 , batch:  643 , train loss:  18.314956665039062\n","epoch:  12 , batch:  644 , train loss:  21.441001892089844\n","epoch:  12 , batch:  645 , train loss:  17.775711059570312\n","epoch:  12 , batch:  646 , train loss:  8.906909942626953\n","epoch:  12 , batch:  647 , train loss:  12.045503616333008\n","epoch:  12 , batch:  648 , train loss:  21.194456100463867\n","epoch:  12 , batch:  649 , train loss:  16.978919982910156\n","epoch:  12 , batch:  650 , train loss:  15.270137786865234\n","epoch:  12 , batch:  651 , train loss:  14.75157356262207\n","epoch:  12 , batch:  652 , train loss:  15.208419799804688\n","epoch:  12 , batch:  653 , train loss:  8.732266426086426\n","epoch:  12 , batch:  654 , train loss:  15.767483711242676\n","epoch:  12 , batch:  655 , train loss:  18.26069450378418\n","epoch:  12 , batch:  656 , train loss:  18.377437591552734\n","epoch:  12 , batch:  657 , train loss:  12.878726959228516\n","epoch:  12 , batch:  658 , train loss:  22.15569305419922\n","epoch:  12 , batch:  659 , train loss:  16.903438568115234\n","epoch:  12 , batch:  660 , train loss:  22.67336082458496\n","epoch:  12 , batch:  661 , train loss:  25.619321823120117\n","epoch:  12 , batch:  662 , train loss:  15.69152545928955\n","epoch:  12 , batch:  663 , train loss:  9.107166290283203\n","epoch:  12 , batch:  664 , train loss:  16.843664169311523\n","epoch:  12 , batch:  665 , train loss:  18.18476104736328\n","epoch:  12 , batch:  666 , train loss:  19.761754989624023\n","epoch:  12 , batch:  667 , train loss:  24.372072219848633\n","epoch:  12 , batch:  668 , train loss:  21.90915298461914\n","epoch:  12 , batch:  669 , train loss:  19.569355010986328\n","epoch:  12 , batch:  670 , train loss:  17.526626586914062\n","epoch:  12 , batch:  671 , train loss:  22.865591049194336\n","epoch:  12 , batch:  672 , train loss:  12.350359916687012\n","epoch:  12 , batch:  673 , train loss:  11.689193725585938\n","epoch:  12 , batch:  674 , train loss:  27.290855407714844\n","epoch:  12 , batch:  675 , train loss:  12.812468528747559\n","epoch:  12 , batch:  676 , train loss:  11.273797035217285\n","epoch:  12 , batch:  677 , train loss:  13.88264274597168\n","epoch:  12 , batch:  678 , train loss:  11.726608276367188\n","epoch:  12 , batch:  679 , train loss:  14.486525535583496\n","epoch:  12 , batch:  680 , train loss:  15.827676773071289\n","epoch:  12 , batch:  681 , train loss:  15.628629684448242\n","epoch:  12 , batch:  682 , train loss:  21.774887084960938\n","epoch:  12 , batch:  683 , train loss:  14.157795906066895\n","epoch:  12 , batch:  684 , train loss:  10.197304725646973\n","epoch:  12 , batch:  685 , train loss:  16.04324722290039\n","epoch:  12 , batch:  686 , train loss:  12.207893371582031\n","epoch:  12 , batch:  687 , train loss:  23.445125579833984\n","epoch:  12 , batch:  688 , train loss:  16.735464096069336\n","epoch:  12 , batch:  689 , train loss:  9.633792877197266\n","epoch:  12 , batch:  690 , train loss:  16.041810989379883\n","epoch:  12 , batch:  691 , train loss:  4.841309070587158\n","epoch:  12 , batch:  692 , train loss:  11.007369041442871\n","epoch:  12 , batch:  693 , train loss:  11.814041137695312\n","epoch:  12 , batch:  694 , train loss:  12.977789878845215\n","epoch:  12 , batch:  695 , train loss:  18.49477767944336\n","epoch:  12 , batch:  696 , train loss:  9.928075790405273\n","epoch:  12 , batch:  697 , train loss:  14.107357025146484\n","epoch:  12 , batch:  698 , train loss:  24.859403610229492\n","epoch:  12 , batch:  699 , train loss:  10.577193260192871\n","epoch:  12 , batch:  700 , train loss:  11.429004669189453\n","epoch:  12 , batch:  701 , train loss:  11.30896282196045\n","epoch:  12 , batch:  702 , train loss:  15.48929500579834\n","epoch:  12 , batch:  703 , train loss:  12.18860912322998\n","epoch:  12 , batch:  704 , train loss:  11.33410358428955\n","epoch:  12 , batch:  705 , train loss:  23.20502281188965\n","epoch:  12 , batch:  706 , train loss:  14.813089370727539\n","epoch:  12 , batch:  707 , train loss:  11.082067489624023\n","epoch:  12 , batch:  708 , train loss:  22.787315368652344\n","epoch:  12 , batch:  709 , train loss:  13.450837135314941\n","epoch:  12 , batch:  710 , train loss:  10.12196159362793\n","epoch:  12 , batch:  711 , train loss:  9.267138481140137\n","epoch:  12 , batch:  712 , train loss:  13.72060489654541\n","epoch:  12 , batch:  713 , train loss:  4.745509147644043\n","epoch:  12 , batch:  714 , train loss:  11.194791793823242\n","epoch:  12 , batch:  715 , train loss:  16.29906463623047\n","epoch:  12 , batch:  716 , train loss:  8.940032005310059\n","epoch:  12 , batch:  717 , train loss:  22.013986587524414\n","epoch:  12 , batch:  718 , train loss:  13.43262004852295\n","epoch:  12 , batch:  719 , train loss:  8.370387077331543\n","epoch:  12 , batch:  720 , train loss:  20.679821014404297\n","epoch:  12 , batch:  721 , train loss:  26.17719268798828\n","epoch:  12 , batch:  722 , train loss:  17.830766677856445\n","epoch:  12 , batch:  723 , train loss:  12.287187576293945\n","epoch:  12 , batch:  724 , train loss:  12.913232803344727\n","epoch:  12 , batch:  725 , train loss:  14.234857559204102\n","epoch:  12 , batch:  726 , train loss:  16.625883102416992\n","epoch:  12 , batch:  727 , train loss:  7.795527935028076\n","epoch:  12 , batch:  728 , train loss:  16.188106536865234\n","epoch:  12 , batch:  729 , train loss:  10.514304161071777\n","epoch:  12 , batch:  730 , train loss:  8.788223266601562\n","epoch:  12 , batch:  731 , train loss:  14.402000427246094\n","epoch:  12 , batch:  732 , train loss:  13.65344524383545\n","epoch:  12 , batch:  733 , train loss:  16.06234359741211\n","epoch:  12 , batch:  734 , train loss:  12.207481384277344\n","epoch:  12 , batch:  735 , train loss:  13.546746253967285\n","epoch:  12 , batch:  736 , train loss:  15.463159561157227\n","epoch:  12 , batch:  737 , train loss:  21.189481735229492\n","epoch:  12 , batch:  738 , train loss:  18.74011993408203\n","epoch:  12 , batch:  739 , train loss:  14.209933280944824\n","epoch:  12 , batch:  740 , train loss:  12.036080360412598\n","epoch:  12 , batch:  741 , train loss:  11.220773696899414\n","epoch:  12 , batch:  742 , train loss:  15.085271835327148\n","epoch:  12 , batch:  743 , train loss:  24.86192512512207\n","epoch:  12 , batch:  744 , train loss:  24.76244354248047\n","epoch:  12 , batch:  745 , train loss:  14.396407127380371\n","epoch:  12 , batch:  746 , train loss:  17.161617279052734\n","epoch:  12 , batch:  747 , train loss:  15.55962085723877\n","epoch:  12 , batch:  748 , train loss:  17.33742904663086\n","epoch:  12 , batch:  749 , train loss:  14.134099006652832\n","epoch:  12 , batch:  750 , train loss:  8.2572021484375\n","epoch:  12 , batch:  751 , train loss:  13.930964469909668\n","epoch:  12 , batch:  752 , train loss:  21.158769607543945\n","epoch:  12 , batch:  753 , train loss:  12.181785583496094\n","epoch:  12 , batch:  754 , train loss:  18.733816146850586\n","epoch:  12 , batch:  755 , train loss:  24.533504486083984\n","epoch:  12 , batch:  756 , train loss:  22.38179588317871\n","epoch:  12 , batch:  757 , train loss:  11.497321128845215\n","epoch:  12 , batch:  758 , train loss:  15.094195365905762\n","epoch:  12 , batch:  759 , train loss:  17.29157066345215\n","epoch:  12 , batch:  760 , train loss:  21.05297088623047\n","epoch:  12 , batch:  761 , train loss:  18.418472290039062\n","epoch:  12 , batch:  762 , train loss:  11.955958366394043\n","epoch:  12 , batch:  763 , train loss:  19.94621467590332\n","epoch:  12 , batch:  764 , train loss:  14.371270179748535\n","epoch:  12 , batch:  765 , train loss:  17.928131103515625\n","epoch:  12 , batch:  766 , train loss:  25.700464248657227\n","epoch:  12 , batch:  767 , train loss:  13.584980964660645\n","epoch:  12 , batch:  768 , train loss:  23.416706085205078\n","epoch:  12 , batch:  769 , train loss:  7.407459259033203\n","epoch:  12 , batch:  770 , train loss:  14.340149879455566\n","epoch:  12 , batch:  771 , train loss:  7.704840183258057\n","epoch:  12 , batch:  772 , train loss:  10.963041305541992\n","epoch:  12 , batch:  773 , train loss:  19.101970672607422\n","epoch:  12 , batch:  774 , train loss:  22.695354461669922\n","epoch:  12 , batch:  775 , train loss:  10.509560585021973\n","epoch:  12 , batch:  776 , train loss:  10.074379920959473\n","epoch:  12 , batch:  777 , train loss:  14.4332275390625\n","epoch:  12 , batch:  778 , train loss:  16.436800003051758\n","epoch:  12 , batch:  779 , train loss:  15.693341255187988\n","epoch:  12 , batch:  780 , train loss:  19.81127166748047\n","epoch:  12 , batch:  781 , train loss:  18.967445373535156\n","epoch:  12 , batch:  782 , train loss:  8.86664867401123\n","epoch:  12 , batch:  783 , train loss:  13.165202140808105\n","epoch:  12 , batch:  784 , train loss:  12.22451114654541\n","epoch:  12 , batch:  785 , train loss:  12.642777442932129\n","epoch:  12 , batch:  786 , train loss:  18.99639129638672\n","epoch:  12 , batch:  787 , train loss:  13.060001373291016\n","epoch:  12 , batch:  788 , train loss:  18.377168655395508\n","epoch:  12 , batch:  789 , train loss:  12.091145515441895\n","epoch:  12 , batch:  790 , train loss:  9.499768257141113\n","epoch:  12 , batch:  791 , train loss:  7.458131313323975\n","epoch:  12 , batch:  792 , train loss:  15.376994132995605\n","epoch:  12 , batch:  793 , train loss:  18.331178665161133\n","epoch:  12 , batch:  794 , train loss:  33.6204833984375\n","epoch:  12 , batch:  795 , train loss:  15.93371868133545\n","epoch:  12 , batch:  796 , train loss:  15.86509895324707\n","epoch:  12 , batch:  797 , train loss:  15.37165641784668\n","epoch:  12 , batch:  798 , train loss:  12.557023048400879\n","epoch:  12 , batch:  799 , train loss:  19.66688346862793\n","epoch:  12 , batch:  800 , train loss:  18.431520462036133\n","epoch:  12 , batch:  801 , train loss:  16.113367080688477\n","epoch:  12 , batch:  802 , train loss:  17.083925247192383\n","epoch:  12 , batch:  803 , train loss:  11.32657241821289\n","epoch:  12 , batch:  804 , train loss:  15.350811958312988\n","epoch:  12 , batch:  805 , train loss:  18.93452262878418\n","epoch:  12 , batch:  806 , train loss:  9.00263786315918\n","epoch:  12 , batch:  807 , train loss:  10.975483894348145\n","epoch:  12 , batch:  808 , train loss:  12.298030853271484\n","epoch:  12 , batch:  809 , train loss:  19.795310974121094\n","epoch:  12 , batch:  810 , train loss:  27.568525314331055\n","epoch:  12 , batch:  811 , train loss:  24.363216400146484\n","epoch:  12 , batch:  812 , train loss:  33.19425964355469\n","epoch:  12 , batch:  813 , train loss:  9.994991302490234\n","epoch:  12 , batch:  814 , train loss:  15.978581428527832\n","epoch:  12 , batch:  815 , train loss:  12.123612403869629\n","epoch:  12 , batch:  816 , train loss:  15.99438190460205\n","epoch:  12 , batch:  817 , train loss:  7.4673614501953125\n","epoch:  12 , batch:  818 , train loss:  8.635577201843262\n","epoch:  12 , batch:  819 , train loss:  26.278684616088867\n","epoch:  12 , batch:  820 , train loss:  15.697758674621582\n","epoch:  12 , batch:  821 , train loss:  19.313011169433594\n","epoch:  12 , batch:  822 , train loss:  9.913884162902832\n","epoch:  12 , batch:  823 , train loss:  19.388267517089844\n","epoch:  12 , batch:  824 , train loss:  10.398296356201172\n","epoch:  12 , batch:  825 , train loss:  18.276390075683594\n","epoch:  12 , batch:  826 , train loss:  13.568203926086426\n","epoch:  12 , batch:  827 , train loss:  8.432863235473633\n","epoch:  12 , batch:  828 , train loss:  14.992938041687012\n","epoch:  12 , batch:  829 , train loss:  10.051888465881348\n","epoch:  12 , batch:  830 , train loss:  19.998455047607422\n","epoch:  12 , batch:  831 , train loss:  18.22132110595703\n","epoch:  12 , batch:  832 , train loss:  15.452119827270508\n","epoch:  12 , batch:  833 , train loss:  16.3902530670166\n","epoch:  12 , batch:  834 , train loss:  20.72583770751953\n","epoch:  12 , batch:  835 , train loss:  10.893111228942871\n","epoch:  12 , batch:  836 , train loss:  10.874417304992676\n","epoch:  12 , batch:  837 , train loss:  15.006518363952637\n","epoch:  12 , batch:  838 , train loss:  17.19597053527832\n","epoch:  12 , batch:  839 , train loss:  18.313201904296875\n","epoch:  12 , batch:  840 , train loss:  22.570695877075195\n","epoch:  12 , batch:  841 , train loss:  6.601744174957275\n","epoch:  12 , batch:  842 , train loss:  31.84077262878418\n","epoch:  12 , batch:  843 , train loss:  12.36337661743164\n","epoch:  12 , batch:  844 , train loss:  19.486955642700195\n","epoch:  12 , batch:  845 , train loss:  10.810248374938965\n","epoch:  12 , batch:  846 , train loss:  23.714553833007812\n","epoch:  12 , batch:  847 , train loss:  27.434541702270508\n","epoch:  12 , batch:  848 , train loss:  21.757413864135742\n","epoch:  12 , batch:  849 , train loss:  10.63240909576416\n","epoch:  12 , batch:  850 , train loss:  8.039200782775879\n","epoch:  12 , batch:  851 , train loss:  11.388876914978027\n","epoch:  12 , batch:  852 , train loss:  20.859830856323242\n","epoch:  12 , batch:  853 , train loss:  13.859127044677734\n","epoch:  12 , batch:  854 , train loss:  19.166500091552734\n","epoch:  12 , batch:  855 , train loss:  7.351663112640381\n","epoch:  12 , batch:  856 , train loss:  14.065814971923828\n","epoch:  12 , batch:  857 , train loss:  10.491164207458496\n","epoch:  12 , batch:  858 , train loss:  11.252756118774414\n","epoch:  12 , batch:  859 , train loss:  7.059120178222656\n","epoch:  12 , batch:  860 , train loss:  12.044417381286621\n","epoch:  12 , batch:  861 , train loss:  12.8258056640625\n","epoch:  12 , batch:  862 , train loss:  12.366697311401367\n","epoch:  12 , batch:  863 , train loss:  13.490959167480469\n","epoch:  12 , batch:  864 , train loss:  15.759262084960938\n","epoch:  12 , batch:  865 , train loss:  9.1053466796875\n","epoch:  12 , batch:  866 , train loss:  16.48933219909668\n","epoch:  12 , batch:  867 , train loss:  24.219791412353516\n","epoch:  12 , batch:  868 , train loss:  20.668123245239258\n","epoch:  12 , batch:  869 , train loss:  15.745593070983887\n","epoch:  12 , batch:  870 , train loss:  9.472455978393555\n","epoch:  12 , batch:  871 , train loss:  14.468334197998047\n","epoch:  12 , batch:  872 , train loss:  17.925893783569336\n","epoch:  12 , batch:  873 , train loss:  23.181283950805664\n","epoch:  12 , batch:  874 , train loss:  7.6112165451049805\n","epoch:  12 , batch:  875 , train loss:  25.216358184814453\n","epoch:  12 , batch:  876 , train loss:  17.164875030517578\n","epoch:  12 , batch:  877 , train loss:  18.27625274658203\n","epoch:  12 , batch:  878 , train loss:  18.833019256591797\n","epoch:  12 , batch:  879 , train loss:  14.133517265319824\n","epoch:  12 , batch:  880 , train loss:  16.042261123657227\n","epoch:  12 , batch:  881 , train loss:  13.304842948913574\n","epoch:  12 , batch:  882 , train loss:  10.440028190612793\n","epoch:  12 , batch:  883 , train loss:  25.501312255859375\n","epoch:  12 , batch:  884 , train loss:  29.238069534301758\n","epoch:  12 , batch:  885 , train loss:  9.306796073913574\n","epoch:  12 , batch:  886 , train loss:  15.333697319030762\n","epoch:  12 , batch:  887 , train loss:  17.35499382019043\n","epoch:  12 , batch:  888 , train loss:  8.935734748840332\n","epoch:  12 , batch:  889 , train loss:  7.856402397155762\n","epoch:  12 , batch:  890 , train loss:  14.524198532104492\n","epoch:  12 , batch:  891 , train loss:  17.655616760253906\n","epoch:  12 , batch:  892 , train loss:  15.956582069396973\n","epoch:  12 , batch:  893 , train loss:  19.23051643371582\n","epoch:  12 , batch:  894 , train loss:  16.525224685668945\n","epoch:  12 , batch:  895 , train loss:  14.011826515197754\n","epoch:  12 , batch:  896 , train loss:  13.1195707321167\n","epoch:  12 , batch:  897 , train loss:  9.219487190246582\n","epoch:  12 , batch:  898 , train loss:  15.573805809020996\n","epoch:  12 , batch:  899 , train loss:  7.991821765899658\n","epoch:  12 , batch:  900 , train loss:  16.99066734313965\n","epoch:  12 , batch:  901 , train loss:  12.504497528076172\n","epoch:  12 , batch:  902 , train loss:  15.164202690124512\n","epoch:  12 , batch:  903 , train loss:  16.58746910095215\n","epoch:  12 , batch:  904 , train loss:  11.04759693145752\n","epoch:  12 , batch:  905 , train loss:  14.526263236999512\n","epoch:  12 , batch:  906 , train loss:  11.732556343078613\n","epoch:  12 , batch:  907 , train loss:  16.141822814941406\n","epoch:  12 , batch:  908 , train loss:  13.970004081726074\n","epoch:  12 , batch:  909 , train loss:  17.01430320739746\n","epoch:  12 , batch:  910 , train loss:  15.146769523620605\n","epoch:  12 , batch:  911 , train loss:  16.54828643798828\n","epoch:  12 , batch:  912 , train loss:  17.989412307739258\n","epoch:  12 , batch:  913 , train loss:  18.62864112854004\n","epoch:  12 , batch:  914 , train loss:  9.91976547241211\n","epoch:  12 , batch:  915 , train loss:  23.380075454711914\n","epoch:  12 , batch:  916 , train loss:  6.465841293334961\n","epoch:  12 , batch:  917 , train loss:  15.032323837280273\n","epoch:  12 , batch:  918 , train loss:  15.137603759765625\n","epoch:  12 , batch:  919 , train loss:  18.701879501342773\n","epoch:  12 , batch:  920 , train loss:  13.186484336853027\n","epoch:  12 , batch:  921 , train loss:  21.504980087280273\n","epoch:  12 , batch:  922 , train loss:  17.089479446411133\n","epoch:  12 , batch:  923 , train loss:  7.08270263671875\n","epoch:  12 , batch:  924 , train loss:  19.40484619140625\n","epoch:  12 , batch:  925 , train loss:  15.948684692382812\n","epoch:  12 , batch:  926 , train loss:  20.446670532226562\n","epoch:  12 , batch:  927 , train loss:  10.991450309753418\n","epoch:  12 , batch:  928 , train loss:  12.530721664428711\n","epoch:  12 , batch:  929 , train loss:  11.779321670532227\n","epoch:  12 , batch:  930 , train loss:  10.93749713897705\n","epoch:  12 , batch:  931 , train loss:  12.49509334564209\n","epoch:  12 , batch:  932 , train loss:  14.021760940551758\n","epoch:  12 , batch:  933 , train loss:  10.341111183166504\n","epoch:  12 , batch:  934 , train loss:  17.204530715942383\n","epoch:  12 , batch:  935 , train loss:  16.615825653076172\n","epoch:  12 , batch:  936 , train loss:  22.43349838256836\n","epoch:  12 , batch:  937 , train loss:  3.138731002807617\n","Accuracy of train set: 0.9074666666666666\n","epoch:  12 , batch:  0 , test loss:  17.24758529663086\n","epoch:  12 , batch:  1 , test loss:  15.201742172241211\n","epoch:  12 , batch:  2 , test loss:  31.00028419494629\n","epoch:  12 , batch:  3 , test loss:  18.87978744506836\n","epoch:  12 , batch:  4 , test loss:  23.36284637451172\n","epoch:  12 , batch:  5 , test loss:  17.87740707397461\n","epoch:  12 , batch:  6 , test loss:  23.348581314086914\n","epoch:  12 , batch:  7 , test loss:  26.364261627197266\n","epoch:  12 , batch:  8 , test loss:  10.625770568847656\n","epoch:  12 , batch:  9 , test loss:  15.54686164855957\n","epoch:  12 , batch:  10 , test loss:  14.735746383666992\n","epoch:  12 , batch:  11 , test loss:  17.521936416625977\n","epoch:  12 , batch:  12 , test loss:  20.722257614135742\n","epoch:  12 , batch:  13 , test loss:  25.646032333374023\n","epoch:  12 , batch:  14 , test loss:  11.987785339355469\n","epoch:  12 , batch:  15 , test loss:  36.66095733642578\n","epoch:  12 , batch:  16 , test loss:  50.85268783569336\n","epoch:  12 , batch:  17 , test loss:  10.897119522094727\n","epoch:  12 , batch:  18 , test loss:  32.32927703857422\n","epoch:  12 , batch:  19 , test loss:  13.409293174743652\n","epoch:  12 , batch:  20 , test loss:  28.721948623657227\n","epoch:  12 , batch:  21 , test loss:  22.295942306518555\n","epoch:  12 , batch:  22 , test loss:  13.942412376403809\n","epoch:  12 , batch:  23 , test loss:  29.745243072509766\n","epoch:  12 , batch:  24 , test loss:  28.855993270874023\n","epoch:  12 , batch:  25 , test loss:  11.999053955078125\n","epoch:  12 , batch:  26 , test loss:  26.921493530273438\n","epoch:  12 , batch:  27 , test loss:  14.87889289855957\n","epoch:  12 , batch:  28 , test loss:  17.416797637939453\n","epoch:  12 , batch:  29 , test loss:  23.960477828979492\n","epoch:  12 , batch:  30 , test loss:  20.142446517944336\n","epoch:  12 , batch:  31 , test loss:  9.530954360961914\n","epoch:  12 , batch:  32 , test loss:  21.54414939880371\n","epoch:  12 , batch:  33 , test loss:  20.9680118560791\n","epoch:  12 , batch:  34 , test loss:  20.45380401611328\n","epoch:  12 , batch:  35 , test loss:  11.995025634765625\n","epoch:  12 , batch:  36 , test loss:  19.17938995361328\n","epoch:  12 , batch:  37 , test loss:  18.23052406311035\n","epoch:  12 , batch:  38 , test loss:  11.800009727478027\n","epoch:  12 , batch:  39 , test loss:  20.162368774414062\n","epoch:  12 , batch:  40 , test loss:  19.63052749633789\n","epoch:  12 , batch:  41 , test loss:  20.734962463378906\n","epoch:  12 , batch:  42 , test loss:  29.06267547607422\n","epoch:  12 , batch:  43 , test loss:  29.38816261291504\n","epoch:  12 , batch:  44 , test loss:  18.36436653137207\n","epoch:  12 , batch:  45 , test loss:  22.013288497924805\n","epoch:  12 , batch:  46 , test loss:  14.812782287597656\n","epoch:  12 , batch:  47 , test loss:  12.721427917480469\n","epoch:  12 , batch:  48 , test loss:  29.779312133789062\n","epoch:  12 , batch:  49 , test loss:  16.503435134887695\n","epoch:  12 , batch:  50 , test loss:  5.675487518310547\n","epoch:  12 , batch:  51 , test loss:  21.581951141357422\n","epoch:  12 , batch:  52 , test loss:  12.419224739074707\n","epoch:  12 , batch:  53 , test loss:  9.946844100952148\n","epoch:  12 , batch:  54 , test loss:  21.573965072631836\n","epoch:  12 , batch:  55 , test loss:  9.411356925964355\n","epoch:  12 , batch:  56 , test loss:  11.604838371276855\n","epoch:  12 , batch:  57 , test loss:  27.566484451293945\n","epoch:  12 , batch:  58 , test loss:  17.441774368286133\n","epoch:  12 , batch:  59 , test loss:  13.115317344665527\n","epoch:  12 , batch:  60 , test loss:  18.197019577026367\n","epoch:  12 , batch:  61 , test loss:  17.479379653930664\n","epoch:  12 , batch:  62 , test loss:  23.45355796813965\n","epoch:  12 , batch:  63 , test loss:  22.93602180480957\n","epoch:  12 , batch:  64 , test loss:  20.286449432373047\n","epoch:  12 , batch:  65 , test loss:  22.68935775756836\n","epoch:  12 , batch:  66 , test loss:  20.95535659790039\n","epoch:  12 , batch:  67 , test loss:  32.27751541137695\n","epoch:  12 , batch:  68 , test loss:  16.169397354125977\n","epoch:  12 , batch:  69 , test loss:  9.382170677185059\n","epoch:  12 , batch:  70 , test loss:  30.40770721435547\n","epoch:  12 , batch:  71 , test loss:  19.6347599029541\n","epoch:  12 , batch:  72 , test loss:  21.069828033447266\n","epoch:  12 , batch:  73 , test loss:  24.802623748779297\n","epoch:  12 , batch:  74 , test loss:  21.062965393066406\n","epoch:  12 , batch:  75 , test loss:  12.428181648254395\n","epoch:  12 , batch:  76 , test loss:  19.10033416748047\n","epoch:  12 , batch:  77 , test loss:  17.557321548461914\n","epoch:  12 , batch:  78 , test loss:  24.051212310791016\n","epoch:  12 , batch:  79 , test loss:  22.475744247436523\n","epoch:  12 , batch:  80 , test loss:  19.07929801940918\n","epoch:  12 , batch:  81 , test loss:  36.8592643737793\n","epoch:  12 , batch:  82 , test loss:  30.98325538635254\n","epoch:  12 , batch:  83 , test loss:  16.695968627929688\n","epoch:  12 , batch:  84 , test loss:  8.35535717010498\n","epoch:  12 , batch:  85 , test loss:  22.384523391723633\n","epoch:  12 , batch:  86 , test loss:  25.118741989135742\n","epoch:  12 , batch:  87 , test loss:  15.722715377807617\n","epoch:  12 , batch:  88 , test loss:  18.127126693725586\n","epoch:  12 , batch:  89 , test loss:  22.0291805267334\n","epoch:  12 , batch:  90 , test loss:  14.501279830932617\n","epoch:  12 , batch:  91 , test loss:  23.66611099243164\n","epoch:  12 , batch:  92 , test loss:  17.689395904541016\n","epoch:  12 , batch:  93 , test loss:  16.193363189697266\n","epoch:  12 , batch:  94 , test loss:  24.765724182128906\n","epoch:  12 , batch:  95 , test loss:  45.179039001464844\n","epoch:  12 , batch:  96 , test loss:  20.046463012695312\n","epoch:  12 , batch:  97 , test loss:  16.040246963500977\n","epoch:  12 , batch:  98 , test loss:  46.3817138671875\n","epoch:  12 , batch:  99 , test loss:  25.881258010864258\n","epoch:  12 , batch:  100 , test loss:  26.589826583862305\n","epoch:  12 , batch:  101 , test loss:  7.608818531036377\n","epoch:  12 , batch:  102 , test loss:  29.70923614501953\n","epoch:  12 , batch:  103 , test loss:  11.387664794921875\n","epoch:  12 , batch:  104 , test loss:  21.939918518066406\n","epoch:  12 , batch:  105 , test loss:  23.522804260253906\n","epoch:  12 , batch:  106 , test loss:  25.986114501953125\n","epoch:  12 , batch:  107 , test loss:  17.118961334228516\n","epoch:  12 , batch:  108 , test loss:  17.941341400146484\n","epoch:  12 , batch:  109 , test loss:  14.25614070892334\n","epoch:  12 , batch:  110 , test loss:  22.046663284301758\n","epoch:  12 , batch:  111 , test loss:  9.849831581115723\n","epoch:  12 , batch:  112 , test loss:  16.440311431884766\n","epoch:  12 , batch:  113 , test loss:  25.324951171875\n","epoch:  12 , batch:  114 , test loss:  35.83223342895508\n","epoch:  12 , batch:  115 , test loss:  26.490909576416016\n","epoch:  12 , batch:  116 , test loss:  19.354625701904297\n","epoch:  12 , batch:  117 , test loss:  18.25836944580078\n","epoch:  12 , batch:  118 , test loss:  31.857547760009766\n","epoch:  12 , batch:  119 , test loss:  12.29377269744873\n","epoch:  12 , batch:  120 , test loss:  29.97704315185547\n","epoch:  12 , batch:  121 , test loss:  19.08450698852539\n","epoch:  12 , batch:  122 , test loss:  20.870410919189453\n","epoch:  12 , batch:  123 , test loss:  16.896404266357422\n","epoch:  12 , batch:  124 , test loss:  18.801740646362305\n","epoch:  12 , batch:  125 , test loss:  26.873823165893555\n","epoch:  12 , batch:  126 , test loss:  25.419343948364258\n","epoch:  12 , batch:  127 , test loss:  30.25849723815918\n","epoch:  12 , batch:  128 , test loss:  17.289716720581055\n","epoch:  12 , batch:  129 , test loss:  16.442392349243164\n","epoch:  12 , batch:  130 , test loss:  22.633054733276367\n","epoch:  12 , batch:  131 , test loss:  19.796432495117188\n","epoch:  12 , batch:  132 , test loss:  21.191593170166016\n","epoch:  12 , batch:  133 , test loss:  14.342255592346191\n","epoch:  12 , batch:  134 , test loss:  24.539491653442383\n","epoch:  12 , batch:  135 , test loss:  15.71660327911377\n","epoch:  12 , batch:  136 , test loss:  20.240345001220703\n","epoch:  12 , batch:  137 , test loss:  16.584823608398438\n","epoch:  12 , batch:  138 , test loss:  11.56773567199707\n","epoch:  12 , batch:  139 , test loss:  24.260587692260742\n","epoch:  12 , batch:  140 , test loss:  15.456143379211426\n","epoch:  12 , batch:  141 , test loss:  20.04660415649414\n","epoch:  12 , batch:  142 , test loss:  30.534578323364258\n","epoch:  12 , batch:  143 , test loss:  38.61589813232422\n","epoch:  12 , batch:  144 , test loss:  18.76971435546875\n","epoch:  12 , batch:  145 , test loss:  19.14290428161621\n","epoch:  12 , batch:  146 , test loss:  25.674875259399414\n","epoch:  12 , batch:  147 , test loss:  22.263547897338867\n","epoch:  12 , batch:  148 , test loss:  21.51919937133789\n","epoch:  12 , batch:  149 , test loss:  22.643564224243164\n","epoch:  12 , batch:  150 , test loss:  18.769052505493164\n","epoch:  12 , batch:  151 , test loss:  17.262460708618164\n","epoch:  12 , batch:  152 , test loss:  8.003872871398926\n","epoch:  12 , batch:  153 , test loss:  36.05671310424805\n","epoch:  12 , batch:  154 , test loss:  15.758010864257812\n","epoch:  12 , batch:  155 , test loss:  16.717979431152344\n","epoch:  12 , batch:  156 , test loss:  4.341669082641602\n","Accuracy of pytorch_model set: 0.8847\n","epoch:  13 , batch:  0 , train loss:  17.798480987548828\n","epoch:  13 , batch:  1 , train loss:  22.707691192626953\n","epoch:  13 , batch:  2 , train loss:  7.584842681884766\n","epoch:  13 , batch:  3 , train loss:  13.512375831604004\n","epoch:  13 , batch:  4 , train loss:  15.068099021911621\n","epoch:  13 , batch:  5 , train loss:  13.968411445617676\n","epoch:  13 , batch:  6 , train loss:  10.8316650390625\n","epoch:  13 , batch:  7 , train loss:  20.020166397094727\n","epoch:  13 , batch:  8 , train loss:  19.602922439575195\n","epoch:  13 , batch:  9 , train loss:  20.664419174194336\n","epoch:  13 , batch:  10 , train loss:  25.127832412719727\n","epoch:  13 , batch:  11 , train loss:  8.377903938293457\n","epoch:  13 , batch:  12 , train loss:  19.334030151367188\n","epoch:  13 , batch:  13 , train loss:  13.017104148864746\n","epoch:  13 , batch:  14 , train loss:  15.708218574523926\n","epoch:  13 , batch:  15 , train loss:  9.758010864257812\n","epoch:  13 , batch:  16 , train loss:  9.154870986938477\n","epoch:  13 , batch:  17 , train loss:  13.903542518615723\n","epoch:  13 , batch:  18 , train loss:  4.4270405769348145\n","epoch:  13 , batch:  19 , train loss:  7.386143207550049\n","epoch:  13 , batch:  20 , train loss:  33.412017822265625\n","epoch:  13 , batch:  21 , train loss:  19.63248634338379\n","epoch:  13 , batch:  22 , train loss:  12.071950912475586\n","epoch:  13 , batch:  23 , train loss:  15.817971229553223\n","epoch:  13 , batch:  24 , train loss:  15.398439407348633\n","epoch:  13 , batch:  25 , train loss:  15.341073989868164\n","epoch:  13 , batch:  26 , train loss:  17.958200454711914\n","epoch:  13 , batch:  27 , train loss:  14.073829650878906\n","epoch:  13 , batch:  28 , train loss:  13.606602668762207\n","epoch:  13 , batch:  29 , train loss:  19.404813766479492\n","epoch:  13 , batch:  30 , train loss:  9.086173057556152\n","epoch:  13 , batch:  31 , train loss:  10.929126739501953\n","epoch:  13 , batch:  32 , train loss:  14.588767051696777\n","epoch:  13 , batch:  33 , train loss:  22.027507781982422\n","epoch:  13 , batch:  34 , train loss:  6.236301898956299\n","epoch:  13 , batch:  35 , train loss:  8.838079452514648\n","epoch:  13 , batch:  36 , train loss:  8.370147705078125\n","epoch:  13 , batch:  37 , train loss:  12.134328842163086\n","epoch:  13 , batch:  38 , train loss:  8.346954345703125\n","epoch:  13 , batch:  39 , train loss:  17.03278923034668\n","epoch:  13 , batch:  40 , train loss:  12.506492614746094\n","epoch:  13 , batch:  41 , train loss:  17.14014434814453\n","epoch:  13 , batch:  42 , train loss:  17.913049697875977\n","epoch:  13 , batch:  43 , train loss:  16.12537384033203\n","epoch:  13 , batch:  44 , train loss:  6.655137062072754\n","epoch:  13 , batch:  45 , train loss:  20.712966918945312\n","epoch:  13 , batch:  46 , train loss:  12.50403881072998\n","epoch:  13 , batch:  47 , train loss:  8.290200233459473\n","epoch:  13 , batch:  48 , train loss:  7.522519111633301\n","epoch:  13 , batch:  49 , train loss:  15.974679946899414\n","epoch:  13 , batch:  50 , train loss:  31.428022384643555\n","epoch:  13 , batch:  51 , train loss:  17.556209564208984\n","epoch:  13 , batch:  52 , train loss:  16.621761322021484\n","epoch:  13 , batch:  53 , train loss:  13.429195404052734\n","epoch:  13 , batch:  54 , train loss:  17.73323631286621\n","epoch:  13 , batch:  55 , train loss:  17.567668914794922\n","epoch:  13 , batch:  56 , train loss:  12.269951820373535\n","epoch:  13 , batch:  57 , train loss:  9.577698707580566\n","epoch:  13 , batch:  58 , train loss:  20.22148323059082\n","epoch:  13 , batch:  59 , train loss:  11.594584465026855\n","epoch:  13 , batch:  60 , train loss:  25.326501846313477\n","epoch:  13 , batch:  61 , train loss:  16.91986083984375\n","epoch:  13 , batch:  62 , train loss:  20.617509841918945\n","epoch:  13 , batch:  63 , train loss:  7.839509963989258\n","epoch:  13 , batch:  64 , train loss:  14.938551902770996\n","epoch:  13 , batch:  65 , train loss:  18.53684425354004\n","epoch:  13 , batch:  66 , train loss:  8.988131523132324\n","epoch:  13 , batch:  67 , train loss:  9.72087574005127\n","epoch:  13 , batch:  68 , train loss:  15.673945426940918\n","epoch:  13 , batch:  69 , train loss:  6.954256534576416\n","epoch:  13 , batch:  70 , train loss:  8.489150047302246\n","epoch:  13 , batch:  71 , train loss:  5.8230061531066895\n","epoch:  13 , batch:  72 , train loss:  28.376609802246094\n","epoch:  13 , batch:  73 , train loss:  14.189911842346191\n","epoch:  13 , batch:  74 , train loss:  14.2791748046875\n","epoch:  13 , batch:  75 , train loss:  13.571123123168945\n","epoch:  13 , batch:  76 , train loss:  17.683706283569336\n","epoch:  13 , batch:  77 , train loss:  15.315923690795898\n","epoch:  13 , batch:  78 , train loss:  11.174217224121094\n","epoch:  13 , batch:  79 , train loss:  12.176913261413574\n","epoch:  13 , batch:  80 , train loss:  11.02724552154541\n","epoch:  13 , batch:  81 , train loss:  10.435271263122559\n","epoch:  13 , batch:  82 , train loss:  14.052783012390137\n","epoch:  13 , batch:  83 , train loss:  14.2287015914917\n","epoch:  13 , batch:  84 , train loss:  8.022658348083496\n","epoch:  13 , batch:  85 , train loss:  16.391813278198242\n","epoch:  13 , batch:  86 , train loss:  17.34578514099121\n","epoch:  13 , batch:  87 , train loss:  14.673871040344238\n","epoch:  13 , batch:  88 , train loss:  11.07363510131836\n","epoch:  13 , batch:  89 , train loss:  26.713871002197266\n","epoch:  13 , batch:  90 , train loss:  12.339067459106445\n","epoch:  13 , batch:  91 , train loss:  13.37522029876709\n","epoch:  13 , batch:  92 , train loss:  8.772331237792969\n","epoch:  13 , batch:  93 , train loss:  15.177774429321289\n","epoch:  13 , batch:  94 , train loss:  15.102493286132812\n","epoch:  13 , batch:  95 , train loss:  21.967466354370117\n","epoch:  13 , batch:  96 , train loss:  20.0084285736084\n","epoch:  13 , batch:  97 , train loss:  22.604860305786133\n","epoch:  13 , batch:  98 , train loss:  22.230091094970703\n","epoch:  13 , batch:  99 , train loss:  10.925712585449219\n","epoch:  13 , batch:  100 , train loss:  5.864669322967529\n","epoch:  13 , batch:  101 , train loss:  17.553159713745117\n","epoch:  13 , batch:  102 , train loss:  12.39006519317627\n","epoch:  13 , batch:  103 , train loss:  10.24385929107666\n","epoch:  13 , batch:  104 , train loss:  12.070508003234863\n","epoch:  13 , batch:  105 , train loss:  8.776796340942383\n","epoch:  13 , batch:  106 , train loss:  8.106810569763184\n","epoch:  13 , batch:  107 , train loss:  9.1876220703125\n","epoch:  13 , batch:  108 , train loss:  16.91638946533203\n","epoch:  13 , batch:  109 , train loss:  11.655336380004883\n","epoch:  13 , batch:  110 , train loss:  11.718786239624023\n","epoch:  13 , batch:  111 , train loss:  30.158353805541992\n","epoch:  13 , batch:  112 , train loss:  18.029861450195312\n","epoch:  13 , batch:  113 , train loss:  19.047006607055664\n","epoch:  13 , batch:  114 , train loss:  8.054865837097168\n","epoch:  13 , batch:  115 , train loss:  16.11699867248535\n","epoch:  13 , batch:  116 , train loss:  19.808313369750977\n","epoch:  13 , batch:  117 , train loss:  22.712203979492188\n","epoch:  13 , batch:  118 , train loss:  20.991506576538086\n","epoch:  13 , batch:  119 , train loss:  11.44205379486084\n","epoch:  13 , batch:  120 , train loss:  15.216728210449219\n","epoch:  13 , batch:  121 , train loss:  15.521589279174805\n","epoch:  13 , batch:  122 , train loss:  24.135698318481445\n","epoch:  13 , batch:  123 , train loss:  9.636751174926758\n","epoch:  13 , batch:  124 , train loss:  21.175344467163086\n","epoch:  13 , batch:  125 , train loss:  17.800376892089844\n","epoch:  13 , batch:  126 , train loss:  14.838250160217285\n","epoch:  13 , batch:  127 , train loss:  23.877513885498047\n","epoch:  13 , batch:  128 , train loss:  12.941837310791016\n","epoch:  13 , batch:  129 , train loss:  18.7088623046875\n","epoch:  13 , batch:  130 , train loss:  10.556796073913574\n","epoch:  13 , batch:  131 , train loss:  13.930471420288086\n","epoch:  13 , batch:  132 , train loss:  16.629823684692383\n","epoch:  13 , batch:  133 , train loss:  14.27762222290039\n","epoch:  13 , batch:  134 , train loss:  14.231739044189453\n","epoch:  13 , batch:  135 , train loss:  18.45001220703125\n","epoch:  13 , batch:  136 , train loss:  27.075132369995117\n","epoch:  13 , batch:  137 , train loss:  10.570591926574707\n","epoch:  13 , batch:  138 , train loss:  15.243217468261719\n","epoch:  13 , batch:  139 , train loss:  12.831646919250488\n","epoch:  13 , batch:  140 , train loss:  14.499340057373047\n","epoch:  13 , batch:  141 , train loss:  16.685211181640625\n","epoch:  13 , batch:  142 , train loss:  9.773670196533203\n","epoch:  13 , batch:  143 , train loss:  12.442412376403809\n","epoch:  13 , batch:  144 , train loss:  14.344179153442383\n","epoch:  13 , batch:  145 , train loss:  20.29022979736328\n","epoch:  13 , batch:  146 , train loss:  9.385095596313477\n","epoch:  13 , batch:  147 , train loss:  5.834476947784424\n","epoch:  13 , batch:  148 , train loss:  8.46873664855957\n","epoch:  13 , batch:  149 , train loss:  10.659517288208008\n","epoch:  13 , batch:  150 , train loss:  26.735471725463867\n","epoch:  13 , batch:  151 , train loss:  8.375264167785645\n","epoch:  13 , batch:  152 , train loss:  9.090156555175781\n","epoch:  13 , batch:  153 , train loss:  20.507078170776367\n","epoch:  13 , batch:  154 , train loss:  30.835712432861328\n","epoch:  13 , batch:  155 , train loss:  16.3995361328125\n","epoch:  13 , batch:  156 , train loss:  12.60178279876709\n","epoch:  13 , batch:  157 , train loss:  14.243243217468262\n","epoch:  13 , batch:  158 , train loss:  22.67603302001953\n","epoch:  13 , batch:  159 , train loss:  13.600049018859863\n","epoch:  13 , batch:  160 , train loss:  8.439815521240234\n","epoch:  13 , batch:  161 , train loss:  15.615530014038086\n","epoch:  13 , batch:  162 , train loss:  11.868152618408203\n","epoch:  13 , batch:  163 , train loss:  19.613065719604492\n","epoch:  13 , batch:  164 , train loss:  12.00067138671875\n","epoch:  13 , batch:  165 , train loss:  9.499151229858398\n","epoch:  13 , batch:  166 , train loss:  13.005714416503906\n","epoch:  13 , batch:  167 , train loss:  17.026397705078125\n","epoch:  13 , batch:  168 , train loss:  12.021705627441406\n","epoch:  13 , batch:  169 , train loss:  11.476639747619629\n","epoch:  13 , batch:  170 , train loss:  7.184019088745117\n","epoch:  13 , batch:  171 , train loss:  14.884971618652344\n","epoch:  13 , batch:  172 , train loss:  7.8188323974609375\n","epoch:  13 , batch:  173 , train loss:  14.94141960144043\n","epoch:  13 , batch:  174 , train loss:  14.662508964538574\n","epoch:  13 , batch:  175 , train loss:  14.650263786315918\n","epoch:  13 , batch:  176 , train loss:  15.63267993927002\n","epoch:  13 , batch:  177 , train loss:  11.528220176696777\n","epoch:  13 , batch:  178 , train loss:  8.690775871276855\n","epoch:  13 , batch:  179 , train loss:  21.754880905151367\n","epoch:  13 , batch:  180 , train loss:  15.373957633972168\n","epoch:  13 , batch:  181 , train loss:  7.067142486572266\n","epoch:  13 , batch:  182 , train loss:  13.414934158325195\n","epoch:  13 , batch:  183 , train loss:  9.74262809753418\n","epoch:  13 , batch:  184 , train loss:  7.7270894050598145\n","epoch:  13 , batch:  185 , train loss:  29.93840980529785\n","epoch:  13 , batch:  186 , train loss:  11.223944664001465\n","epoch:  13 , batch:  187 , train loss:  11.873556137084961\n","epoch:  13 , batch:  188 , train loss:  8.842252731323242\n","epoch:  13 , batch:  189 , train loss:  22.88897705078125\n","epoch:  13 , batch:  190 , train loss:  13.645339965820312\n","epoch:  13 , batch:  191 , train loss:  19.86180877685547\n","epoch:  13 , batch:  192 , train loss:  20.014585494995117\n","epoch:  13 , batch:  193 , train loss:  18.085716247558594\n","epoch:  13 , batch:  194 , train loss:  9.316715240478516\n","epoch:  13 , batch:  195 , train loss:  9.351163864135742\n","epoch:  13 , batch:  196 , train loss:  9.817111015319824\n","epoch:  13 , batch:  197 , train loss:  13.280252456665039\n","epoch:  13 , batch:  198 , train loss:  12.467329025268555\n","epoch:  13 , batch:  199 , train loss:  7.403755187988281\n","epoch:  13 , batch:  200 , train loss:  14.984177589416504\n","epoch:  13 , batch:  201 , train loss:  14.603097915649414\n","epoch:  13 , batch:  202 , train loss:  19.825166702270508\n","epoch:  13 , batch:  203 , train loss:  9.105374336242676\n","epoch:  13 , batch:  204 , train loss:  8.681325912475586\n","epoch:  13 , batch:  205 , train loss:  23.06273651123047\n","epoch:  13 , batch:  206 , train loss:  22.530588150024414\n","epoch:  13 , batch:  207 , train loss:  11.970329284667969\n","epoch:  13 , batch:  208 , train loss:  8.103967666625977\n","epoch:  13 , batch:  209 , train loss:  28.802337646484375\n","epoch:  13 , batch:  210 , train loss:  30.341415405273438\n","epoch:  13 , batch:  211 , train loss:  17.340341567993164\n","epoch:  13 , batch:  212 , train loss:  32.022613525390625\n","epoch:  13 , batch:  213 , train loss:  20.664443969726562\n","epoch:  13 , batch:  214 , train loss:  14.094465255737305\n","epoch:  13 , batch:  215 , train loss:  17.465524673461914\n","epoch:  13 , batch:  216 , train loss:  14.249855041503906\n","epoch:  13 , batch:  217 , train loss:  9.760478973388672\n","epoch:  13 , batch:  218 , train loss:  13.895176887512207\n","epoch:  13 , batch:  219 , train loss:  8.850664138793945\n","epoch:  13 , batch:  220 , train loss:  16.946992874145508\n","epoch:  13 , batch:  221 , train loss:  18.650182723999023\n","epoch:  13 , batch:  222 , train loss:  13.815589904785156\n","epoch:  13 , batch:  223 , train loss:  11.9306001663208\n","epoch:  13 , batch:  224 , train loss:  24.18006134033203\n","epoch:  13 , batch:  225 , train loss:  10.945409774780273\n","epoch:  13 , batch:  226 , train loss:  9.001901626586914\n","epoch:  13 , batch:  227 , train loss:  19.497272491455078\n","epoch:  13 , batch:  228 , train loss:  17.07556915283203\n","epoch:  13 , batch:  229 , train loss:  10.87413215637207\n","epoch:  13 , batch:  230 , train loss:  16.780811309814453\n","epoch:  13 , batch:  231 , train loss:  14.260908126831055\n","epoch:  13 , batch:  232 , train loss:  13.252212524414062\n","epoch:  13 , batch:  233 , train loss:  17.571979522705078\n","epoch:  13 , batch:  234 , train loss:  20.89404296875\n","epoch:  13 , batch:  235 , train loss:  15.620016098022461\n","epoch:  13 , batch:  236 , train loss:  17.947376251220703\n","epoch:  13 , batch:  237 , train loss:  24.323909759521484\n","epoch:  13 , batch:  238 , train loss:  19.926992416381836\n","epoch:  13 , batch:  239 , train loss:  15.658434867858887\n","epoch:  13 , batch:  240 , train loss:  13.0247802734375\n","epoch:  13 , batch:  241 , train loss:  11.999792098999023\n","epoch:  13 , batch:  242 , train loss:  22.11693572998047\n","epoch:  13 , batch:  243 , train loss:  8.372357368469238\n","epoch:  13 , batch:  244 , train loss:  28.153202056884766\n","epoch:  13 , batch:  245 , train loss:  14.84329605102539\n","epoch:  13 , batch:  246 , train loss:  10.158723831176758\n","epoch:  13 , batch:  247 , train loss:  13.872775077819824\n","epoch:  13 , batch:  248 , train loss:  12.471805572509766\n","epoch:  13 , batch:  249 , train loss:  12.868705749511719\n","epoch:  13 , batch:  250 , train loss:  8.592181205749512\n","epoch:  13 , batch:  251 , train loss:  13.693032264709473\n","epoch:  13 , batch:  252 , train loss:  15.381304740905762\n","epoch:  13 , batch:  253 , train loss:  18.458478927612305\n","epoch:  13 , batch:  254 , train loss:  8.1259126663208\n","epoch:  13 , batch:  255 , train loss:  20.225309371948242\n","epoch:  13 , batch:  256 , train loss:  8.566781044006348\n","epoch:  13 , batch:  257 , train loss:  13.407733917236328\n","epoch:  13 , batch:  258 , train loss:  15.359844207763672\n","epoch:  13 , batch:  259 , train loss:  18.211139678955078\n","epoch:  13 , batch:  260 , train loss:  15.810306549072266\n","epoch:  13 , batch:  261 , train loss:  13.814489364624023\n","epoch:  13 , batch:  262 , train loss:  9.517780303955078\n","epoch:  13 , batch:  263 , train loss:  12.019486427307129\n","epoch:  13 , batch:  264 , train loss:  12.05532455444336\n","epoch:  13 , batch:  265 , train loss:  19.187091827392578\n","epoch:  13 , batch:  266 , train loss:  16.60164451599121\n","epoch:  13 , batch:  267 , train loss:  15.579837799072266\n","epoch:  13 , batch:  268 , train loss:  11.21873950958252\n","epoch:  13 , batch:  269 , train loss:  9.537948608398438\n","epoch:  13 , batch:  270 , train loss:  9.791958808898926\n","epoch:  13 , batch:  271 , train loss:  9.253368377685547\n","epoch:  13 , batch:  272 , train loss:  15.934440612792969\n","epoch:  13 , batch:  273 , train loss:  14.380743980407715\n","epoch:  13 , batch:  274 , train loss:  13.885951042175293\n","epoch:  13 , batch:  275 , train loss:  31.515216827392578\n","epoch:  13 , batch:  276 , train loss:  15.392354011535645\n","epoch:  13 , batch:  277 , train loss:  17.631792068481445\n","epoch:  13 , batch:  278 , train loss:  19.324909210205078\n","epoch:  13 , batch:  279 , train loss:  25.64130401611328\n","epoch:  13 , batch:  280 , train loss:  11.819307327270508\n","epoch:  13 , batch:  281 , train loss:  15.304408073425293\n","epoch:  13 , batch:  282 , train loss:  15.298140525817871\n","epoch:  13 , batch:  283 , train loss:  11.651182174682617\n","epoch:  13 , batch:  284 , train loss:  16.755006790161133\n","epoch:  13 , batch:  285 , train loss:  15.689574241638184\n","epoch:  13 , batch:  286 , train loss:  28.359207153320312\n","epoch:  13 , batch:  287 , train loss:  13.96848201751709\n","epoch:  13 , batch:  288 , train loss:  14.025487899780273\n","epoch:  13 , batch:  289 , train loss:  13.344483375549316\n","epoch:  13 , batch:  290 , train loss:  21.531347274780273\n","epoch:  13 , batch:  291 , train loss:  13.72991943359375\n","epoch:  13 , batch:  292 , train loss:  17.76149559020996\n","epoch:  13 , batch:  293 , train loss:  21.358137130737305\n","epoch:  13 , batch:  294 , train loss:  17.989206314086914\n","epoch:  13 , batch:  295 , train loss:  22.495756149291992\n","epoch:  13 , batch:  296 , train loss:  10.199814796447754\n","epoch:  13 , batch:  297 , train loss:  16.825546264648438\n","epoch:  13 , batch:  298 , train loss:  12.298588752746582\n","epoch:  13 , batch:  299 , train loss:  14.361763954162598\n","epoch:  13 , batch:  300 , train loss:  12.32792854309082\n","epoch:  13 , batch:  301 , train loss:  10.305245399475098\n","epoch:  13 , batch:  302 , train loss:  8.835994720458984\n","epoch:  13 , batch:  303 , train loss:  12.924031257629395\n","epoch:  13 , batch:  304 , train loss:  23.6767635345459\n","epoch:  13 , batch:  305 , train loss:  10.680225372314453\n","epoch:  13 , batch:  306 , train loss:  9.091033935546875\n","epoch:  13 , batch:  307 , train loss:  15.976901054382324\n","epoch:  13 , batch:  308 , train loss:  11.945703506469727\n","epoch:  13 , batch:  309 , train loss:  20.927175521850586\n","epoch:  13 , batch:  310 , train loss:  8.74183464050293\n","epoch:  13 , batch:  311 , train loss:  8.643308639526367\n","epoch:  13 , batch:  312 , train loss:  20.78532600402832\n","epoch:  13 , batch:  313 , train loss:  6.134415626525879\n","epoch:  13 , batch:  314 , train loss:  16.587493896484375\n","epoch:  13 , batch:  315 , train loss:  21.138290405273438\n","epoch:  13 , batch:  316 , train loss:  19.078012466430664\n","epoch:  13 , batch:  317 , train loss:  13.061017990112305\n","epoch:  13 , batch:  318 , train loss:  17.023324966430664\n","epoch:  13 , batch:  319 , train loss:  17.72007179260254\n","epoch:  13 , batch:  320 , train loss:  16.5742244720459\n","epoch:  13 , batch:  321 , train loss:  12.238117218017578\n","epoch:  13 , batch:  322 , train loss:  24.287214279174805\n","epoch:  13 , batch:  323 , train loss:  14.550360679626465\n","epoch:  13 , batch:  324 , train loss:  15.362797737121582\n","epoch:  13 , batch:  325 , train loss:  15.089667320251465\n","epoch:  13 , batch:  326 , train loss:  12.104119300842285\n","epoch:  13 , batch:  327 , train loss:  13.278188705444336\n","epoch:  13 , batch:  328 , train loss:  2.8032684326171875\n","epoch:  13 , batch:  329 , train loss:  14.440555572509766\n","epoch:  13 , batch:  330 , train loss:  10.093313217163086\n","epoch:  13 , batch:  331 , train loss:  13.435951232910156\n","epoch:  13 , batch:  332 , train loss:  12.178305625915527\n","epoch:  13 , batch:  333 , train loss:  10.081535339355469\n","epoch:  13 , batch:  334 , train loss:  18.82083511352539\n","epoch:  13 , batch:  335 , train loss:  24.022207260131836\n","epoch:  13 , batch:  336 , train loss:  13.339554786682129\n","epoch:  13 , batch:  337 , train loss:  17.899694442749023\n","epoch:  13 , batch:  338 , train loss:  12.3203125\n","epoch:  13 , batch:  339 , train loss:  30.876501083374023\n","epoch:  13 , batch:  340 , train loss:  20.23911476135254\n","epoch:  13 , batch:  341 , train loss:  15.78473949432373\n","epoch:  13 , batch:  342 , train loss:  9.941082000732422\n","epoch:  13 , batch:  343 , train loss:  10.311707496643066\n","epoch:  13 , batch:  344 , train loss:  14.695745468139648\n","epoch:  13 , batch:  345 , train loss:  33.4311408996582\n","epoch:  13 , batch:  346 , train loss:  24.51105308532715\n","epoch:  13 , batch:  347 , train loss:  12.15178394317627\n","epoch:  13 , batch:  348 , train loss:  10.133662223815918\n","epoch:  13 , batch:  349 , train loss:  5.955564022064209\n","epoch:  13 , batch:  350 , train loss:  18.785900115966797\n","epoch:  13 , batch:  351 , train loss:  24.283794403076172\n","epoch:  13 , batch:  352 , train loss:  7.586830139160156\n","epoch:  13 , batch:  353 , train loss:  4.905032634735107\n","epoch:  13 , batch:  354 , train loss:  10.84048080444336\n","epoch:  13 , batch:  355 , train loss:  9.006902694702148\n","epoch:  13 , batch:  356 , train loss:  11.87265682220459\n","epoch:  13 , batch:  357 , train loss:  13.614740371704102\n","epoch:  13 , batch:  358 , train loss:  11.25727653503418\n","epoch:  13 , batch:  359 , train loss:  16.078012466430664\n","epoch:  13 , batch:  360 , train loss:  21.98159408569336\n","epoch:  13 , batch:  361 , train loss:  18.823408126831055\n","epoch:  13 , batch:  362 , train loss:  10.56306266784668\n","epoch:  13 , batch:  363 , train loss:  25.33611488342285\n","epoch:  13 , batch:  364 , train loss:  17.680397033691406\n","epoch:  13 , batch:  365 , train loss:  20.677013397216797\n","epoch:  13 , batch:  366 , train loss:  23.94530487060547\n","epoch:  13 , batch:  367 , train loss:  12.054119110107422\n","epoch:  13 , batch:  368 , train loss:  14.076200485229492\n","epoch:  13 , batch:  369 , train loss:  8.198311805725098\n","epoch:  13 , batch:  370 , train loss:  13.63049030303955\n","epoch:  13 , batch:  371 , train loss:  15.40334415435791\n","epoch:  13 , batch:  372 , train loss:  8.455552101135254\n","epoch:  13 , batch:  373 , train loss:  11.308570861816406\n","epoch:  13 , batch:  374 , train loss:  9.753799438476562\n","epoch:  13 , batch:  375 , train loss:  19.029987335205078\n","epoch:  13 , batch:  376 , train loss:  13.98901653289795\n","epoch:  13 , batch:  377 , train loss:  12.373825073242188\n","epoch:  13 , batch:  378 , train loss:  16.540170669555664\n","epoch:  13 , batch:  379 , train loss:  7.55964469909668\n","epoch:  13 , batch:  380 , train loss:  9.195758819580078\n","epoch:  13 , batch:  381 , train loss:  13.75247573852539\n","epoch:  13 , batch:  382 , train loss:  19.12066078186035\n","epoch:  13 , batch:  383 , train loss:  8.351901054382324\n","epoch:  13 , batch:  384 , train loss:  15.665348052978516\n","epoch:  13 , batch:  385 , train loss:  6.630523681640625\n","epoch:  13 , batch:  386 , train loss:  15.136906623840332\n","epoch:  13 , batch:  387 , train loss:  6.246739864349365\n","epoch:  13 , batch:  388 , train loss:  14.185364723205566\n","epoch:  13 , batch:  389 , train loss:  4.088903903961182\n","epoch:  13 , batch:  390 , train loss:  20.881147384643555\n","epoch:  13 , batch:  391 , train loss:  9.563315391540527\n","epoch:  13 , batch:  392 , train loss:  13.714123725891113\n","epoch:  13 , batch:  393 , train loss:  18.067615509033203\n","epoch:  13 , batch:  394 , train loss:  11.993389129638672\n","epoch:  13 , batch:  395 , train loss:  15.068467140197754\n","epoch:  13 , batch:  396 , train loss:  9.294844627380371\n","epoch:  13 , batch:  397 , train loss:  14.119094848632812\n","epoch:  13 , batch:  398 , train loss:  16.58277130126953\n","epoch:  13 , batch:  399 , train loss:  7.549978733062744\n","epoch:  13 , batch:  400 , train loss:  11.80986213684082\n","epoch:  13 , batch:  401 , train loss:  10.737112998962402\n","epoch:  13 , batch:  402 , train loss:  14.727988243103027\n","epoch:  13 , batch:  403 , train loss:  9.263760566711426\n","epoch:  13 , batch:  404 , train loss:  13.508715629577637\n","epoch:  13 , batch:  405 , train loss:  18.987361907958984\n","epoch:  13 , batch:  406 , train loss:  16.208980560302734\n","epoch:  13 , batch:  407 , train loss:  17.055538177490234\n","epoch:  13 , batch:  408 , train loss:  15.619279861450195\n","epoch:  13 , batch:  409 , train loss:  19.438390731811523\n","epoch:  13 , batch:  410 , train loss:  14.776712417602539\n","epoch:  13 , batch:  411 , train loss:  17.568859100341797\n","epoch:  13 , batch:  412 , train loss:  8.324612617492676\n","epoch:  13 , batch:  413 , train loss:  15.382675170898438\n","epoch:  13 , batch:  414 , train loss:  9.810674667358398\n","epoch:  13 , batch:  415 , train loss:  21.374378204345703\n","epoch:  13 , batch:  416 , train loss:  13.929835319519043\n","epoch:  13 , batch:  417 , train loss:  15.896862983703613\n","epoch:  13 , batch:  418 , train loss:  11.144908905029297\n","epoch:  13 , batch:  419 , train loss:  5.984688758850098\n","epoch:  13 , batch:  420 , train loss:  9.21470832824707\n","epoch:  13 , batch:  421 , train loss:  25.9521427154541\n","epoch:  13 , batch:  422 , train loss:  13.407532691955566\n","epoch:  13 , batch:  423 , train loss:  22.614299774169922\n","epoch:  13 , batch:  424 , train loss:  19.112409591674805\n","epoch:  13 , batch:  425 , train loss:  23.117061614990234\n","epoch:  13 , batch:  426 , train loss:  21.20734214782715\n","epoch:  13 , batch:  427 , train loss:  14.285955429077148\n","epoch:  13 , batch:  428 , train loss:  19.589534759521484\n","epoch:  13 , batch:  429 , train loss:  15.489376068115234\n","epoch:  13 , batch:  430 , train loss:  13.732524871826172\n","epoch:  13 , batch:  431 , train loss:  11.871456146240234\n","epoch:  13 , batch:  432 , train loss:  12.444658279418945\n","epoch:  13 , batch:  433 , train loss:  13.808931350708008\n","epoch:  13 , batch:  434 , train loss:  12.357044219970703\n","epoch:  13 , batch:  435 , train loss:  11.831136703491211\n","epoch:  13 , batch:  436 , train loss:  12.966053009033203\n","epoch:  13 , batch:  437 , train loss:  4.760185241699219\n","epoch:  13 , batch:  438 , train loss:  22.64158821105957\n","epoch:  13 , batch:  439 , train loss:  21.48368263244629\n","epoch:  13 , batch:  440 , train loss:  27.310684204101562\n","epoch:  13 , batch:  441 , train loss:  19.332300186157227\n","epoch:  13 , batch:  442 , train loss:  13.374829292297363\n","epoch:  13 , batch:  443 , train loss:  14.236335754394531\n","epoch:  13 , batch:  444 , train loss:  9.493694305419922\n","epoch:  13 , batch:  445 , train loss:  17.55830192565918\n","epoch:  13 , batch:  446 , train loss:  22.119619369506836\n","epoch:  13 , batch:  447 , train loss:  23.062917709350586\n","epoch:  13 , batch:  448 , train loss:  14.737449645996094\n","epoch:  13 , batch:  449 , train loss:  15.099346160888672\n","epoch:  13 , batch:  450 , train loss:  15.888162612915039\n","epoch:  13 , batch:  451 , train loss:  10.320314407348633\n","epoch:  13 , batch:  452 , train loss:  22.988780975341797\n","epoch:  13 , batch:  453 , train loss:  29.19282341003418\n","epoch:  13 , batch:  454 , train loss:  15.288371086120605\n","epoch:  13 , batch:  455 , train loss:  25.393169403076172\n","epoch:  13 , batch:  456 , train loss:  11.42691707611084\n","epoch:  13 , batch:  457 , train loss:  27.02646255493164\n","epoch:  13 , batch:  458 , train loss:  10.414353370666504\n","epoch:  13 , batch:  459 , train loss:  14.496795654296875\n","epoch:  13 , batch:  460 , train loss:  14.481088638305664\n","epoch:  13 , batch:  461 , train loss:  19.725265502929688\n","epoch:  13 , batch:  462 , train loss:  13.352983474731445\n","epoch:  13 , batch:  463 , train loss:  19.944246292114258\n","epoch:  13 , batch:  464 , train loss:  28.43462371826172\n","epoch:  13 , batch:  465 , train loss:  11.45588493347168\n","epoch:  13 , batch:  466 , train loss:  18.44474983215332\n","epoch:  13 , batch:  467 , train loss:  14.510106086730957\n","epoch:  13 , batch:  468 , train loss:  11.292901992797852\n","epoch:  13 , batch:  469 , train loss:  13.175003051757812\n","epoch:  13 , batch:  470 , train loss:  9.69930362701416\n","epoch:  13 , batch:  471 , train loss:  14.484651565551758\n","epoch:  13 , batch:  472 , train loss:  9.622337341308594\n","epoch:  13 , batch:  473 , train loss:  9.504841804504395\n","epoch:  13 , batch:  474 , train loss:  11.524191856384277\n","epoch:  13 , batch:  475 , train loss:  18.009706497192383\n","epoch:  13 , batch:  476 , train loss:  12.109893798828125\n","epoch:  13 , batch:  477 , train loss:  6.865375518798828\n","epoch:  13 , batch:  478 , train loss:  20.04694175720215\n","epoch:  13 , batch:  479 , train loss:  9.850532531738281\n","epoch:  13 , batch:  480 , train loss:  22.664838790893555\n","epoch:  13 , batch:  481 , train loss:  11.356372833251953\n","epoch:  13 , batch:  482 , train loss:  15.307576179504395\n","epoch:  13 , batch:  483 , train loss:  22.343584060668945\n","epoch:  13 , batch:  484 , train loss:  8.50230884552002\n","epoch:  13 , batch:  485 , train loss:  14.114994049072266\n","epoch:  13 , batch:  486 , train loss:  11.727825164794922\n","epoch:  13 , batch:  487 , train loss:  13.280695915222168\n","epoch:  13 , batch:  488 , train loss:  18.35285186767578\n","epoch:  13 , batch:  489 , train loss:  9.105568885803223\n","epoch:  13 , batch:  490 , train loss:  16.182371139526367\n","epoch:  13 , batch:  491 , train loss:  15.960018157958984\n","epoch:  13 , batch:  492 , train loss:  15.162038803100586\n","epoch:  13 , batch:  493 , train loss:  24.78326416015625\n","epoch:  13 , batch:  494 , train loss:  18.570064544677734\n","epoch:  13 , batch:  495 , train loss:  18.325031280517578\n","epoch:  13 , batch:  496 , train loss:  16.35622215270996\n","epoch:  13 , batch:  497 , train loss:  7.390745162963867\n","epoch:  13 , batch:  498 , train loss:  19.06975555419922\n","epoch:  13 , batch:  499 , train loss:  26.31554412841797\n","epoch:  13 , batch:  500 , train loss:  8.369915008544922\n","epoch:  13 , batch:  501 , train loss:  12.168843269348145\n","epoch:  13 , batch:  502 , train loss:  10.98247241973877\n","epoch:  13 , batch:  503 , train loss:  21.11246681213379\n","epoch:  13 , batch:  504 , train loss:  11.095662117004395\n","epoch:  13 , batch:  505 , train loss:  14.574686050415039\n","epoch:  13 , batch:  506 , train loss:  13.155367851257324\n","epoch:  13 , batch:  507 , train loss:  16.866788864135742\n","epoch:  13 , batch:  508 , train loss:  16.36035919189453\n","epoch:  13 , batch:  509 , train loss:  8.072492599487305\n","epoch:  13 , batch:  510 , train loss:  19.63802719116211\n","epoch:  13 , batch:  511 , train loss:  14.32951831817627\n","epoch:  13 , batch:  512 , train loss:  19.467222213745117\n","epoch:  13 , batch:  513 , train loss:  14.28890609741211\n","epoch:  13 , batch:  514 , train loss:  12.271784782409668\n","epoch:  13 , batch:  515 , train loss:  18.269189834594727\n","epoch:  13 , batch:  516 , train loss:  17.815616607666016\n","epoch:  13 , batch:  517 , train loss:  33.85280990600586\n","epoch:  13 , batch:  518 , train loss:  24.08099937438965\n","epoch:  13 , batch:  519 , train loss:  14.518670082092285\n","epoch:  13 , batch:  520 , train loss:  15.29028034210205\n","epoch:  13 , batch:  521 , train loss:  11.03844165802002\n","epoch:  13 , batch:  522 , train loss:  16.70931053161621\n","epoch:  13 , batch:  523 , train loss:  16.627826690673828\n","epoch:  13 , batch:  524 , train loss:  11.71606159210205\n","epoch:  13 , batch:  525 , train loss:  13.337455749511719\n","epoch:  13 , batch:  526 , train loss:  13.650189399719238\n","epoch:  13 , batch:  527 , train loss:  15.485939979553223\n","epoch:  13 , batch:  528 , train loss:  9.769157409667969\n","epoch:  13 , batch:  529 , train loss:  5.997339248657227\n","epoch:  13 , batch:  530 , train loss:  5.231529235839844\n","epoch:  13 , batch:  531 , train loss:  7.876647472381592\n","epoch:  13 , batch:  532 , train loss:  9.598087310791016\n","epoch:  13 , batch:  533 , train loss:  16.212535858154297\n","epoch:  13 , batch:  534 , train loss:  21.565107345581055\n","epoch:  13 , batch:  535 , train loss:  10.70241928100586\n","epoch:  13 , batch:  536 , train loss:  12.214627265930176\n","epoch:  13 , batch:  537 , train loss:  6.202852249145508\n","epoch:  13 , batch:  538 , train loss:  17.0065975189209\n","epoch:  13 , batch:  539 , train loss:  12.971029281616211\n","epoch:  13 , batch:  540 , train loss:  13.957426071166992\n","epoch:  13 , batch:  541 , train loss:  14.088871955871582\n","epoch:  13 , batch:  542 , train loss:  18.09201431274414\n","epoch:  13 , batch:  543 , train loss:  8.89016056060791\n","epoch:  13 , batch:  544 , train loss:  20.142436981201172\n","epoch:  13 , batch:  545 , train loss:  19.38343048095703\n","epoch:  13 , batch:  546 , train loss:  12.588323593139648\n","epoch:  13 , batch:  547 , train loss:  14.529032707214355\n","epoch:  13 , batch:  548 , train loss:  15.260265350341797\n","epoch:  13 , batch:  549 , train loss:  18.966571807861328\n","epoch:  13 , batch:  550 , train loss:  11.642837524414062\n","epoch:  13 , batch:  551 , train loss:  19.82183074951172\n","epoch:  13 , batch:  552 , train loss:  13.61649227142334\n","epoch:  13 , batch:  553 , train loss:  23.935020446777344\n","epoch:  13 , batch:  554 , train loss:  14.708076477050781\n","epoch:  13 , batch:  555 , train loss:  7.739810466766357\n","epoch:  13 , batch:  556 , train loss:  14.008283615112305\n","epoch:  13 , batch:  557 , train loss:  16.77246856689453\n","epoch:  13 , batch:  558 , train loss:  24.7362060546875\n","epoch:  13 , batch:  559 , train loss:  15.319422721862793\n","epoch:  13 , batch:  560 , train loss:  12.54195785522461\n","epoch:  13 , batch:  561 , train loss:  23.860055923461914\n","epoch:  13 , batch:  562 , train loss:  6.707576751708984\n","epoch:  13 , batch:  563 , train loss:  24.741945266723633\n","epoch:  13 , batch:  564 , train loss:  14.267507553100586\n","epoch:  13 , batch:  565 , train loss:  10.93461799621582\n","epoch:  13 , batch:  566 , train loss:  14.961292266845703\n","epoch:  13 , batch:  567 , train loss:  10.434504508972168\n","epoch:  13 , batch:  568 , train loss:  30.644454956054688\n","epoch:  13 , batch:  569 , train loss:  21.480819702148438\n","epoch:  13 , batch:  570 , train loss:  11.217606544494629\n","epoch:  13 , batch:  571 , train loss:  13.495161056518555\n","epoch:  13 , batch:  572 , train loss:  8.630648612976074\n","epoch:  13 , batch:  573 , train loss:  33.918601989746094\n","epoch:  13 , batch:  574 , train loss:  10.198896408081055\n","epoch:  13 , batch:  575 , train loss:  14.658867835998535\n","epoch:  13 , batch:  576 , train loss:  16.428773880004883\n","epoch:  13 , batch:  577 , train loss:  20.157691955566406\n","epoch:  13 , batch:  578 , train loss:  16.226354598999023\n","epoch:  13 , batch:  579 , train loss:  7.919666290283203\n","epoch:  13 , batch:  580 , train loss:  22.654211044311523\n","epoch:  13 , batch:  581 , train loss:  15.809852600097656\n","epoch:  13 , batch:  582 , train loss:  28.11594009399414\n","epoch:  13 , batch:  583 , train loss:  5.509559154510498\n","epoch:  13 , batch:  584 , train loss:  26.07015037536621\n","epoch:  13 , batch:  585 , train loss:  10.343575477600098\n","epoch:  13 , batch:  586 , train loss:  13.315986633300781\n","epoch:  13 , batch:  587 , train loss:  26.219005584716797\n","epoch:  13 , batch:  588 , train loss:  16.913883209228516\n","epoch:  13 , batch:  589 , train loss:  29.510433197021484\n","epoch:  13 , batch:  590 , train loss:  13.259331703186035\n","epoch:  13 , batch:  591 , train loss:  9.30917739868164\n","epoch:  13 , batch:  592 , train loss:  20.465726852416992\n","epoch:  13 , batch:  593 , train loss:  12.511113166809082\n","epoch:  13 , batch:  594 , train loss:  14.014511108398438\n","epoch:  13 , batch:  595 , train loss:  11.295406341552734\n","epoch:  13 , batch:  596 , train loss:  16.0295352935791\n","epoch:  13 , batch:  597 , train loss:  15.141901969909668\n","epoch:  13 , batch:  598 , train loss:  13.54023551940918\n","epoch:  13 , batch:  599 , train loss:  15.092950820922852\n","epoch:  13 , batch:  600 , train loss:  14.241040229797363\n","epoch:  13 , batch:  601 , train loss:  23.902421951293945\n","epoch:  13 , batch:  602 , train loss:  18.653026580810547\n","epoch:  13 , batch:  603 , train loss:  25.472370147705078\n","epoch:  13 , batch:  604 , train loss:  15.677820205688477\n","epoch:  13 , batch:  605 , train loss:  26.228769302368164\n","epoch:  13 , batch:  606 , train loss:  13.402153968811035\n","epoch:  13 , batch:  607 , train loss:  26.29877471923828\n","epoch:  13 , batch:  608 , train loss:  21.862449645996094\n","epoch:  13 , batch:  609 , train loss:  17.429723739624023\n","epoch:  13 , batch:  610 , train loss:  25.6154842376709\n","epoch:  13 , batch:  611 , train loss:  13.449793815612793\n","epoch:  13 , batch:  612 , train loss:  16.796907424926758\n","epoch:  13 , batch:  613 , train loss:  19.363239288330078\n","epoch:  13 , batch:  614 , train loss:  19.021650314331055\n","epoch:  13 , batch:  615 , train loss:  16.52393341064453\n","epoch:  13 , batch:  616 , train loss:  9.696516036987305\n","epoch:  13 , batch:  617 , train loss:  14.198819160461426\n","epoch:  13 , batch:  618 , train loss:  11.495712280273438\n","epoch:  13 , batch:  619 , train loss:  12.426029205322266\n","epoch:  13 , batch:  620 , train loss:  11.652629852294922\n","epoch:  13 , batch:  621 , train loss:  16.312808990478516\n","epoch:  13 , batch:  622 , train loss:  10.770254135131836\n","epoch:  13 , batch:  623 , train loss:  6.371436595916748\n","epoch:  13 , batch:  624 , train loss:  13.172417640686035\n","epoch:  13 , batch:  625 , train loss:  10.127863883972168\n","epoch:  13 , batch:  626 , train loss:  12.653213500976562\n","epoch:  13 , batch:  627 , train loss:  11.722111701965332\n","epoch:  13 , batch:  628 , train loss:  17.910972595214844\n","epoch:  13 , batch:  629 , train loss:  21.97296714782715\n","epoch:  13 , batch:  630 , train loss:  13.574618339538574\n","epoch:  13 , batch:  631 , train loss:  14.944796562194824\n","epoch:  13 , batch:  632 , train loss:  25.73211097717285\n","epoch:  13 , batch:  633 , train loss:  9.6182279586792\n","epoch:  13 , batch:  634 , train loss:  5.381239891052246\n","epoch:  13 , batch:  635 , train loss:  10.956982612609863\n","epoch:  13 , batch:  636 , train loss:  8.2152738571167\n","epoch:  13 , batch:  637 , train loss:  17.078659057617188\n","epoch:  13 , batch:  638 , train loss:  19.43879508972168\n","epoch:  13 , batch:  639 , train loss:  8.690031051635742\n","epoch:  13 , batch:  640 , train loss:  15.541674613952637\n","epoch:  13 , batch:  641 , train loss:  15.215670585632324\n","epoch:  13 , batch:  642 , train loss:  18.85371208190918\n","epoch:  13 , batch:  643 , train loss:  14.572427749633789\n","epoch:  13 , batch:  644 , train loss:  10.77808952331543\n","epoch:  13 , batch:  645 , train loss:  14.076475143432617\n","epoch:  13 , batch:  646 , train loss:  16.26328468322754\n","epoch:  13 , batch:  647 , train loss:  10.642839431762695\n","epoch:  13 , batch:  648 , train loss:  16.958646774291992\n","epoch:  13 , batch:  649 , train loss:  7.790410041809082\n","epoch:  13 , batch:  650 , train loss:  8.181843757629395\n","epoch:  13 , batch:  651 , train loss:  11.537683486938477\n","epoch:  13 , batch:  652 , train loss:  12.087779998779297\n","epoch:  13 , batch:  653 , train loss:  15.794976234436035\n","epoch:  13 , batch:  654 , train loss:  13.578680038452148\n","epoch:  13 , batch:  655 , train loss:  13.124665260314941\n","epoch:  13 , batch:  656 , train loss:  12.292487144470215\n","epoch:  13 , batch:  657 , train loss:  19.540573120117188\n","epoch:  13 , batch:  658 , train loss:  10.370623588562012\n","epoch:  13 , batch:  659 , train loss:  5.835479259490967\n","epoch:  13 , batch:  660 , train loss:  17.141937255859375\n","epoch:  13 , batch:  661 , train loss:  10.906875610351562\n","epoch:  13 , batch:  662 , train loss:  16.023143768310547\n","epoch:  13 , batch:  663 , train loss:  7.675915241241455\n","epoch:  13 , batch:  664 , train loss:  14.0355224609375\n","epoch:  13 , batch:  665 , train loss:  20.317874908447266\n","epoch:  13 , batch:  666 , train loss:  14.057126998901367\n","epoch:  13 , batch:  667 , train loss:  13.926182746887207\n","epoch:  13 , batch:  668 , train loss:  18.83427619934082\n","epoch:  13 , batch:  669 , train loss:  14.230488777160645\n","epoch:  13 , batch:  670 , train loss:  27.67616081237793\n","epoch:  13 , batch:  671 , train loss:  11.20833683013916\n","epoch:  13 , batch:  672 , train loss:  17.79279899597168\n","epoch:  13 , batch:  673 , train loss:  11.540969848632812\n","epoch:  13 , batch:  674 , train loss:  4.913345813751221\n","epoch:  13 , batch:  675 , train loss:  10.754382133483887\n","epoch:  13 , batch:  676 , train loss:  14.32853889465332\n","epoch:  13 , batch:  677 , train loss:  16.184389114379883\n","epoch:  13 , batch:  678 , train loss:  18.744903564453125\n","epoch:  13 , batch:  679 , train loss:  14.622654914855957\n","epoch:  13 , batch:  680 , train loss:  10.434829711914062\n","epoch:  13 , batch:  681 , train loss:  13.214823722839355\n","epoch:  13 , batch:  682 , train loss:  14.771940231323242\n","epoch:  13 , batch:  683 , train loss:  20.858753204345703\n","epoch:  13 , batch:  684 , train loss:  13.529727935791016\n","epoch:  13 , batch:  685 , train loss:  12.588308334350586\n","epoch:  13 , batch:  686 , train loss:  19.545726776123047\n","epoch:  13 , batch:  687 , train loss:  14.028655052185059\n","epoch:  13 , batch:  688 , train loss:  16.112260818481445\n","epoch:  13 , batch:  689 , train loss:  8.21323013305664\n","epoch:  13 , batch:  690 , train loss:  13.665753364562988\n","epoch:  13 , batch:  691 , train loss:  15.029338836669922\n","epoch:  13 , batch:  692 , train loss:  14.835482597351074\n","epoch:  13 , batch:  693 , train loss:  25.31171417236328\n","epoch:  13 , batch:  694 , train loss:  26.843013763427734\n","epoch:  13 , batch:  695 , train loss:  16.27474021911621\n","epoch:  13 , batch:  696 , train loss:  14.242112159729004\n","epoch:  13 , batch:  697 , train loss:  14.96420955657959\n","epoch:  13 , batch:  698 , train loss:  19.790782928466797\n","epoch:  13 , batch:  699 , train loss:  18.084585189819336\n","epoch:  13 , batch:  700 , train loss:  18.71249008178711\n","epoch:  13 , batch:  701 , train loss:  19.285022735595703\n","epoch:  13 , batch:  702 , train loss:  16.81800079345703\n","epoch:  13 , batch:  703 , train loss:  14.072273254394531\n","epoch:  13 , batch:  704 , train loss:  12.927318572998047\n","epoch:  13 , batch:  705 , train loss:  14.43539047241211\n","epoch:  13 , batch:  706 , train loss:  11.515687942504883\n","epoch:  13 , batch:  707 , train loss:  20.653600692749023\n","epoch:  13 , batch:  708 , train loss:  14.335569381713867\n","epoch:  13 , batch:  709 , train loss:  13.382460594177246\n","epoch:  13 , batch:  710 , train loss:  22.791549682617188\n","epoch:  13 , batch:  711 , train loss:  19.62074851989746\n","epoch:  13 , batch:  712 , train loss:  15.161852836608887\n","epoch:  13 , batch:  713 , train loss:  14.949540138244629\n","epoch:  13 , batch:  714 , train loss:  10.071810722351074\n","epoch:  13 , batch:  715 , train loss:  9.829569816589355\n","epoch:  13 , batch:  716 , train loss:  8.432600021362305\n","epoch:  13 , batch:  717 , train loss:  18.342655181884766\n","epoch:  13 , batch:  718 , train loss:  22.71051597595215\n","epoch:  13 , batch:  719 , train loss:  17.058366775512695\n","epoch:  13 , batch:  720 , train loss:  10.542226791381836\n","epoch:  13 , batch:  721 , train loss:  19.48989486694336\n","epoch:  13 , batch:  722 , train loss:  7.820440292358398\n","epoch:  13 , batch:  723 , train loss:  11.421738624572754\n","epoch:  13 , batch:  724 , train loss:  15.907752990722656\n","epoch:  13 , batch:  725 , train loss:  17.296945571899414\n","epoch:  13 , batch:  726 , train loss:  19.449628829956055\n","epoch:  13 , batch:  727 , train loss:  14.013631820678711\n","epoch:  13 , batch:  728 , train loss:  9.859506607055664\n","epoch:  13 , batch:  729 , train loss:  18.10479736328125\n","epoch:  13 , batch:  730 , train loss:  12.678972244262695\n","epoch:  13 , batch:  731 , train loss:  13.650684356689453\n","epoch:  13 , batch:  732 , train loss:  8.386945724487305\n","epoch:  13 , batch:  733 , train loss:  11.195043563842773\n","epoch:  13 , batch:  734 , train loss:  32.60717010498047\n","epoch:  13 , batch:  735 , train loss:  14.098690032958984\n","epoch:  13 , batch:  736 , train loss:  13.191122055053711\n","epoch:  13 , batch:  737 , train loss:  12.77995777130127\n","epoch:  13 , batch:  738 , train loss:  12.01679801940918\n","epoch:  13 , batch:  739 , train loss:  14.558334350585938\n","epoch:  13 , batch:  740 , train loss:  20.08463478088379\n","epoch:  13 , batch:  741 , train loss:  24.56679344177246\n","epoch:  13 , batch:  742 , train loss:  11.628975868225098\n","epoch:  13 , batch:  743 , train loss:  16.443557739257812\n","epoch:  13 , batch:  744 , train loss:  35.33980941772461\n","epoch:  13 , batch:  745 , train loss:  13.87548542022705\n","epoch:  13 , batch:  746 , train loss:  6.53244686126709\n","epoch:  13 , batch:  747 , train loss:  9.580042839050293\n","epoch:  13 , batch:  748 , train loss:  15.99502182006836\n","epoch:  13 , batch:  749 , train loss:  21.74863052368164\n","epoch:  13 , batch:  750 , train loss:  20.454421997070312\n","epoch:  13 , batch:  751 , train loss:  16.16326141357422\n","epoch:  13 , batch:  752 , train loss:  11.60877799987793\n","epoch:  13 , batch:  753 , train loss:  12.26020622253418\n","epoch:  13 , batch:  754 , train loss:  7.499886989593506\n","epoch:  13 , batch:  755 , train loss:  21.181894302368164\n","epoch:  13 , batch:  756 , train loss:  24.43351173400879\n","epoch:  13 , batch:  757 , train loss:  13.846628189086914\n","epoch:  13 , batch:  758 , train loss:  22.438234329223633\n","epoch:  13 , batch:  759 , train loss:  23.31608009338379\n","epoch:  13 , batch:  760 , train loss:  17.162296295166016\n","epoch:  13 , batch:  761 , train loss:  33.825233459472656\n","epoch:  13 , batch:  762 , train loss:  12.771895408630371\n","epoch:  13 , batch:  763 , train loss:  23.56580924987793\n","epoch:  13 , batch:  764 , train loss:  10.263993263244629\n","epoch:  13 , batch:  765 , train loss:  16.353654861450195\n","epoch:  13 , batch:  766 , train loss:  24.662532806396484\n","epoch:  13 , batch:  767 , train loss:  14.173250198364258\n","epoch:  13 , batch:  768 , train loss:  13.778608322143555\n","epoch:  13 , batch:  769 , train loss:  18.616165161132812\n","epoch:  13 , batch:  770 , train loss:  13.803997039794922\n","epoch:  13 , batch:  771 , train loss:  20.290334701538086\n","epoch:  13 , batch:  772 , train loss:  20.75225830078125\n","epoch:  13 , batch:  773 , train loss:  13.427658081054688\n","epoch:  13 , batch:  774 , train loss:  9.964006423950195\n","epoch:  13 , batch:  775 , train loss:  10.613945007324219\n","epoch:  13 , batch:  776 , train loss:  10.582404136657715\n","epoch:  13 , batch:  777 , train loss:  11.635693550109863\n","epoch:  13 , batch:  778 , train loss:  19.515972137451172\n","epoch:  13 , batch:  779 , train loss:  28.335613250732422\n","epoch:  13 , batch:  780 , train loss:  17.310171127319336\n","epoch:  13 , batch:  781 , train loss:  9.952115058898926\n","epoch:  13 , batch:  782 , train loss:  21.020954132080078\n","epoch:  13 , batch:  783 , train loss:  22.408456802368164\n","epoch:  13 , batch:  784 , train loss:  19.197858810424805\n","epoch:  13 , batch:  785 , train loss:  15.138264656066895\n","epoch:  13 , batch:  786 , train loss:  16.92417335510254\n","epoch:  13 , batch:  787 , train loss:  10.297394752502441\n","epoch:  13 , batch:  788 , train loss:  11.197630882263184\n","epoch:  13 , batch:  789 , train loss:  9.099283218383789\n","epoch:  13 , batch:  790 , train loss:  9.764045715332031\n","epoch:  13 , batch:  791 , train loss:  11.822587966918945\n","epoch:  13 , batch:  792 , train loss:  25.589576721191406\n","epoch:  13 , batch:  793 , train loss:  15.342578887939453\n","epoch:  13 , batch:  794 , train loss:  17.3710994720459\n","epoch:  13 , batch:  795 , train loss:  7.892134189605713\n","epoch:  13 , batch:  796 , train loss:  15.381403923034668\n","epoch:  13 , batch:  797 , train loss:  24.502471923828125\n","epoch:  13 , batch:  798 , train loss:  9.96227741241455\n","epoch:  13 , batch:  799 , train loss:  14.865816116333008\n","epoch:  13 , batch:  800 , train loss:  23.150339126586914\n","epoch:  13 , batch:  801 , train loss:  35.48476028442383\n","epoch:  13 , batch:  802 , train loss:  11.391115188598633\n","epoch:  13 , batch:  803 , train loss:  28.14983367919922\n","epoch:  13 , batch:  804 , train loss:  11.215380668640137\n","epoch:  13 , batch:  805 , train loss:  14.550357818603516\n","epoch:  13 , batch:  806 , train loss:  18.76067543029785\n","epoch:  13 , batch:  807 , train loss:  11.861759185791016\n","epoch:  13 , batch:  808 , train loss:  10.507290840148926\n","epoch:  13 , batch:  809 , train loss:  18.981908798217773\n","epoch:  13 , batch:  810 , train loss:  9.735028266906738\n","epoch:  13 , batch:  811 , train loss:  13.757452011108398\n","epoch:  13 , batch:  812 , train loss:  8.646163940429688\n","epoch:  13 , batch:  813 , train loss:  18.062118530273438\n","epoch:  13 , batch:  814 , train loss:  8.920893669128418\n","epoch:  13 , batch:  815 , train loss:  24.624910354614258\n","epoch:  13 , batch:  816 , train loss:  17.80679702758789\n","epoch:  13 , batch:  817 , train loss:  11.491094589233398\n","epoch:  13 , batch:  818 , train loss:  12.799714088439941\n","epoch:  13 , batch:  819 , train loss:  17.39809799194336\n","epoch:  13 , batch:  820 , train loss:  10.418521881103516\n","epoch:  13 , batch:  821 , train loss:  6.918097019195557\n","epoch:  13 , batch:  822 , train loss:  9.663166999816895\n","epoch:  13 , batch:  823 , train loss:  16.05339813232422\n","epoch:  13 , batch:  824 , train loss:  7.852694511413574\n","epoch:  13 , batch:  825 , train loss:  17.221147537231445\n","epoch:  13 , batch:  826 , train loss:  16.05501937866211\n","epoch:  13 , batch:  827 , train loss:  18.921043395996094\n","epoch:  13 , batch:  828 , train loss:  14.728934288024902\n","epoch:  13 , batch:  829 , train loss:  18.027482986450195\n","epoch:  13 , batch:  830 , train loss:  18.02682113647461\n","epoch:  13 , batch:  831 , train loss:  19.846416473388672\n","epoch:  13 , batch:  832 , train loss:  7.201645851135254\n","epoch:  13 , batch:  833 , train loss:  15.496431350708008\n","epoch:  13 , batch:  834 , train loss:  11.749886512756348\n","epoch:  13 , batch:  835 , train loss:  11.525009155273438\n","epoch:  13 , batch:  836 , train loss:  14.628531455993652\n","epoch:  13 , batch:  837 , train loss:  14.815412521362305\n","epoch:  13 , batch:  838 , train loss:  9.686141014099121\n","epoch:  13 , batch:  839 , train loss:  23.359949111938477\n","epoch:  13 , batch:  840 , train loss:  18.558076858520508\n","epoch:  13 , batch:  841 , train loss:  8.736583709716797\n","epoch:  13 , batch:  842 , train loss:  8.380971908569336\n","epoch:  13 , batch:  843 , train loss:  14.717771530151367\n","epoch:  13 , batch:  844 , train loss:  15.578290939331055\n","epoch:  13 , batch:  845 , train loss:  16.5008544921875\n","epoch:  13 , batch:  846 , train loss:  16.384178161621094\n","epoch:  13 , batch:  847 , train loss:  16.75469207763672\n","epoch:  13 , batch:  848 , train loss:  17.799959182739258\n","epoch:  13 , batch:  849 , train loss:  14.790740966796875\n","epoch:  13 , batch:  850 , train loss:  18.305557250976562\n","epoch:  13 , batch:  851 , train loss:  28.50794792175293\n","epoch:  13 , batch:  852 , train loss:  16.178016662597656\n","epoch:  13 , batch:  853 , train loss:  23.261577606201172\n","epoch:  13 , batch:  854 , train loss:  19.74054718017578\n","epoch:  13 , batch:  855 , train loss:  19.210716247558594\n","epoch:  13 , batch:  856 , train loss:  17.455062866210938\n","epoch:  13 , batch:  857 , train loss:  19.61365509033203\n","epoch:  13 , batch:  858 , train loss:  10.630660057067871\n","epoch:  13 , batch:  859 , train loss:  10.116399765014648\n","epoch:  13 , batch:  860 , train loss:  18.474590301513672\n","epoch:  13 , batch:  861 , train loss:  16.05109977722168\n","epoch:  13 , batch:  862 , train loss:  24.263044357299805\n","epoch:  13 , batch:  863 , train loss:  14.529295921325684\n","epoch:  13 , batch:  864 , train loss:  19.283550262451172\n","epoch:  13 , batch:  865 , train loss:  16.733036041259766\n","epoch:  13 , batch:  866 , train loss:  10.04052448272705\n","epoch:  13 , batch:  867 , train loss:  19.062639236450195\n","epoch:  13 , batch:  868 , train loss:  13.597421646118164\n","epoch:  13 , batch:  869 , train loss:  11.189437866210938\n","epoch:  13 , batch:  870 , train loss:  8.202659606933594\n","epoch:  13 , batch:  871 , train loss:  10.786216735839844\n","epoch:  13 , batch:  872 , train loss:  6.551238536834717\n","epoch:  13 , batch:  873 , train loss:  14.263150215148926\n","epoch:  13 , batch:  874 , train loss:  15.195481300354004\n","epoch:  13 , batch:  875 , train loss:  15.046131134033203\n","epoch:  13 , batch:  876 , train loss:  21.530014038085938\n","epoch:  13 , batch:  877 , train loss:  13.337538719177246\n","epoch:  13 , batch:  878 , train loss:  17.595966339111328\n","epoch:  13 , batch:  879 , train loss:  17.50326919555664\n","epoch:  13 , batch:  880 , train loss:  15.814891815185547\n","epoch:  13 , batch:  881 , train loss:  20.17849349975586\n","epoch:  13 , batch:  882 , train loss:  9.871021270751953\n","epoch:  13 , batch:  883 , train loss:  18.10198211669922\n","epoch:  13 , batch:  884 , train loss:  13.10451889038086\n","epoch:  13 , batch:  885 , train loss:  11.496562004089355\n","epoch:  13 , batch:  886 , train loss:  21.290861129760742\n","epoch:  13 , batch:  887 , train loss:  19.30170249938965\n","epoch:  13 , batch:  888 , train loss:  11.738637924194336\n","epoch:  13 , batch:  889 , train loss:  19.337263107299805\n","epoch:  13 , batch:  890 , train loss:  11.599358558654785\n","epoch:  13 , batch:  891 , train loss:  9.085454940795898\n","epoch:  13 , batch:  892 , train loss:  26.724342346191406\n","epoch:  13 , batch:  893 , train loss:  19.424514770507812\n","epoch:  13 , batch:  894 , train loss:  20.449758529663086\n","epoch:  13 , batch:  895 , train loss:  12.827540397644043\n","epoch:  13 , batch:  896 , train loss:  12.9701566696167\n","epoch:  13 , batch:  897 , train loss:  20.161710739135742\n","epoch:  13 , batch:  898 , train loss:  9.889079093933105\n","epoch:  13 , batch:  899 , train loss:  9.318862915039062\n","epoch:  13 , batch:  900 , train loss:  5.545988082885742\n","epoch:  13 , batch:  901 , train loss:  21.684165954589844\n","epoch:  13 , batch:  902 , train loss:  6.863681316375732\n","epoch:  13 , batch:  903 , train loss:  18.44950294494629\n","epoch:  13 , batch:  904 , train loss:  17.307750701904297\n","epoch:  13 , batch:  905 , train loss:  9.853385925292969\n","epoch:  13 , batch:  906 , train loss:  19.756479263305664\n","epoch:  13 , batch:  907 , train loss:  18.123628616333008\n","epoch:  13 , batch:  908 , train loss:  10.082139015197754\n","epoch:  13 , batch:  909 , train loss:  14.171369552612305\n","epoch:  13 , batch:  910 , train loss:  15.925881385803223\n","epoch:  13 , batch:  911 , train loss:  10.866114616394043\n","epoch:  13 , batch:  912 , train loss:  26.454374313354492\n","epoch:  13 , batch:  913 , train loss:  13.506661415100098\n","epoch:  13 , batch:  914 , train loss:  9.375765800476074\n","epoch:  13 , batch:  915 , train loss:  14.161519050598145\n","epoch:  13 , batch:  916 , train loss:  11.503135681152344\n","epoch:  13 , batch:  917 , train loss:  13.341355323791504\n","epoch:  13 , batch:  918 , train loss:  13.925384521484375\n","epoch:  13 , batch:  919 , train loss:  20.53411102294922\n","epoch:  13 , batch:  920 , train loss:  16.901630401611328\n","epoch:  13 , batch:  921 , train loss:  9.105573654174805\n","epoch:  13 , batch:  922 , train loss:  14.05324935913086\n","epoch:  13 , batch:  923 , train loss:  30.080568313598633\n","epoch:  13 , batch:  924 , train loss:  33.816123962402344\n","epoch:  13 , batch:  925 , train loss:  21.307819366455078\n","epoch:  13 , batch:  926 , train loss:  15.044029235839844\n","epoch:  13 , batch:  927 , train loss:  10.37192440032959\n","epoch:  13 , batch:  928 , train loss:  12.820534706115723\n","epoch:  13 , batch:  929 , train loss:  11.03425407409668\n","epoch:  13 , batch:  930 , train loss:  20.08302116394043\n","epoch:  13 , batch:  931 , train loss:  9.783180236816406\n","epoch:  13 , batch:  932 , train loss:  12.686483383178711\n","epoch:  13 , batch:  933 , train loss:  15.047429084777832\n","epoch:  13 , batch:  934 , train loss:  16.305015563964844\n","epoch:  13 , batch:  935 , train loss:  16.946931838989258\n","epoch:  13 , batch:  936 , train loss:  14.372498512268066\n","epoch:  13 , batch:  937 , train loss:  9.166318893432617\n","Accuracy of train set: 0.9101166666666667\n","epoch:  13 , batch:  0 , test loss:  26.509958267211914\n","epoch:  13 , batch:  1 , test loss:  16.34749984741211\n","epoch:  13 , batch:  2 , test loss:  23.487030029296875\n","epoch:  13 , batch:  3 , test loss:  27.48008155822754\n","epoch:  13 , batch:  4 , test loss:  36.24258041381836\n","epoch:  13 , batch:  5 , test loss:  28.63865852355957\n","epoch:  13 , batch:  6 , test loss:  16.60616683959961\n","epoch:  13 , batch:  7 , test loss:  24.70836067199707\n","epoch:  13 , batch:  8 , test loss:  20.44952964782715\n","epoch:  13 , batch:  9 , test loss:  13.000907897949219\n","epoch:  13 , batch:  10 , test loss:  24.58173370361328\n","epoch:  13 , batch:  11 , test loss:  18.303787231445312\n","epoch:  13 , batch:  12 , test loss:  23.260400772094727\n","epoch:  13 , batch:  13 , test loss:  22.312070846557617\n","epoch:  13 , batch:  14 , test loss:  7.654987335205078\n","epoch:  13 , batch:  15 , test loss:  7.967092514038086\n","epoch:  13 , batch:  16 , test loss:  21.034330368041992\n","epoch:  13 , batch:  17 , test loss:  12.228456497192383\n","epoch:  13 , batch:  18 , test loss:  31.293426513671875\n","epoch:  13 , batch:  19 , test loss:  13.96812915802002\n","epoch:  13 , batch:  20 , test loss:  18.110111236572266\n","epoch:  13 , batch:  21 , test loss:  12.95699405670166\n","epoch:  13 , batch:  22 , test loss:  29.909107208251953\n","epoch:  13 , batch:  23 , test loss:  16.951162338256836\n","epoch:  13 , batch:  24 , test loss:  19.411724090576172\n","epoch:  13 , batch:  25 , test loss:  19.81022071838379\n","epoch:  13 , batch:  26 , test loss:  19.543094635009766\n","epoch:  13 , batch:  27 , test loss:  21.359214782714844\n","epoch:  13 , batch:  28 , test loss:  18.93014144897461\n","epoch:  13 , batch:  29 , test loss:  24.073331832885742\n","epoch:  13 , batch:  30 , test loss:  25.8336181640625\n","epoch:  13 , batch:  31 , test loss:  23.51470184326172\n","epoch:  13 , batch:  32 , test loss:  14.003023147583008\n","epoch:  13 , batch:  33 , test loss:  33.481895446777344\n","epoch:  13 , batch:  34 , test loss:  22.6430606842041\n","epoch:  13 , batch:  35 , test loss:  18.36089515686035\n","epoch:  13 , batch:  36 , test loss:  28.41697883605957\n","epoch:  13 , batch:  37 , test loss:  17.74032974243164\n","epoch:  13 , batch:  38 , test loss:  20.508508682250977\n","epoch:  13 , batch:  39 , test loss:  16.736719131469727\n","epoch:  13 , batch:  40 , test loss:  21.411653518676758\n","epoch:  13 , batch:  41 , test loss:  27.85085678100586\n","epoch:  13 , batch:  42 , test loss:  13.126008987426758\n","epoch:  13 , batch:  43 , test loss:  20.087589263916016\n","epoch:  13 , batch:  44 , test loss:  20.89999008178711\n","epoch:  13 , batch:  45 , test loss:  23.183561325073242\n","epoch:  13 , batch:  46 , test loss:  18.17581558227539\n","epoch:  13 , batch:  47 , test loss:  19.11491584777832\n","epoch:  13 , batch:  48 , test loss:  9.709012985229492\n","epoch:  13 , batch:  49 , test loss:  29.00348472595215\n","epoch:  13 , batch:  50 , test loss:  32.26380920410156\n","epoch:  13 , batch:  51 , test loss:  21.80452537536621\n","epoch:  13 , batch:  52 , test loss:  21.765459060668945\n","epoch:  13 , batch:  53 , test loss:  16.209733963012695\n","epoch:  13 , batch:  54 , test loss:  25.151384353637695\n","epoch:  13 , batch:  55 , test loss:  26.165752410888672\n","epoch:  13 , batch:  56 , test loss:  17.532194137573242\n","epoch:  13 , batch:  57 , test loss:  28.967472076416016\n","epoch:  13 , batch:  58 , test loss:  21.743457794189453\n","epoch:  13 , batch:  59 , test loss:  21.25457000732422\n","epoch:  13 , batch:  60 , test loss:  22.932476043701172\n","epoch:  13 , batch:  61 , test loss:  19.397621154785156\n","epoch:  13 , batch:  62 , test loss:  15.597901344299316\n","epoch:  13 , batch:  63 , test loss:  28.02918815612793\n","epoch:  13 , batch:  64 , test loss:  15.169185638427734\n","epoch:  13 , batch:  65 , test loss:  16.395238876342773\n","epoch:  13 , batch:  66 , test loss:  36.080074310302734\n","epoch:  13 , batch:  67 , test loss:  13.405521392822266\n","epoch:  13 , batch:  68 , test loss:  19.11014747619629\n","epoch:  13 , batch:  69 , test loss:  21.49343490600586\n","epoch:  13 , batch:  70 , test loss:  13.558687210083008\n","epoch:  13 , batch:  71 , test loss:  16.158905029296875\n","epoch:  13 , batch:  72 , test loss:  21.82563018798828\n","epoch:  13 , batch:  73 , test loss:  25.33615493774414\n","epoch:  13 , batch:  74 , test loss:  16.335542678833008\n","epoch:  13 , batch:  75 , test loss:  31.593137741088867\n","epoch:  13 , batch:  76 , test loss:  15.417078018188477\n","epoch:  13 , batch:  77 , test loss:  40.894813537597656\n","epoch:  13 , batch:  78 , test loss:  34.904293060302734\n","epoch:  13 , batch:  79 , test loss:  37.634483337402344\n","epoch:  13 , batch:  80 , test loss:  17.609851837158203\n","epoch:  13 , batch:  81 , test loss:  15.105908393859863\n","epoch:  13 , batch:  82 , test loss:  19.94429588317871\n","epoch:  13 , batch:  83 , test loss:  15.082417488098145\n","epoch:  13 , batch:  84 , test loss:  16.888717651367188\n","epoch:  13 , batch:  85 , test loss:  9.76593017578125\n","epoch:  13 , batch:  86 , test loss:  30.43528938293457\n","epoch:  13 , batch:  87 , test loss:  13.161890029907227\n","epoch:  13 , batch:  88 , test loss:  20.13559913635254\n","epoch:  13 , batch:  89 , test loss:  18.310874938964844\n","epoch:  13 , batch:  90 , test loss:  14.098389625549316\n","epoch:  13 , batch:  91 , test loss:  24.611154556274414\n","epoch:  13 , batch:  92 , test loss:  12.38022518157959\n","epoch:  13 , batch:  93 , test loss:  13.328471183776855\n","epoch:  13 , batch:  94 , test loss:  22.628726959228516\n","epoch:  13 , batch:  95 , test loss:  15.834123611450195\n","epoch:  13 , batch:  96 , test loss:  27.992229461669922\n","epoch:  13 , batch:  97 , test loss:  22.223379135131836\n","epoch:  13 , batch:  98 , test loss:  28.900617599487305\n","epoch:  13 , batch:  99 , test loss:  16.572629928588867\n","epoch:  13 , batch:  100 , test loss:  21.93109130859375\n","epoch:  13 , batch:  101 , test loss:  15.611184120178223\n","epoch:  13 , batch:  102 , test loss:  12.570594787597656\n","epoch:  13 , batch:  103 , test loss:  26.06182861328125\n","epoch:  13 , batch:  104 , test loss:  28.977998733520508\n","epoch:  13 , batch:  105 , test loss:  25.94622039794922\n","epoch:  13 , batch:  106 , test loss:  31.94568634033203\n","epoch:  13 , batch:  107 , test loss:  22.002822875976562\n","epoch:  13 , batch:  108 , test loss:  36.53401184082031\n","epoch:  13 , batch:  109 , test loss:  26.890605926513672\n","epoch:  13 , batch:  110 , test loss:  33.852439880371094\n","epoch:  13 , batch:  111 , test loss:  16.70594024658203\n","epoch:  13 , batch:  112 , test loss:  20.00209617614746\n","epoch:  13 , batch:  113 , test loss:  34.31229782104492\n","epoch:  13 , batch:  114 , test loss:  14.524261474609375\n","epoch:  13 , batch:  115 , test loss:  15.49693775177002\n","epoch:  13 , batch:  116 , test loss:  16.715280532836914\n","epoch:  13 , batch:  117 , test loss:  11.47439956665039\n","epoch:  13 , batch:  118 , test loss:  19.935253143310547\n","epoch:  13 , batch:  119 , test loss:  14.75159740447998\n","epoch:  13 , batch:  120 , test loss:  14.409833908081055\n","epoch:  13 , batch:  121 , test loss:  18.1485652923584\n","epoch:  13 , batch:  122 , test loss:  20.266448974609375\n","epoch:  13 , batch:  123 , test loss:  13.482438087463379\n","epoch:  13 , batch:  124 , test loss:  33.3157844543457\n","epoch:  13 , batch:  125 , test loss:  19.848466873168945\n","epoch:  13 , batch:  126 , test loss:  14.72183895111084\n","epoch:  13 , batch:  127 , test loss:  11.960453033447266\n","epoch:  13 , batch:  128 , test loss:  17.22745132446289\n","epoch:  13 , batch:  129 , test loss:  31.439626693725586\n","epoch:  13 , batch:  130 , test loss:  19.80104637145996\n","epoch:  13 , batch:  131 , test loss:  15.870224952697754\n","epoch:  13 , batch:  132 , test loss:  25.865901947021484\n","epoch:  13 , batch:  133 , test loss:  17.541767120361328\n","epoch:  13 , batch:  134 , test loss:  17.977294921875\n","epoch:  13 , batch:  135 , test loss:  25.893787384033203\n","epoch:  13 , batch:  136 , test loss:  15.831753730773926\n","epoch:  13 , batch:  137 , test loss:  23.608070373535156\n","epoch:  13 , batch:  138 , test loss:  20.80571937561035\n","epoch:  13 , batch:  139 , test loss:  19.209259033203125\n","epoch:  13 , batch:  140 , test loss:  19.94603157043457\n","epoch:  13 , batch:  141 , test loss:  19.980806350708008\n","epoch:  13 , batch:  142 , test loss:  41.06023025512695\n","epoch:  13 , batch:  143 , test loss:  30.30144691467285\n","epoch:  13 , batch:  144 , test loss:  17.27503204345703\n","epoch:  13 , batch:  145 , test loss:  22.13543128967285\n","epoch:  13 , batch:  146 , test loss:  15.920398712158203\n","epoch:  13 , batch:  147 , test loss:  16.220670700073242\n","epoch:  13 , batch:  148 , test loss:  20.236955642700195\n","epoch:  13 , batch:  149 , test loss:  16.3550968170166\n","epoch:  13 , batch:  150 , test loss:  17.62612533569336\n","epoch:  13 , batch:  151 , test loss:  36.69081115722656\n","epoch:  13 , batch:  152 , test loss:  38.1575927734375\n","epoch:  13 , batch:  153 , test loss:  34.05150604248047\n","epoch:  13 , batch:  154 , test loss:  16.008258819580078\n","epoch:  13 , batch:  155 , test loss:  22.7452335357666\n","epoch:  13 , batch:  156 , test loss:  2.3683924674987793\n","Accuracy of pytorch_model set: 0.8842\n","epoch:  14 , batch:  0 , train loss:  17.802453994750977\n","epoch:  14 , batch:  1 , train loss:  15.736333847045898\n","epoch:  14 , batch:  2 , train loss:  9.489899635314941\n","epoch:  14 , batch:  3 , train loss:  13.370331764221191\n","epoch:  14 , batch:  4 , train loss:  14.37546157836914\n","epoch:  14 , batch:  5 , train loss:  17.38593864440918\n","epoch:  14 , batch:  6 , train loss:  13.529242515563965\n","epoch:  14 , batch:  7 , train loss:  15.821810722351074\n","epoch:  14 , batch:  8 , train loss:  12.300625801086426\n","epoch:  14 , batch:  9 , train loss:  7.680330753326416\n","epoch:  14 , batch:  10 , train loss:  14.897261619567871\n","epoch:  14 , batch:  11 , train loss:  11.895858764648438\n","epoch:  14 , batch:  12 , train loss:  7.290099143981934\n","epoch:  14 , batch:  13 , train loss:  10.898335456848145\n","epoch:  14 , batch:  14 , train loss:  10.05628490447998\n","epoch:  14 , batch:  15 , train loss:  14.700617790222168\n","epoch:  14 , batch:  16 , train loss:  10.035050392150879\n","epoch:  14 , batch:  17 , train loss:  13.576908111572266\n","epoch:  14 , batch:  18 , train loss:  10.44991397857666\n","epoch:  14 , batch:  19 , train loss:  18.478334426879883\n","epoch:  14 , batch:  20 , train loss:  11.161937713623047\n","epoch:  14 , batch:  21 , train loss:  13.793085098266602\n","epoch:  14 , batch:  22 , train loss:  13.8478422164917\n","epoch:  14 , batch:  23 , train loss:  13.444683074951172\n","epoch:  14 , batch:  24 , train loss:  16.88869857788086\n","epoch:  14 , batch:  25 , train loss:  6.2486371994018555\n","epoch:  14 , batch:  26 , train loss:  9.556313514709473\n","epoch:  14 , batch:  27 , train loss:  14.300111770629883\n","epoch:  14 , batch:  28 , train loss:  8.88720703125\n","epoch:  14 , batch:  29 , train loss:  12.231551170349121\n","epoch:  14 , batch:  30 , train loss:  3.817654848098755\n","epoch:  14 , batch:  31 , train loss:  13.206342697143555\n","epoch:  14 , batch:  32 , train loss:  15.185664176940918\n","epoch:  14 , batch:  33 , train loss:  20.861854553222656\n","epoch:  14 , batch:  34 , train loss:  18.4200439453125\n","epoch:  14 , batch:  35 , train loss:  17.53389549255371\n","epoch:  14 , batch:  36 , train loss:  13.902547836303711\n","epoch:  14 , batch:  37 , train loss:  10.599235534667969\n","epoch:  14 , batch:  38 , train loss:  13.061723709106445\n","epoch:  14 , batch:  39 , train loss:  6.896749973297119\n","epoch:  14 , batch:  40 , train loss:  20.69954490661621\n","epoch:  14 , batch:  41 , train loss:  7.210962772369385\n","epoch:  14 , batch:  42 , train loss:  21.00773048400879\n","epoch:  14 , batch:  43 , train loss:  15.424784660339355\n","epoch:  14 , batch:  44 , train loss:  24.41501235961914\n","epoch:  14 , batch:  45 , train loss:  12.285420417785645\n","epoch:  14 , batch:  46 , train loss:  14.078003883361816\n","epoch:  14 , batch:  47 , train loss:  17.020408630371094\n","epoch:  14 , batch:  48 , train loss:  7.820631504058838\n","epoch:  14 , batch:  49 , train loss:  18.760290145874023\n","epoch:  14 , batch:  50 , train loss:  13.087618827819824\n","epoch:  14 , batch:  51 , train loss:  20.333452224731445\n","epoch:  14 , batch:  52 , train loss:  17.67432403564453\n","epoch:  14 , batch:  53 , train loss:  24.676116943359375\n","epoch:  14 , batch:  54 , train loss:  15.720351219177246\n","epoch:  14 , batch:  55 , train loss:  13.154014587402344\n","epoch:  14 , batch:  56 , train loss:  12.21019172668457\n","epoch:  14 , batch:  57 , train loss:  11.974101066589355\n","epoch:  14 , batch:  58 , train loss:  15.273347854614258\n","epoch:  14 , batch:  59 , train loss:  14.37558650970459\n","epoch:  14 , batch:  60 , train loss:  13.041041374206543\n","epoch:  14 , batch:  61 , train loss:  15.438382148742676\n","epoch:  14 , batch:  62 , train loss:  9.844958305358887\n","epoch:  14 , batch:  63 , train loss:  3.957108736038208\n","epoch:  14 , batch:  64 , train loss:  8.57957649230957\n","epoch:  14 , batch:  65 , train loss:  13.536065101623535\n","epoch:  14 , batch:  66 , train loss:  12.789414405822754\n","epoch:  14 , batch:  67 , train loss:  9.34448528289795\n","epoch:  14 , batch:  68 , train loss:  14.023760795593262\n","epoch:  14 , batch:  69 , train loss:  17.46489143371582\n","epoch:  14 , batch:  70 , train loss:  16.40420913696289\n","epoch:  14 , batch:  71 , train loss:  19.23793601989746\n","epoch:  14 , batch:  72 , train loss:  14.188060760498047\n","epoch:  14 , batch:  73 , train loss:  10.038284301757812\n","epoch:  14 , batch:  74 , train loss:  14.951278686523438\n","epoch:  14 , batch:  75 , train loss:  16.812339782714844\n","epoch:  14 , batch:  76 , train loss:  14.285676956176758\n","epoch:  14 , batch:  77 , train loss:  5.45497989654541\n","epoch:  14 , batch:  78 , train loss:  18.4039306640625\n","epoch:  14 , batch:  79 , train loss:  8.625079154968262\n","epoch:  14 , batch:  80 , train loss:  15.211007118225098\n","epoch:  14 , batch:  81 , train loss:  8.55970573425293\n","epoch:  14 , batch:  82 , train loss:  9.383869171142578\n","epoch:  14 , batch:  83 , train loss:  15.38712215423584\n","epoch:  14 , batch:  84 , train loss:  9.080403327941895\n","epoch:  14 , batch:  85 , train loss:  21.912778854370117\n","epoch:  14 , batch:  86 , train loss:  9.75925064086914\n","epoch:  14 , batch:  87 , train loss:  12.033137321472168\n","epoch:  14 , batch:  88 , train loss:  20.348140716552734\n","epoch:  14 , batch:  89 , train loss:  20.34937858581543\n","epoch:  14 , batch:  90 , train loss:  12.632402420043945\n","epoch:  14 , batch:  91 , train loss:  14.465335845947266\n","epoch:  14 , batch:  92 , train loss:  10.130916595458984\n","epoch:  14 , batch:  93 , train loss:  15.286993980407715\n","epoch:  14 , batch:  94 , train loss:  15.91181755065918\n","epoch:  14 , batch:  95 , train loss:  15.34703254699707\n","epoch:  14 , batch:  96 , train loss:  17.282371520996094\n","epoch:  14 , batch:  97 , train loss:  18.647850036621094\n","epoch:  14 , batch:  98 , train loss:  9.697961807250977\n","epoch:  14 , batch:  99 , train loss:  6.8954033851623535\n","epoch:  14 , batch:  100 , train loss:  19.740394592285156\n","epoch:  14 , batch:  101 , train loss:  11.959035873413086\n","epoch:  14 , batch:  102 , train loss:  15.004019737243652\n","epoch:  14 , batch:  103 , train loss:  13.77769660949707\n","epoch:  14 , batch:  104 , train loss:  17.681976318359375\n","epoch:  14 , batch:  105 , train loss:  7.428042411804199\n","epoch:  14 , batch:  106 , train loss:  8.996432304382324\n","epoch:  14 , batch:  107 , train loss:  15.981032371520996\n","epoch:  14 , batch:  108 , train loss:  16.114757537841797\n","epoch:  14 , batch:  109 , train loss:  9.99209213256836\n","epoch:  14 , batch:  110 , train loss:  11.1558256149292\n","epoch:  14 , batch:  111 , train loss:  12.247383117675781\n","epoch:  14 , batch:  112 , train loss:  8.329461097717285\n","epoch:  14 , batch:  113 , train loss:  20.696990966796875\n","epoch:  14 , batch:  114 , train loss:  14.393584251403809\n","epoch:  14 , batch:  115 , train loss:  10.044008255004883\n","epoch:  14 , batch:  116 , train loss:  18.304964065551758\n","epoch:  14 , batch:  117 , train loss:  10.993624687194824\n","epoch:  14 , batch:  118 , train loss:  10.818615913391113\n","epoch:  14 , batch:  119 , train loss:  15.796518325805664\n","epoch:  14 , batch:  120 , train loss:  12.418723106384277\n","epoch:  14 , batch:  121 , train loss:  21.538562774658203\n","epoch:  14 , batch:  122 , train loss:  13.067672729492188\n","epoch:  14 , batch:  123 , train loss:  10.52380084991455\n","epoch:  14 , batch:  124 , train loss:  8.719514846801758\n","epoch:  14 , batch:  125 , train loss:  8.160191535949707\n","epoch:  14 , batch:  126 , train loss:  9.724658966064453\n","epoch:  14 , batch:  127 , train loss:  10.3905029296875\n","epoch:  14 , batch:  128 , train loss:  12.052964210510254\n","epoch:  14 , batch:  129 , train loss:  15.971893310546875\n","epoch:  14 , batch:  130 , train loss:  19.99924659729004\n","epoch:  14 , batch:  131 , train loss:  11.650472640991211\n","epoch:  14 , batch:  132 , train loss:  25.80333709716797\n","epoch:  14 , batch:  133 , train loss:  8.606084823608398\n","epoch:  14 , batch:  134 , train loss:  27.100561141967773\n","epoch:  14 , batch:  135 , train loss:  14.867851257324219\n","epoch:  14 , batch:  136 , train loss:  8.841808319091797\n","epoch:  14 , batch:  137 , train loss:  18.543041229248047\n","epoch:  14 , batch:  138 , train loss:  12.151031494140625\n","epoch:  14 , batch:  139 , train loss:  12.641559600830078\n","epoch:  14 , batch:  140 , train loss:  12.444217681884766\n","epoch:  14 , batch:  141 , train loss:  10.119917869567871\n","epoch:  14 , batch:  142 , train loss:  7.444418907165527\n","epoch:  14 , batch:  143 , train loss:  13.78836441040039\n","epoch:  14 , batch:  144 , train loss:  17.044038772583008\n","epoch:  14 , batch:  145 , train loss:  16.496837615966797\n","epoch:  14 , batch:  146 , train loss:  8.485711097717285\n","epoch:  14 , batch:  147 , train loss:  11.727542877197266\n","epoch:  14 , batch:  148 , train loss:  14.399827003479004\n","epoch:  14 , batch:  149 , train loss:  17.283601760864258\n","epoch:  14 , batch:  150 , train loss:  27.578752517700195\n","epoch:  14 , batch:  151 , train loss:  23.83707046508789\n","epoch:  14 , batch:  152 , train loss:  11.646245956420898\n","epoch:  14 , batch:  153 , train loss:  15.113192558288574\n","epoch:  14 , batch:  154 , train loss:  10.546096801757812\n","epoch:  14 , batch:  155 , train loss:  13.59067440032959\n","epoch:  14 , batch:  156 , train loss:  11.55787181854248\n","epoch:  14 , batch:  157 , train loss:  11.877286911010742\n","epoch:  14 , batch:  158 , train loss:  12.911849021911621\n","epoch:  14 , batch:  159 , train loss:  12.238070487976074\n","epoch:  14 , batch:  160 , train loss:  9.08053207397461\n","epoch:  14 , batch:  161 , train loss:  10.708428382873535\n","epoch:  14 , batch:  162 , train loss:  9.361175537109375\n","epoch:  14 , batch:  163 , train loss:  19.23432731628418\n","epoch:  14 , batch:  164 , train loss:  10.194293022155762\n","epoch:  14 , batch:  165 , train loss:  20.11474609375\n","epoch:  14 , batch:  166 , train loss:  15.805340766906738\n","epoch:  14 , batch:  167 , train loss:  19.840679168701172\n","epoch:  14 , batch:  168 , train loss:  14.6326904296875\n","epoch:  14 , batch:  169 , train loss:  27.807920455932617\n","epoch:  14 , batch:  170 , train loss:  12.742850303649902\n","epoch:  14 , batch:  171 , train loss:  11.177820205688477\n","epoch:  14 , batch:  172 , train loss:  8.834750175476074\n","epoch:  14 , batch:  173 , train loss:  26.644039154052734\n","epoch:  14 , batch:  174 , train loss:  12.254512786865234\n","epoch:  14 , batch:  175 , train loss:  15.554583549499512\n","epoch:  14 , batch:  176 , train loss:  13.507753372192383\n","epoch:  14 , batch:  177 , train loss:  19.341642379760742\n","epoch:  14 , batch:  178 , train loss:  11.321048736572266\n","epoch:  14 , batch:  179 , train loss:  9.333678245544434\n","epoch:  14 , batch:  180 , train loss:  19.143447875976562\n","epoch:  14 , batch:  181 , train loss:  10.938383102416992\n","epoch:  14 , batch:  182 , train loss:  14.400469779968262\n","epoch:  14 , batch:  183 , train loss:  18.166576385498047\n","epoch:  14 , batch:  184 , train loss:  8.952895164489746\n","epoch:  14 , batch:  185 , train loss:  8.724161148071289\n","epoch:  14 , batch:  186 , train loss:  10.65267562866211\n","epoch:  14 , batch:  187 , train loss:  17.303504943847656\n","epoch:  14 , batch:  188 , train loss:  9.458320617675781\n","epoch:  14 , batch:  189 , train loss:  12.460488319396973\n","epoch:  14 , batch:  190 , train loss:  10.841581344604492\n","epoch:  14 , batch:  191 , train loss:  10.203407287597656\n","epoch:  14 , batch:  192 , train loss:  15.09501838684082\n","epoch:  14 , batch:  193 , train loss:  16.189809799194336\n","epoch:  14 , batch:  194 , train loss:  13.905458450317383\n","epoch:  14 , batch:  195 , train loss:  14.793408393859863\n","epoch:  14 , batch:  196 , train loss:  11.372926712036133\n","epoch:  14 , batch:  197 , train loss:  9.499855041503906\n","epoch:  14 , batch:  198 , train loss:  24.7293758392334\n","epoch:  14 , batch:  199 , train loss:  19.137807846069336\n","epoch:  14 , batch:  200 , train loss:  6.585486888885498\n","epoch:  14 , batch:  201 , train loss:  11.223794937133789\n","epoch:  14 , batch:  202 , train loss:  10.979952812194824\n","epoch:  14 , batch:  203 , train loss:  9.680493354797363\n","epoch:  14 , batch:  204 , train loss:  9.316410064697266\n","epoch:  14 , batch:  205 , train loss:  4.716203689575195\n","epoch:  14 , batch:  206 , train loss:  13.972123146057129\n","epoch:  14 , batch:  207 , train loss:  11.478854179382324\n","epoch:  14 , batch:  208 , train loss:  19.426267623901367\n","epoch:  14 , batch:  209 , train loss:  14.48265266418457\n","epoch:  14 , batch:  210 , train loss:  11.759170532226562\n","epoch:  14 , batch:  211 , train loss:  28.540319442749023\n","epoch:  14 , batch:  212 , train loss:  16.245819091796875\n","epoch:  14 , batch:  213 , train loss:  15.40489387512207\n","epoch:  14 , batch:  214 , train loss:  21.858491897583008\n","epoch:  14 , batch:  215 , train loss:  14.322107315063477\n","epoch:  14 , batch:  216 , train loss:  7.8081793785095215\n","epoch:  14 , batch:  217 , train loss:  13.385071754455566\n","epoch:  14 , batch:  218 , train loss:  10.992791175842285\n","epoch:  14 , batch:  219 , train loss:  9.130986213684082\n","epoch:  14 , batch:  220 , train loss:  18.16813850402832\n","epoch:  14 , batch:  221 , train loss:  16.367923736572266\n","epoch:  14 , batch:  222 , train loss:  13.938825607299805\n","epoch:  14 , batch:  223 , train loss:  10.16074275970459\n","epoch:  14 , batch:  224 , train loss:  18.189342498779297\n","epoch:  14 , batch:  225 , train loss:  22.499359130859375\n","epoch:  14 , batch:  226 , train loss:  13.2305908203125\n","epoch:  14 , batch:  227 , train loss:  6.16561222076416\n","epoch:  14 , batch:  228 , train loss:  8.520622253417969\n","epoch:  14 , batch:  229 , train loss:  14.274409294128418\n","epoch:  14 , batch:  230 , train loss:  8.573164939880371\n","epoch:  14 , batch:  231 , train loss:  14.506278991699219\n","epoch:  14 , batch:  232 , train loss:  14.648123741149902\n","epoch:  14 , batch:  233 , train loss:  6.955267906188965\n","epoch:  14 , batch:  234 , train loss:  26.198909759521484\n","epoch:  14 , batch:  235 , train loss:  17.756567001342773\n","epoch:  14 , batch:  236 , train loss:  18.203645706176758\n","epoch:  14 , batch:  237 , train loss:  4.301218032836914\n","epoch:  14 , batch:  238 , train loss:  18.89303970336914\n","epoch:  14 , batch:  239 , train loss:  12.52957820892334\n","epoch:  14 , batch:  240 , train loss:  11.565566062927246\n","epoch:  14 , batch:  241 , train loss:  11.35780143737793\n","epoch:  14 , batch:  242 , train loss:  14.381752967834473\n","epoch:  14 , batch:  243 , train loss:  9.206926345825195\n","epoch:  14 , batch:  244 , train loss:  9.608099937438965\n","epoch:  14 , batch:  245 , train loss:  10.892459869384766\n","epoch:  14 , batch:  246 , train loss:  17.43856430053711\n","epoch:  14 , batch:  247 , train loss:  13.584498405456543\n","epoch:  14 , batch:  248 , train loss:  25.67239761352539\n","epoch:  14 , batch:  249 , train loss:  12.735404014587402\n","epoch:  14 , batch:  250 , train loss:  18.386470794677734\n","epoch:  14 , batch:  251 , train loss:  10.151000022888184\n","epoch:  14 , batch:  252 , train loss:  11.007405281066895\n","epoch:  14 , batch:  253 , train loss:  11.393068313598633\n","epoch:  14 , batch:  254 , train loss:  12.19685173034668\n","epoch:  14 , batch:  255 , train loss:  11.993555068969727\n","epoch:  14 , batch:  256 , train loss:  19.279903411865234\n","epoch:  14 , batch:  257 , train loss:  11.487404823303223\n","epoch:  14 , batch:  258 , train loss:  8.122220993041992\n","epoch:  14 , batch:  259 , train loss:  14.648810386657715\n","epoch:  14 , batch:  260 , train loss:  15.909628868103027\n","epoch:  14 , batch:  261 , train loss:  14.515287399291992\n","epoch:  14 , batch:  262 , train loss:  10.250893592834473\n","epoch:  14 , batch:  263 , train loss:  5.043091773986816\n","epoch:  14 , batch:  264 , train loss:  10.575018882751465\n","epoch:  14 , batch:  265 , train loss:  15.798696517944336\n","epoch:  14 , batch:  266 , train loss:  18.315624237060547\n","epoch:  14 , batch:  267 , train loss:  10.935101509094238\n","epoch:  14 , batch:  268 , train loss:  7.706961154937744\n","epoch:  14 , batch:  269 , train loss:  15.451435089111328\n","epoch:  14 , batch:  270 , train loss:  7.418222427368164\n","epoch:  14 , batch:  271 , train loss:  5.594501495361328\n","epoch:  14 , batch:  272 , train loss:  15.288060188293457\n","epoch:  14 , batch:  273 , train loss:  12.72298526763916\n","epoch:  14 , batch:  274 , train loss:  8.619576454162598\n","epoch:  14 , batch:  275 , train loss:  14.651285171508789\n","epoch:  14 , batch:  276 , train loss:  8.395085334777832\n","epoch:  14 , batch:  277 , train loss:  6.856617450714111\n","epoch:  14 , batch:  278 , train loss:  10.00969123840332\n","epoch:  14 , batch:  279 , train loss:  21.387205123901367\n","epoch:  14 , batch:  280 , train loss:  13.172082901000977\n","epoch:  14 , batch:  281 , train loss:  30.781126022338867\n","epoch:  14 , batch:  282 , train loss:  24.706979751586914\n","epoch:  14 , batch:  283 , train loss:  10.222029685974121\n","epoch:  14 , batch:  284 , train loss:  8.957657814025879\n","epoch:  14 , batch:  285 , train loss:  11.999763488769531\n","epoch:  14 , batch:  286 , train loss:  14.491178512573242\n","epoch:  14 , batch:  287 , train loss:  13.945112228393555\n","epoch:  14 , batch:  288 , train loss:  24.726835250854492\n","epoch:  14 , batch:  289 , train loss:  17.8635311126709\n","epoch:  14 , batch:  290 , train loss:  14.722257614135742\n","epoch:  14 , batch:  291 , train loss:  15.492352485656738\n","epoch:  14 , batch:  292 , train loss:  13.843338966369629\n","epoch:  14 , batch:  293 , train loss:  9.954906463623047\n","epoch:  14 , batch:  294 , train loss:  14.303245544433594\n","epoch:  14 , batch:  295 , train loss:  8.406821250915527\n","epoch:  14 , batch:  296 , train loss:  8.091273307800293\n","epoch:  14 , batch:  297 , train loss:  8.70200252532959\n","epoch:  14 , batch:  298 , train loss:  23.848398208618164\n","epoch:  14 , batch:  299 , train loss:  19.13356590270996\n","epoch:  14 , batch:  300 , train loss:  13.588471412658691\n","epoch:  14 , batch:  301 , train loss:  13.073055267333984\n","epoch:  14 , batch:  302 , train loss:  11.272684097290039\n","epoch:  14 , batch:  303 , train loss:  11.549373626708984\n","epoch:  14 , batch:  304 , train loss:  6.534630298614502\n","epoch:  14 , batch:  305 , train loss:  20.07386589050293\n","epoch:  14 , batch:  306 , train loss:  14.054577827453613\n","epoch:  14 , batch:  307 , train loss:  16.537214279174805\n","epoch:  14 , batch:  308 , train loss:  10.348736763000488\n","epoch:  14 , batch:  309 , train loss:  14.57834529876709\n","epoch:  14 , batch:  310 , train loss:  9.972636222839355\n","epoch:  14 , batch:  311 , train loss:  22.254037857055664\n","epoch:  14 , batch:  312 , train loss:  10.242467880249023\n","epoch:  14 , batch:  313 , train loss:  37.94487380981445\n","epoch:  14 , batch:  314 , train loss:  12.619972229003906\n","epoch:  14 , batch:  315 , train loss:  18.948884963989258\n","epoch:  14 , batch:  316 , train loss:  10.561697959899902\n","epoch:  14 , batch:  317 , train loss:  21.460155487060547\n","epoch:  14 , batch:  318 , train loss:  8.451316833496094\n","epoch:  14 , batch:  319 , train loss:  10.101758003234863\n","epoch:  14 , batch:  320 , train loss:  10.953372955322266\n","epoch:  14 , batch:  321 , train loss:  20.083276748657227\n","epoch:  14 , batch:  322 , train loss:  22.148357391357422\n","epoch:  14 , batch:  323 , train loss:  19.26814079284668\n","epoch:  14 , batch:  324 , train loss:  9.876489639282227\n","epoch:  14 , batch:  325 , train loss:  9.819430351257324\n","epoch:  14 , batch:  326 , train loss:  8.634268760681152\n","epoch:  14 , batch:  327 , train loss:  27.861711502075195\n","epoch:  14 , batch:  328 , train loss:  17.96104621887207\n","epoch:  14 , batch:  329 , train loss:  10.572394371032715\n","epoch:  14 , batch:  330 , train loss:  13.090848922729492\n","epoch:  14 , batch:  331 , train loss:  7.195553302764893\n","epoch:  14 , batch:  332 , train loss:  18.63055419921875\n","epoch:  14 , batch:  333 , train loss:  16.57411003112793\n","epoch:  14 , batch:  334 , train loss:  10.400031089782715\n","epoch:  14 , batch:  335 , train loss:  11.115467071533203\n","epoch:  14 , batch:  336 , train loss:  10.353157043457031\n","epoch:  14 , batch:  337 , train loss:  20.485244750976562\n","epoch:  14 , batch:  338 , train loss:  8.657636642456055\n","epoch:  14 , batch:  339 , train loss:  16.07863426208496\n","epoch:  14 , batch:  340 , train loss:  12.831781387329102\n","epoch:  14 , batch:  341 , train loss:  20.964136123657227\n","epoch:  14 , batch:  342 , train loss:  20.24053955078125\n","epoch:  14 , batch:  343 , train loss:  14.032181739807129\n","epoch:  14 , batch:  344 , train loss:  10.409128189086914\n","epoch:  14 , batch:  345 , train loss:  11.303522109985352\n","epoch:  14 , batch:  346 , train loss:  20.560501098632812\n","epoch:  14 , batch:  347 , train loss:  10.489439010620117\n","epoch:  14 , batch:  348 , train loss:  14.752044677734375\n","epoch:  14 , batch:  349 , train loss:  15.031148910522461\n","epoch:  14 , batch:  350 , train loss:  7.546731948852539\n","epoch:  14 , batch:  351 , train loss:  7.1603474617004395\n","epoch:  14 , batch:  352 , train loss:  21.07567596435547\n","epoch:  14 , batch:  353 , train loss:  13.743922233581543\n","epoch:  14 , batch:  354 , train loss:  16.606245040893555\n","epoch:  14 , batch:  355 , train loss:  15.584905624389648\n","epoch:  14 , batch:  356 , train loss:  13.6357421875\n","epoch:  14 , batch:  357 , train loss:  24.11884880065918\n","epoch:  14 , batch:  358 , train loss:  18.430490493774414\n","epoch:  14 , batch:  359 , train loss:  16.536224365234375\n","epoch:  14 , batch:  360 , train loss:  12.647992134094238\n","epoch:  14 , batch:  361 , train loss:  2.694390058517456\n","epoch:  14 , batch:  362 , train loss:  13.49521255493164\n","epoch:  14 , batch:  363 , train loss:  24.5161190032959\n","epoch:  14 , batch:  364 , train loss:  31.847450256347656\n","epoch:  14 , batch:  365 , train loss:  21.384389877319336\n","epoch:  14 , batch:  366 , train loss:  13.830698013305664\n","epoch:  14 , batch:  367 , train loss:  28.713459014892578\n","epoch:  14 , batch:  368 , train loss:  14.12330436706543\n","epoch:  14 , batch:  369 , train loss:  18.341068267822266\n","epoch:  14 , batch:  370 , train loss:  19.35141944885254\n","epoch:  14 , batch:  371 , train loss:  26.654233932495117\n","epoch:  14 , batch:  372 , train loss:  15.376717567443848\n","epoch:  14 , batch:  373 , train loss:  22.117984771728516\n","epoch:  14 , batch:  374 , train loss:  18.990665435791016\n","epoch:  14 , batch:  375 , train loss:  9.855669021606445\n","epoch:  14 , batch:  376 , train loss:  10.381065368652344\n","epoch:  14 , batch:  377 , train loss:  15.261813163757324\n","epoch:  14 , batch:  378 , train loss:  9.143534660339355\n","epoch:  14 , batch:  379 , train loss:  16.852535247802734\n","epoch:  14 , batch:  380 , train loss:  13.255784034729004\n","epoch:  14 , batch:  381 , train loss:  7.307345390319824\n","epoch:  14 , batch:  382 , train loss:  13.703102111816406\n","epoch:  14 , batch:  383 , train loss:  8.854484558105469\n","epoch:  14 , batch:  384 , train loss:  6.850086212158203\n","epoch:  14 , batch:  385 , train loss:  17.34387969970703\n","epoch:  14 , batch:  386 , train loss:  19.074398040771484\n","epoch:  14 , batch:  387 , train loss:  18.332828521728516\n","epoch:  14 , batch:  388 , train loss:  8.167509078979492\n","epoch:  14 , batch:  389 , train loss:  25.159713745117188\n","epoch:  14 , batch:  390 , train loss:  21.002365112304688\n","epoch:  14 , batch:  391 , train loss:  16.4175968170166\n","epoch:  14 , batch:  392 , train loss:  23.725645065307617\n","epoch:  14 , batch:  393 , train loss:  7.281149864196777\n","epoch:  14 , batch:  394 , train loss:  15.159051895141602\n","epoch:  14 , batch:  395 , train loss:  12.250425338745117\n","epoch:  14 , batch:  396 , train loss:  14.96103572845459\n","epoch:  14 , batch:  397 , train loss:  11.34128475189209\n","epoch:  14 , batch:  398 , train loss:  13.55614185333252\n","epoch:  14 , batch:  399 , train loss:  11.937283515930176\n","epoch:  14 , batch:  400 , train loss:  7.966095447540283\n","epoch:  14 , batch:  401 , train loss:  21.032669067382812\n","epoch:  14 , batch:  402 , train loss:  13.12966251373291\n","epoch:  14 , batch:  403 , train loss:  6.826519012451172\n","epoch:  14 , batch:  404 , train loss:  10.232319831848145\n","epoch:  14 , batch:  405 , train loss:  18.481672286987305\n","epoch:  14 , batch:  406 , train loss:  17.583293914794922\n","epoch:  14 , batch:  407 , train loss:  24.13058090209961\n","epoch:  14 , batch:  408 , train loss:  10.057168960571289\n","epoch:  14 , batch:  409 , train loss:  6.421377658843994\n","epoch:  14 , batch:  410 , train loss:  15.099848747253418\n","epoch:  14 , batch:  411 , train loss:  6.6965508460998535\n","epoch:  14 , batch:  412 , train loss:  15.895125389099121\n","epoch:  14 , batch:  413 , train loss:  19.192293167114258\n","epoch:  14 , batch:  414 , train loss:  8.803238868713379\n","epoch:  14 , batch:  415 , train loss:  18.805625915527344\n","epoch:  14 , batch:  416 , train loss:  9.903844833374023\n","epoch:  14 , batch:  417 , train loss:  15.303388595581055\n","epoch:  14 , batch:  418 , train loss:  13.754399299621582\n","epoch:  14 , batch:  419 , train loss:  23.955806732177734\n","epoch:  14 , batch:  420 , train loss:  9.00218677520752\n","epoch:  14 , batch:  421 , train loss:  10.681730270385742\n","epoch:  14 , batch:  422 , train loss:  16.57182502746582\n","epoch:  14 , batch:  423 , train loss:  22.128192901611328\n","epoch:  14 , batch:  424 , train loss:  14.809821128845215\n","epoch:  14 , batch:  425 , train loss:  7.441946029663086\n","epoch:  14 , batch:  426 , train loss:  16.03763198852539\n","epoch:  14 , batch:  427 , train loss:  22.73430061340332\n","epoch:  14 , batch:  428 , train loss:  16.193798065185547\n","epoch:  14 , batch:  429 , train loss:  27.136594772338867\n","epoch:  14 , batch:  430 , train loss:  13.761825561523438\n","epoch:  14 , batch:  431 , train loss:  16.035905838012695\n","epoch:  14 , batch:  432 , train loss:  12.441259384155273\n","epoch:  14 , batch:  433 , train loss:  9.031219482421875\n","epoch:  14 , batch:  434 , train loss:  12.555951118469238\n","epoch:  14 , batch:  435 , train loss:  16.8310489654541\n","epoch:  14 , batch:  436 , train loss:  17.226184844970703\n","epoch:  14 , batch:  437 , train loss:  22.933847427368164\n","epoch:  14 , batch:  438 , train loss:  31.06963348388672\n","epoch:  14 , batch:  439 , train loss:  12.157489776611328\n","epoch:  14 , batch:  440 , train loss:  10.481518745422363\n","epoch:  14 , batch:  441 , train loss:  16.931476593017578\n","epoch:  14 , batch:  442 , train loss:  11.014382362365723\n","epoch:  14 , batch:  443 , train loss:  13.86365032196045\n","epoch:  14 , batch:  444 , train loss:  13.014978408813477\n","epoch:  14 , batch:  445 , train loss:  12.218278884887695\n","epoch:  14 , batch:  446 , train loss:  11.589279174804688\n","epoch:  14 , batch:  447 , train loss:  16.823776245117188\n","epoch:  14 , batch:  448 , train loss:  25.993051528930664\n","epoch:  14 , batch:  449 , train loss:  18.612045288085938\n","epoch:  14 , batch:  450 , train loss:  13.056849479675293\n","epoch:  14 , batch:  451 , train loss:  21.657291412353516\n","epoch:  14 , batch:  452 , train loss:  12.453910827636719\n","epoch:  14 , batch:  453 , train loss:  7.1072001457214355\n","epoch:  14 , batch:  454 , train loss:  22.295757293701172\n","epoch:  14 , batch:  455 , train loss:  19.869760513305664\n","epoch:  14 , batch:  456 , train loss:  17.2705020904541\n","epoch:  14 , batch:  457 , train loss:  10.510581970214844\n","epoch:  14 , batch:  458 , train loss:  15.24737548828125\n","epoch:  14 , batch:  459 , train loss:  16.013288497924805\n","epoch:  14 , batch:  460 , train loss:  15.33457088470459\n","epoch:  14 , batch:  461 , train loss:  7.970498561859131\n","epoch:  14 , batch:  462 , train loss:  9.638545989990234\n","epoch:  14 , batch:  463 , train loss:  25.113985061645508\n","epoch:  14 , batch:  464 , train loss:  22.093463897705078\n","epoch:  14 , batch:  465 , train loss:  14.966974258422852\n","epoch:  14 , batch:  466 , train loss:  22.01849365234375\n","epoch:  14 , batch:  467 , train loss:  7.336799621582031\n","epoch:  14 , batch:  468 , train loss:  15.880013465881348\n","epoch:  14 , batch:  469 , train loss:  15.400567054748535\n","epoch:  14 , batch:  470 , train loss:  14.134160041809082\n","epoch:  14 , batch:  471 , train loss:  14.850431442260742\n","epoch:  14 , batch:  472 , train loss:  23.166839599609375\n","epoch:  14 , batch:  473 , train loss:  13.56277084350586\n","epoch:  14 , batch:  474 , train loss:  23.81512451171875\n","epoch:  14 , batch:  475 , train loss:  20.70304298400879\n","epoch:  14 , batch:  476 , train loss:  20.5919132232666\n","epoch:  14 , batch:  477 , train loss:  15.430606842041016\n","epoch:  14 , batch:  478 , train loss:  14.918440818786621\n","epoch:  14 , batch:  479 , train loss:  17.99928092956543\n","epoch:  14 , batch:  480 , train loss:  11.510452270507812\n","epoch:  14 , batch:  481 , train loss:  12.361388206481934\n","epoch:  14 , batch:  482 , train loss:  17.702287673950195\n","epoch:  14 , batch:  483 , train loss:  5.168600559234619\n","epoch:  14 , batch:  484 , train loss:  15.066742897033691\n","epoch:  14 , batch:  485 , train loss:  16.09569549560547\n","epoch:  14 , batch:  486 , train loss:  10.037454605102539\n","epoch:  14 , batch:  487 , train loss:  6.969156742095947\n","epoch:  14 , batch:  488 , train loss:  4.256866455078125\n","epoch:  14 , batch:  489 , train loss:  20.88283920288086\n","epoch:  14 , batch:  490 , train loss:  14.719497680664062\n","epoch:  14 , batch:  491 , train loss:  10.888422966003418\n","epoch:  14 , batch:  492 , train loss:  8.356928825378418\n","epoch:  14 , batch:  493 , train loss:  11.618894577026367\n","epoch:  14 , batch:  494 , train loss:  10.935964584350586\n","epoch:  14 , batch:  495 , train loss:  10.191882133483887\n","epoch:  14 , batch:  496 , train loss:  23.46997833251953\n","epoch:  14 , batch:  497 , train loss:  12.89103889465332\n","epoch:  14 , batch:  498 , train loss:  18.998348236083984\n","epoch:  14 , batch:  499 , train loss:  13.793051719665527\n","epoch:  14 , batch:  500 , train loss:  15.303289413452148\n","epoch:  14 , batch:  501 , train loss:  16.707538604736328\n","epoch:  14 , batch:  502 , train loss:  21.5177001953125\n","epoch:  14 , batch:  503 , train loss:  21.103952407836914\n","epoch:  14 , batch:  504 , train loss:  14.346526145935059\n","epoch:  14 , batch:  505 , train loss:  28.78353500366211\n","epoch:  14 , batch:  506 , train loss:  16.412256240844727\n","epoch:  14 , batch:  507 , train loss:  12.335710525512695\n","epoch:  14 , batch:  508 , train loss:  10.180488586425781\n","epoch:  14 , batch:  509 , train loss:  17.247722625732422\n","epoch:  14 , batch:  510 , train loss:  21.682214736938477\n","epoch:  14 , batch:  511 , train loss:  19.38858985900879\n","epoch:  14 , batch:  512 , train loss:  5.936233043670654\n","epoch:  14 , batch:  513 , train loss:  16.416324615478516\n","epoch:  14 , batch:  514 , train loss:  16.04539680480957\n","epoch:  14 , batch:  515 , train loss:  15.151717185974121\n","epoch:  14 , batch:  516 , train loss:  10.84745979309082\n","epoch:  14 , batch:  517 , train loss:  18.082260131835938\n","epoch:  14 , batch:  518 , train loss:  5.866091728210449\n","epoch:  14 , batch:  519 , train loss:  13.790034294128418\n","epoch:  14 , batch:  520 , train loss:  20.667192459106445\n","epoch:  14 , batch:  521 , train loss:  9.989919662475586\n","epoch:  14 , batch:  522 , train loss:  17.273181915283203\n","epoch:  14 , batch:  523 , train loss:  14.3525390625\n","epoch:  14 , batch:  524 , train loss:  13.22378921508789\n","epoch:  14 , batch:  525 , train loss:  22.788055419921875\n","epoch:  14 , batch:  526 , train loss:  19.408470153808594\n","epoch:  14 , batch:  527 , train loss:  18.816553115844727\n","epoch:  14 , batch:  528 , train loss:  6.652153015136719\n","epoch:  14 , batch:  529 , train loss:  10.9606351852417\n","epoch:  14 , batch:  530 , train loss:  8.762072563171387\n","epoch:  14 , batch:  531 , train loss:  12.76831340789795\n","epoch:  14 , batch:  532 , train loss:  14.333574295043945\n","epoch:  14 , batch:  533 , train loss:  26.411609649658203\n","epoch:  14 , batch:  534 , train loss:  32.61754608154297\n","epoch:  14 , batch:  535 , train loss:  28.43882942199707\n","epoch:  14 , batch:  536 , train loss:  15.059865951538086\n","epoch:  14 , batch:  537 , train loss:  14.268657684326172\n","epoch:  14 , batch:  538 , train loss:  15.854930877685547\n","epoch:  14 , batch:  539 , train loss:  6.7498626708984375\n","epoch:  14 , batch:  540 , train loss:  14.260536193847656\n","epoch:  14 , batch:  541 , train loss:  10.872323989868164\n","epoch:  14 , batch:  542 , train loss:  17.790565490722656\n","epoch:  14 , batch:  543 , train loss:  45.9503288269043\n","epoch:  14 , batch:  544 , train loss:  16.451147079467773\n","epoch:  14 , batch:  545 , train loss:  12.90768051147461\n","epoch:  14 , batch:  546 , train loss:  20.431995391845703\n","epoch:  14 , batch:  547 , train loss:  16.80428695678711\n","epoch:  14 , batch:  548 , train loss:  11.874720573425293\n","epoch:  14 , batch:  549 , train loss:  8.599245071411133\n","epoch:  14 , batch:  550 , train loss:  15.930596351623535\n","epoch:  14 , batch:  551 , train loss:  9.606955528259277\n","epoch:  14 , batch:  552 , train loss:  8.822553634643555\n","epoch:  14 , batch:  553 , train loss:  13.627957344055176\n","epoch:  14 , batch:  554 , train loss:  6.194822311401367\n","epoch:  14 , batch:  555 , train loss:  9.746282577514648\n","epoch:  14 , batch:  556 , train loss:  9.665054321289062\n","epoch:  14 , batch:  557 , train loss:  18.924455642700195\n","epoch:  14 , batch:  558 , train loss:  11.383482933044434\n","epoch:  14 , batch:  559 , train loss:  16.80460548400879\n","epoch:  14 , batch:  560 , train loss:  23.465662002563477\n","epoch:  14 , batch:  561 , train loss:  10.481684684753418\n","epoch:  14 , batch:  562 , train loss:  13.94687271118164\n","epoch:  14 , batch:  563 , train loss:  9.705110549926758\n","epoch:  14 , batch:  564 , train loss:  20.98390769958496\n","epoch:  14 , batch:  565 , train loss:  16.28974151611328\n","epoch:  14 , batch:  566 , train loss:  23.96724510192871\n","epoch:  14 , batch:  567 , train loss:  16.225082397460938\n","epoch:  14 , batch:  568 , train loss:  13.419146537780762\n","epoch:  14 , batch:  569 , train loss:  9.740073204040527\n","epoch:  14 , batch:  570 , train loss:  19.66044044494629\n","epoch:  14 , batch:  571 , train loss:  7.4306135177612305\n","epoch:  14 , batch:  572 , train loss:  16.201749801635742\n","epoch:  14 , batch:  573 , train loss:  15.081305503845215\n","epoch:  14 , batch:  574 , train loss:  12.935675621032715\n","epoch:  14 , batch:  575 , train loss:  16.549480438232422\n","epoch:  14 , batch:  576 , train loss:  14.323663711547852\n","epoch:  14 , batch:  577 , train loss:  8.044209480285645\n","epoch:  14 , batch:  578 , train loss:  9.280268669128418\n","epoch:  14 , batch:  579 , train loss:  7.31383752822876\n","epoch:  14 , batch:  580 , train loss:  11.573051452636719\n","epoch:  14 , batch:  581 , train loss:  20.531906127929688\n","epoch:  14 , batch:  582 , train loss:  12.264505386352539\n","epoch:  14 , batch:  583 , train loss:  12.778055191040039\n","epoch:  14 , batch:  584 , train loss:  20.70098876953125\n","epoch:  14 , batch:  585 , train loss:  16.02931785583496\n","epoch:  14 , batch:  586 , train loss:  21.127670288085938\n","epoch:  14 , batch:  587 , train loss:  13.845000267028809\n","epoch:  14 , batch:  588 , train loss:  13.203685760498047\n","epoch:  14 , batch:  589 , train loss:  9.212332725524902\n","epoch:  14 , batch:  590 , train loss:  10.422849655151367\n","epoch:  14 , batch:  591 , train loss:  19.09857749938965\n","epoch:  14 , batch:  592 , train loss:  16.639467239379883\n","epoch:  14 , batch:  593 , train loss:  8.394720077514648\n","epoch:  14 , batch:  594 , train loss:  4.332220077514648\n","epoch:  14 , batch:  595 , train loss:  24.054222106933594\n","epoch:  14 , batch:  596 , train loss:  10.711430549621582\n","epoch:  14 , batch:  597 , train loss:  13.353168487548828\n","epoch:  14 , batch:  598 , train loss:  8.639570236206055\n","epoch:  14 , batch:  599 , train loss:  24.920408248901367\n","epoch:  14 , batch:  600 , train loss:  14.077366828918457\n","epoch:  14 , batch:  601 , train loss:  16.529434204101562\n","epoch:  14 , batch:  602 , train loss:  17.146074295043945\n","epoch:  14 , batch:  603 , train loss:  11.951322555541992\n","epoch:  14 , batch:  604 , train loss:  20.130640029907227\n","epoch:  14 , batch:  605 , train loss:  18.372350692749023\n","epoch:  14 , batch:  606 , train loss:  7.872654438018799\n","epoch:  14 , batch:  607 , train loss:  11.226189613342285\n","epoch:  14 , batch:  608 , train loss:  11.621712684631348\n","epoch:  14 , batch:  609 , train loss:  10.769927024841309\n","epoch:  14 , batch:  610 , train loss:  17.195833206176758\n","epoch:  14 , batch:  611 , train loss:  10.886462211608887\n","epoch:  14 , batch:  612 , train loss:  15.595111846923828\n","epoch:  14 , batch:  613 , train loss:  21.824325561523438\n","epoch:  14 , batch:  614 , train loss:  13.47131633758545\n","epoch:  14 , batch:  615 , train loss:  10.891035079956055\n","epoch:  14 , batch:  616 , train loss:  8.76956558227539\n","epoch:  14 , batch:  617 , train loss:  8.756424903869629\n","epoch:  14 , batch:  618 , train loss:  26.212312698364258\n","epoch:  14 , batch:  619 , train loss:  15.094351768493652\n","epoch:  14 , batch:  620 , train loss:  15.879790306091309\n","epoch:  14 , batch:  621 , train loss:  12.37157154083252\n","epoch:  14 , batch:  622 , train loss:  27.434452056884766\n","epoch:  14 , batch:  623 , train loss:  29.195241928100586\n","epoch:  14 , batch:  624 , train loss:  15.619229316711426\n","epoch:  14 , batch:  625 , train loss:  16.21405029296875\n","epoch:  14 , batch:  626 , train loss:  17.450876235961914\n","epoch:  14 , batch:  627 , train loss:  14.50777530670166\n","epoch:  14 , batch:  628 , train loss:  16.18338966369629\n","epoch:  14 , batch:  629 , train loss:  24.798721313476562\n","epoch:  14 , batch:  630 , train loss:  11.231472969055176\n","epoch:  14 , batch:  631 , train loss:  17.883487701416016\n","epoch:  14 , batch:  632 , train loss:  13.785900115966797\n","epoch:  14 , batch:  633 , train loss:  18.988901138305664\n","epoch:  14 , batch:  634 , train loss:  16.114601135253906\n","epoch:  14 , batch:  635 , train loss:  21.601043701171875\n","epoch:  14 , batch:  636 , train loss:  13.483567237854004\n","epoch:  14 , batch:  637 , train loss:  12.688838958740234\n","epoch:  14 , batch:  638 , train loss:  10.527225494384766\n","epoch:  14 , batch:  639 , train loss:  12.244194984436035\n","epoch:  14 , batch:  640 , train loss:  17.99062156677246\n","epoch:  14 , batch:  641 , train loss:  21.286293029785156\n","epoch:  14 , batch:  642 , train loss:  12.532960891723633\n","epoch:  14 , batch:  643 , train loss:  20.19554328918457\n","epoch:  14 , batch:  644 , train loss:  7.531504154205322\n","epoch:  14 , batch:  645 , train loss:  10.98070240020752\n","epoch:  14 , batch:  646 , train loss:  13.063655853271484\n","epoch:  14 , batch:  647 , train loss:  13.415258407592773\n","epoch:  14 , batch:  648 , train loss:  15.17213249206543\n","epoch:  14 , batch:  649 , train loss:  20.46919059753418\n","epoch:  14 , batch:  650 , train loss:  13.13390064239502\n","epoch:  14 , batch:  651 , train loss:  13.334651947021484\n","epoch:  14 , batch:  652 , train loss:  17.68028450012207\n","epoch:  14 , batch:  653 , train loss:  18.303062438964844\n","epoch:  14 , batch:  654 , train loss:  15.023006439208984\n","epoch:  14 , batch:  655 , train loss:  21.608793258666992\n","epoch:  14 , batch:  656 , train loss:  17.573684692382812\n","epoch:  14 , batch:  657 , train loss:  13.48759651184082\n","epoch:  14 , batch:  658 , train loss:  16.323577880859375\n","epoch:  14 , batch:  659 , train loss:  9.722039222717285\n","epoch:  14 , batch:  660 , train loss:  15.059590339660645\n","epoch:  14 , batch:  661 , train loss:  15.010263442993164\n","epoch:  14 , batch:  662 , train loss:  14.142321586608887\n","epoch:  14 , batch:  663 , train loss:  19.36598014831543\n","epoch:  14 , batch:  664 , train loss:  15.823759078979492\n","epoch:  14 , batch:  665 , train loss:  10.637063026428223\n","epoch:  14 , batch:  666 , train loss:  11.195717811584473\n","epoch:  14 , batch:  667 , train loss:  19.76767349243164\n","epoch:  14 , batch:  668 , train loss:  14.978293418884277\n","epoch:  14 , batch:  669 , train loss:  25.923582077026367\n","epoch:  14 , batch:  670 , train loss:  10.953295707702637\n","epoch:  14 , batch:  671 , train loss:  21.614826202392578\n","epoch:  14 , batch:  672 , train loss:  7.121382713317871\n","epoch:  14 , batch:  673 , train loss:  7.529585361480713\n","epoch:  14 , batch:  674 , train loss:  15.276835441589355\n","epoch:  14 , batch:  675 , train loss:  14.315266609191895\n","epoch:  14 , batch:  676 , train loss:  12.798530578613281\n","epoch:  14 , batch:  677 , train loss:  22.344697952270508\n","epoch:  14 , batch:  678 , train loss:  16.168859481811523\n","epoch:  14 , batch:  679 , train loss:  25.015758514404297\n","epoch:  14 , batch:  680 , train loss:  12.51998519897461\n","epoch:  14 , batch:  681 , train loss:  10.905117988586426\n","epoch:  14 , batch:  682 , train loss:  20.275218963623047\n","epoch:  14 , batch:  683 , train loss:  23.086864471435547\n","epoch:  14 , batch:  684 , train loss:  12.091429710388184\n","epoch:  14 , batch:  685 , train loss:  13.800796508789062\n","epoch:  14 , batch:  686 , train loss:  24.666906356811523\n","epoch:  14 , batch:  687 , train loss:  16.927669525146484\n","epoch:  14 , batch:  688 , train loss:  14.808177947998047\n","epoch:  14 , batch:  689 , train loss:  10.22026252746582\n","epoch:  14 , batch:  690 , train loss:  21.41509437561035\n","epoch:  14 , batch:  691 , train loss:  9.93050765991211\n","epoch:  14 , batch:  692 , train loss:  16.86969566345215\n","epoch:  14 , batch:  693 , train loss:  28.447690963745117\n","epoch:  14 , batch:  694 , train loss:  14.08978271484375\n","epoch:  14 , batch:  695 , train loss:  18.99009132385254\n","epoch:  14 , batch:  696 , train loss:  12.430335998535156\n","epoch:  14 , batch:  697 , train loss:  13.809736251831055\n","epoch:  14 , batch:  698 , train loss:  10.267725944519043\n","epoch:  14 , batch:  699 , train loss:  20.709209442138672\n","epoch:  14 , batch:  700 , train loss:  20.698421478271484\n","epoch:  14 , batch:  701 , train loss:  9.165610313415527\n","epoch:  14 , batch:  702 , train loss:  21.77880859375\n","epoch:  14 , batch:  703 , train loss:  15.559479713439941\n","epoch:  14 , batch:  704 , train loss:  24.603111267089844\n","epoch:  14 , batch:  705 , train loss:  11.519803047180176\n","epoch:  14 , batch:  706 , train loss:  16.255348205566406\n","epoch:  14 , batch:  707 , train loss:  25.133787155151367\n","epoch:  14 , batch:  708 , train loss:  8.25245189666748\n","epoch:  14 , batch:  709 , train loss:  19.912799835205078\n","epoch:  14 , batch:  710 , train loss:  14.669534683227539\n","epoch:  14 , batch:  711 , train loss:  17.813583374023438\n","epoch:  14 , batch:  712 , train loss:  16.53424072265625\n","epoch:  14 , batch:  713 , train loss:  12.840968132019043\n","epoch:  14 , batch:  714 , train loss:  16.6021728515625\n","epoch:  14 , batch:  715 , train loss:  7.696764945983887\n","epoch:  14 , batch:  716 , train loss:  7.5381598472595215\n","epoch:  14 , batch:  717 , train loss:  8.199126243591309\n","epoch:  14 , batch:  718 , train loss:  8.8839750289917\n","epoch:  14 , batch:  719 , train loss:  13.96744441986084\n","epoch:  14 , batch:  720 , train loss:  11.215081214904785\n","epoch:  14 , batch:  721 , train loss:  12.548519134521484\n","epoch:  14 , batch:  722 , train loss:  19.541759490966797\n","epoch:  14 , batch:  723 , train loss:  6.765379428863525\n","epoch:  14 , batch:  724 , train loss:  16.27459716796875\n","epoch:  14 , batch:  725 , train loss:  15.423209190368652\n","epoch:  14 , batch:  726 , train loss:  11.268945693969727\n","epoch:  14 , batch:  727 , train loss:  14.799072265625\n","epoch:  14 , batch:  728 , train loss:  15.941019058227539\n","epoch:  14 , batch:  729 , train loss:  10.829693794250488\n","epoch:  14 , batch:  730 , train loss:  12.994853019714355\n","epoch:  14 , batch:  731 , train loss:  19.082298278808594\n","epoch:  14 , batch:  732 , train loss:  22.487010955810547\n","epoch:  14 , batch:  733 , train loss:  18.9429988861084\n","epoch:  14 , batch:  734 , train loss:  11.131661415100098\n","epoch:  14 , batch:  735 , train loss:  15.769471168518066\n","epoch:  14 , batch:  736 , train loss:  9.506958961486816\n","epoch:  14 , batch:  737 , train loss:  14.260664939880371\n","epoch:  14 , batch:  738 , train loss:  12.341153144836426\n","epoch:  14 , batch:  739 , train loss:  16.02553939819336\n","epoch:  14 , batch:  740 , train loss:  14.890254020690918\n","epoch:  14 , batch:  741 , train loss:  7.047413349151611\n","epoch:  14 , batch:  742 , train loss:  17.086694717407227\n","epoch:  14 , batch:  743 , train loss:  16.94584846496582\n","epoch:  14 , batch:  744 , train loss:  16.975534439086914\n","epoch:  14 , batch:  745 , train loss:  18.433149337768555\n","epoch:  14 , batch:  746 , train loss:  23.632373809814453\n","epoch:  14 , batch:  747 , train loss:  20.18052101135254\n","epoch:  14 , batch:  748 , train loss:  25.670495986938477\n","epoch:  14 , batch:  749 , train loss:  14.489948272705078\n","epoch:  14 , batch:  750 , train loss:  13.119436264038086\n","epoch:  14 , batch:  751 , train loss:  12.292496681213379\n","epoch:  14 , batch:  752 , train loss:  17.889202117919922\n","epoch:  14 , batch:  753 , train loss:  21.308412551879883\n","epoch:  14 , batch:  754 , train loss:  14.360746383666992\n","epoch:  14 , batch:  755 , train loss:  16.62848663330078\n","epoch:  14 , batch:  756 , train loss:  13.18647289276123\n","epoch:  14 , batch:  757 , train loss:  9.286012649536133\n","epoch:  14 , batch:  758 , train loss:  9.243281364440918\n","epoch:  14 , batch:  759 , train loss:  20.0927791595459\n","epoch:  14 , batch:  760 , train loss:  13.514093399047852\n","epoch:  14 , batch:  761 , train loss:  13.983184814453125\n","epoch:  14 , batch:  762 , train loss:  11.807826042175293\n","epoch:  14 , batch:  763 , train loss:  15.290288925170898\n","epoch:  14 , batch:  764 , train loss:  8.342206001281738\n","epoch:  14 , batch:  765 , train loss:  9.643268585205078\n","epoch:  14 , batch:  766 , train loss:  14.970947265625\n","epoch:  14 , batch:  767 , train loss:  15.256879806518555\n","epoch:  14 , batch:  768 , train loss:  12.750493049621582\n","epoch:  14 , batch:  769 , train loss:  13.437071800231934\n","epoch:  14 , batch:  770 , train loss:  3.451552629470825\n","epoch:  14 , batch:  771 , train loss:  24.879295349121094\n","epoch:  14 , batch:  772 , train loss:  16.12908363342285\n","epoch:  14 , batch:  773 , train loss:  9.764998435974121\n","epoch:  14 , batch:  774 , train loss:  18.060829162597656\n","epoch:  14 , batch:  775 , train loss:  19.46875762939453\n","epoch:  14 , batch:  776 , train loss:  13.24274730682373\n","epoch:  14 , batch:  777 , train loss:  10.493603706359863\n","epoch:  14 , batch:  778 , train loss:  12.633064270019531\n","epoch:  14 , batch:  779 , train loss:  17.389854431152344\n","epoch:  14 , batch:  780 , train loss:  10.963560104370117\n","epoch:  14 , batch:  781 , train loss:  17.156085968017578\n","epoch:  14 , batch:  782 , train loss:  17.66327476501465\n","epoch:  14 , batch:  783 , train loss:  18.017221450805664\n","epoch:  14 , batch:  784 , train loss:  9.42697811126709\n","epoch:  14 , batch:  785 , train loss:  15.947036743164062\n","epoch:  14 , batch:  786 , train loss:  14.386446952819824\n","epoch:  14 , batch:  787 , train loss:  21.840829849243164\n","epoch:  14 , batch:  788 , train loss:  17.2701416015625\n","epoch:  14 , batch:  789 , train loss:  9.554292678833008\n","epoch:  14 , batch:  790 , train loss:  13.393926620483398\n","epoch:  14 , batch:  791 , train loss:  21.31016731262207\n","epoch:  14 , batch:  792 , train loss:  10.917808532714844\n","epoch:  14 , batch:  793 , train loss:  14.310562133789062\n","epoch:  14 , batch:  794 , train loss:  9.678324699401855\n","epoch:  14 , batch:  795 , train loss:  15.805967330932617\n","epoch:  14 , batch:  796 , train loss:  13.936749458312988\n","epoch:  14 , batch:  797 , train loss:  11.982226371765137\n","epoch:  14 , batch:  798 , train loss:  15.96799373626709\n","epoch:  14 , batch:  799 , train loss:  18.976627349853516\n","epoch:  14 , batch:  800 , train loss:  14.407695770263672\n","epoch:  14 , batch:  801 , train loss:  15.208048820495605\n","epoch:  14 , batch:  802 , train loss:  11.093720436096191\n","epoch:  14 , batch:  803 , train loss:  15.55311107635498\n","epoch:  14 , batch:  804 , train loss:  12.640286445617676\n","epoch:  14 , batch:  805 , train loss:  7.893463134765625\n","epoch:  14 , batch:  806 , train loss:  9.05567455291748\n","epoch:  14 , batch:  807 , train loss:  6.8814311027526855\n","epoch:  14 , batch:  808 , train loss:  14.587100982666016\n","epoch:  14 , batch:  809 , train loss:  12.200020790100098\n","epoch:  14 , batch:  810 , train loss:  7.0749592781066895\n","epoch:  14 , batch:  811 , train loss:  14.763073921203613\n","epoch:  14 , batch:  812 , train loss:  13.109943389892578\n","epoch:  14 , batch:  813 , train loss:  12.304221153259277\n","epoch:  14 , batch:  814 , train loss:  15.05800724029541\n","epoch:  14 , batch:  815 , train loss:  11.60101318359375\n","epoch:  14 , batch:  816 , train loss:  9.962132453918457\n","epoch:  14 , batch:  817 , train loss:  13.200075149536133\n","epoch:  14 , batch:  818 , train loss:  18.91778564453125\n","epoch:  14 , batch:  819 , train loss:  20.699506759643555\n","epoch:  14 , batch:  820 , train loss:  17.193267822265625\n","epoch:  14 , batch:  821 , train loss:  15.437089920043945\n","epoch:  14 , batch:  822 , train loss:  11.595860481262207\n","epoch:  14 , batch:  823 , train loss:  17.496780395507812\n","epoch:  14 , batch:  824 , train loss:  15.22288703918457\n","epoch:  14 , batch:  825 , train loss:  12.294111251831055\n","epoch:  14 , batch:  826 , train loss:  12.208081245422363\n","epoch:  14 , batch:  827 , train loss:  20.70407485961914\n","epoch:  14 , batch:  828 , train loss:  26.044044494628906\n","epoch:  14 , batch:  829 , train loss:  19.238420486450195\n","epoch:  14 , batch:  830 , train loss:  11.850689888000488\n","epoch:  14 , batch:  831 , train loss:  17.352258682250977\n","epoch:  14 , batch:  832 , train loss:  14.047088623046875\n","epoch:  14 , batch:  833 , train loss:  20.65776252746582\n","epoch:  14 , batch:  834 , train loss:  12.077794075012207\n","epoch:  14 , batch:  835 , train loss:  21.808643341064453\n","epoch:  14 , batch:  836 , train loss:  29.764278411865234\n","epoch:  14 , batch:  837 , train loss:  9.239032745361328\n","epoch:  14 , batch:  838 , train loss:  13.090556144714355\n","epoch:  14 , batch:  839 , train loss:  19.07999038696289\n","epoch:  14 , batch:  840 , train loss:  21.2446346282959\n","epoch:  14 , batch:  841 , train loss:  23.511232376098633\n","epoch:  14 , batch:  842 , train loss:  10.334928512573242\n","epoch:  14 , batch:  843 , train loss:  14.591440200805664\n","epoch:  14 , batch:  844 , train loss:  14.067461967468262\n","epoch:  14 , batch:  845 , train loss:  16.49945831298828\n","epoch:  14 , batch:  846 , train loss:  17.492116928100586\n","epoch:  14 , batch:  847 , train loss:  18.307523727416992\n","epoch:  14 , batch:  848 , train loss:  16.16373062133789\n","epoch:  14 , batch:  849 , train loss:  13.009881019592285\n","epoch:  14 , batch:  850 , train loss:  11.148167610168457\n","epoch:  14 , batch:  851 , train loss:  8.707645416259766\n","epoch:  14 , batch:  852 , train loss:  6.879504203796387\n","epoch:  14 , batch:  853 , train loss:  10.14914608001709\n","epoch:  14 , batch:  854 , train loss:  21.46692657470703\n","epoch:  14 , batch:  855 , train loss:  19.785123825073242\n","epoch:  14 , batch:  856 , train loss:  11.461584091186523\n","epoch:  14 , batch:  857 , train loss:  8.795008659362793\n","epoch:  14 , batch:  858 , train loss:  8.71118450164795\n","epoch:  14 , batch:  859 , train loss:  16.64520263671875\n","epoch:  14 , batch:  860 , train loss:  11.236227035522461\n","epoch:  14 , batch:  861 , train loss:  20.368831634521484\n","epoch:  14 , batch:  862 , train loss:  10.654091835021973\n","epoch:  14 , batch:  863 , train loss:  9.341054916381836\n","epoch:  14 , batch:  864 , train loss:  20.639989852905273\n","epoch:  14 , batch:  865 , train loss:  12.728564262390137\n","epoch:  14 , batch:  866 , train loss:  9.488555908203125\n","epoch:  14 , batch:  867 , train loss:  11.165199279785156\n","epoch:  14 , batch:  868 , train loss:  16.997533798217773\n","epoch:  14 , batch:  869 , train loss:  15.059603691101074\n","epoch:  14 , batch:  870 , train loss:  22.558374404907227\n","epoch:  14 , batch:  871 , train loss:  17.520896911621094\n","epoch:  14 , batch:  872 , train loss:  13.603818893432617\n","epoch:  14 , batch:  873 , train loss:  8.67824649810791\n","epoch:  14 , batch:  874 , train loss:  12.590571403503418\n","epoch:  14 , batch:  875 , train loss:  21.733461380004883\n","epoch:  14 , batch:  876 , train loss:  23.15709686279297\n","epoch:  14 , batch:  877 , train loss:  19.76148223876953\n","epoch:  14 , batch:  878 , train loss:  24.37488555908203\n","epoch:  14 , batch:  879 , train loss:  15.504744529724121\n","epoch:  14 , batch:  880 , train loss:  10.135543823242188\n","epoch:  14 , batch:  881 , train loss:  17.98448944091797\n","epoch:  14 , batch:  882 , train loss:  9.765214920043945\n","epoch:  14 , batch:  883 , train loss:  14.359672546386719\n","epoch:  14 , batch:  884 , train loss:  15.28857707977295\n","epoch:  14 , batch:  885 , train loss:  21.141408920288086\n","epoch:  14 , batch:  886 , train loss:  16.709726333618164\n","epoch:  14 , batch:  887 , train loss:  14.642596244812012\n","epoch:  14 , batch:  888 , train loss:  14.105056762695312\n","epoch:  14 , batch:  889 , train loss:  15.660354614257812\n","epoch:  14 , batch:  890 , train loss:  17.854019165039062\n","epoch:  14 , batch:  891 , train loss:  12.468982696533203\n","epoch:  14 , batch:  892 , train loss:  15.590421676635742\n","epoch:  14 , batch:  893 , train loss:  11.062355995178223\n","epoch:  14 , batch:  894 , train loss:  16.814882278442383\n","epoch:  14 , batch:  895 , train loss:  7.6557536125183105\n","epoch:  14 , batch:  896 , train loss:  14.27418041229248\n","epoch:  14 , batch:  897 , train loss:  16.854724884033203\n","epoch:  14 , batch:  898 , train loss:  19.812347412109375\n","epoch:  14 , batch:  899 , train loss:  17.845571517944336\n","epoch:  14 , batch:  900 , train loss:  9.607467651367188\n","epoch:  14 , batch:  901 , train loss:  8.611181259155273\n","epoch:  14 , batch:  902 , train loss:  21.560657501220703\n","epoch:  14 , batch:  903 , train loss:  15.298627853393555\n","epoch:  14 , batch:  904 , train loss:  18.262779235839844\n","epoch:  14 , batch:  905 , train loss:  12.047581672668457\n","epoch:  14 , batch:  906 , train loss:  9.955506324768066\n","epoch:  14 , batch:  907 , train loss:  21.076725006103516\n","epoch:  14 , batch:  908 , train loss:  12.028188705444336\n","epoch:  14 , batch:  909 , train loss:  13.128776550292969\n","epoch:  14 , batch:  910 , train loss:  14.25144100189209\n","epoch:  14 , batch:  911 , train loss:  13.051095962524414\n","epoch:  14 , batch:  912 , train loss:  10.665964126586914\n","epoch:  14 , batch:  913 , train loss:  18.39594078063965\n","epoch:  14 , batch:  914 , train loss:  21.745758056640625\n","epoch:  14 , batch:  915 , train loss:  13.550657272338867\n","epoch:  14 , batch:  916 , train loss:  13.380850791931152\n","epoch:  14 , batch:  917 , train loss:  12.018868446350098\n","epoch:  14 , batch:  918 , train loss:  17.793546676635742\n","epoch:  14 , batch:  919 , train loss:  9.106343269348145\n","epoch:  14 , batch:  920 , train loss:  11.05787181854248\n","epoch:  14 , batch:  921 , train loss:  9.471434593200684\n","epoch:  14 , batch:  922 , train loss:  8.046104431152344\n","epoch:  14 , batch:  923 , train loss:  17.29692840576172\n","epoch:  14 , batch:  924 , train loss:  8.3861665725708\n","epoch:  14 , batch:  925 , train loss:  7.486094951629639\n","epoch:  14 , batch:  926 , train loss:  20.444904327392578\n","epoch:  14 , batch:  927 , train loss:  10.266654014587402\n","epoch:  14 , batch:  928 , train loss:  13.882864952087402\n","epoch:  14 , batch:  929 , train loss:  24.263776779174805\n","epoch:  14 , batch:  930 , train loss:  22.004150390625\n","epoch:  14 , batch:  931 , train loss:  26.65247917175293\n","epoch:  14 , batch:  932 , train loss:  16.356876373291016\n","epoch:  14 , batch:  933 , train loss:  9.164216041564941\n","epoch:  14 , batch:  934 , train loss:  12.917409896850586\n","epoch:  14 , batch:  935 , train loss:  8.287672996520996\n","epoch:  14 , batch:  936 , train loss:  17.99966812133789\n","epoch:  14 , batch:  937 , train loss:  7.730398178100586\n","Accuracy of train set: 0.91345\n","epoch:  14 , batch:  0 , test loss:  33.292388916015625\n","epoch:  14 , batch:  1 , test loss:  18.695444107055664\n","epoch:  14 , batch:  2 , test loss:  18.993915557861328\n","epoch:  14 , batch:  3 , test loss:  15.77466869354248\n","epoch:  14 , batch:  4 , test loss:  27.541791915893555\n","epoch:  14 , batch:  5 , test loss:  23.8564453125\n","epoch:  14 , batch:  6 , test loss:  27.040239334106445\n","epoch:  14 , batch:  7 , test loss:  13.536763191223145\n","epoch:  14 , batch:  8 , test loss:  20.112342834472656\n","epoch:  14 , batch:  9 , test loss:  19.822021484375\n","epoch:  14 , batch:  10 , test loss:  18.68632698059082\n","epoch:  14 , batch:  11 , test loss:  18.96422576904297\n","epoch:  14 , batch:  12 , test loss:  20.995128631591797\n","epoch:  14 , batch:  13 , test loss:  19.993181228637695\n","epoch:  14 , batch:  14 , test loss:  23.26268768310547\n","epoch:  14 , batch:  15 , test loss:  25.90308380126953\n","epoch:  14 , batch:  16 , test loss:  18.081087112426758\n","epoch:  14 , batch:  17 , test loss:  26.898374557495117\n","epoch:  14 , batch:  18 , test loss:  28.006122589111328\n","epoch:  14 , batch:  19 , test loss:  21.27747917175293\n","epoch:  14 , batch:  20 , test loss:  19.279512405395508\n","epoch:  14 , batch:  21 , test loss:  25.69986915588379\n","epoch:  14 , batch:  22 , test loss:  17.79561424255371\n","epoch:  14 , batch:  23 , test loss:  15.83857536315918\n","epoch:  14 , batch:  24 , test loss:  16.61764144897461\n","epoch:  14 , batch:  25 , test loss:  16.921489715576172\n","epoch:  14 , batch:  26 , test loss:  17.854820251464844\n","epoch:  14 , batch:  27 , test loss:  16.516231536865234\n","epoch:  14 , batch:  28 , test loss:  20.900781631469727\n","epoch:  14 , batch:  29 , test loss:  13.681640625\n","epoch:  14 , batch:  30 , test loss:  15.666632652282715\n","epoch:  14 , batch:  31 , test loss:  15.120272636413574\n","epoch:  14 , batch:  32 , test loss:  21.951745986938477\n","epoch:  14 , batch:  33 , test loss:  14.804688453674316\n","epoch:  14 , batch:  34 , test loss:  14.493356704711914\n","epoch:  14 , batch:  35 , test loss:  12.582963943481445\n","epoch:  14 , batch:  36 , test loss:  19.80332374572754\n","epoch:  14 , batch:  37 , test loss:  13.51219367980957\n","epoch:  14 , batch:  38 , test loss:  30.091840744018555\n","epoch:  14 , batch:  39 , test loss:  14.385504722595215\n","epoch:  14 , batch:  40 , test loss:  12.524590492248535\n","epoch:  14 , batch:  41 , test loss:  28.4883975982666\n","epoch:  14 , batch:  42 , test loss:  8.098020553588867\n","epoch:  14 , batch:  43 , test loss:  30.46682357788086\n","epoch:  14 , batch:  44 , test loss:  17.185165405273438\n","epoch:  14 , batch:  45 , test loss:  15.397843360900879\n","epoch:  14 , batch:  46 , test loss:  23.238771438598633\n","epoch:  14 , batch:  47 , test loss:  32.02129364013672\n","epoch:  14 , batch:  48 , test loss:  21.749160766601562\n","epoch:  14 , batch:  49 , test loss:  18.27286720275879\n","epoch:  14 , batch:  50 , test loss:  11.582974433898926\n","epoch:  14 , batch:  51 , test loss:  19.342758178710938\n","epoch:  14 , batch:  52 , test loss:  17.02509117126465\n","epoch:  14 , batch:  53 , test loss:  10.851176261901855\n","epoch:  14 , batch:  54 , test loss:  22.620330810546875\n","epoch:  14 , batch:  55 , test loss:  14.433667182922363\n","epoch:  14 , batch:  56 , test loss:  26.294803619384766\n","epoch:  14 , batch:  57 , test loss:  11.383689880371094\n","epoch:  14 , batch:  58 , test loss:  16.24605941772461\n","epoch:  14 , batch:  59 , test loss:  29.84610366821289\n","epoch:  14 , batch:  60 , test loss:  27.54867172241211\n","epoch:  14 , batch:  61 , test loss:  12.487711906433105\n","epoch:  14 , batch:  62 , test loss:  24.18596076965332\n","epoch:  14 , batch:  63 , test loss:  18.195646286010742\n","epoch:  14 , batch:  64 , test loss:  24.813987731933594\n","epoch:  14 , batch:  65 , test loss:  26.608640670776367\n","epoch:  14 , batch:  66 , test loss:  42.152217864990234\n","epoch:  14 , batch:  67 , test loss:  41.577938079833984\n","epoch:  14 , batch:  68 , test loss:  32.34770965576172\n","epoch:  14 , batch:  69 , test loss:  7.806705951690674\n","epoch:  14 , batch:  70 , test loss:  33.45849609375\n","epoch:  14 , batch:  71 , test loss:  16.553226470947266\n","epoch:  14 , batch:  72 , test loss:  27.361347198486328\n","epoch:  14 , batch:  73 , test loss:  16.839994430541992\n","epoch:  14 , batch:  74 , test loss:  7.98042106628418\n","epoch:  14 , batch:  75 , test loss:  18.6827392578125\n","epoch:  14 , batch:  76 , test loss:  17.564836502075195\n","epoch:  14 , batch:  77 , test loss:  17.95755958557129\n","epoch:  14 , batch:  78 , test loss:  35.456092834472656\n","epoch:  14 , batch:  79 , test loss:  13.045371055603027\n","epoch:  14 , batch:  80 , test loss:  11.566728591918945\n","epoch:  14 , batch:  81 , test loss:  15.728297233581543\n","epoch:  14 , batch:  82 , test loss:  40.059547424316406\n","epoch:  14 , batch:  83 , test loss:  20.10523223876953\n","epoch:  14 , batch:  84 , test loss:  36.09706115722656\n","epoch:  14 , batch:  85 , test loss:  18.655458450317383\n","epoch:  14 , batch:  86 , test loss:  24.43415641784668\n","epoch:  14 , batch:  87 , test loss:  23.93146514892578\n","epoch:  14 , batch:  88 , test loss:  19.030441284179688\n","epoch:  14 , batch:  89 , test loss:  25.111448287963867\n","epoch:  14 , batch:  90 , test loss:  21.91646957397461\n","epoch:  14 , batch:  91 , test loss:  12.255614280700684\n","epoch:  14 , batch:  92 , test loss:  21.05702018737793\n","epoch:  14 , batch:  93 , test loss:  24.114849090576172\n","epoch:  14 , batch:  94 , test loss:  22.427507400512695\n","epoch:  14 , batch:  95 , test loss:  34.46495819091797\n","epoch:  14 , batch:  96 , test loss:  20.5036563873291\n","epoch:  14 , batch:  97 , test loss:  12.755545616149902\n","epoch:  14 , batch:  98 , test loss:  9.934159278869629\n","epoch:  14 , batch:  99 , test loss:  14.952658653259277\n","epoch:  14 , batch:  100 , test loss:  9.920177459716797\n","epoch:  14 , batch:  101 , test loss:  27.482730865478516\n","epoch:  14 , batch:  102 , test loss:  19.683555603027344\n","epoch:  14 , batch:  103 , test loss:  15.185330390930176\n","epoch:  14 , batch:  104 , test loss:  15.579590797424316\n","epoch:  14 , batch:  105 , test loss:  16.392412185668945\n","epoch:  14 , batch:  106 , test loss:  9.047091484069824\n","epoch:  14 , batch:  107 , test loss:  16.90488052368164\n","epoch:  14 , batch:  108 , test loss:  23.915847778320312\n","epoch:  14 , batch:  109 , test loss:  24.88833236694336\n","epoch:  14 , batch:  110 , test loss:  24.118310928344727\n","epoch:  14 , batch:  111 , test loss:  24.06092071533203\n","epoch:  14 , batch:  112 , test loss:  25.77136993408203\n","epoch:  14 , batch:  113 , test loss:  25.448331832885742\n","epoch:  14 , batch:  114 , test loss:  22.766828536987305\n","epoch:  14 , batch:  115 , test loss:  20.475284576416016\n","epoch:  14 , batch:  116 , test loss:  24.896697998046875\n","epoch:  14 , batch:  117 , test loss:  28.32542610168457\n","epoch:  14 , batch:  118 , test loss:  12.167110443115234\n","epoch:  14 , batch:  119 , test loss:  11.455713272094727\n","epoch:  14 , batch:  120 , test loss:  26.7159481048584\n","epoch:  14 , batch:  121 , test loss:  29.326385498046875\n","epoch:  14 , batch:  122 , test loss:  32.63497543334961\n","epoch:  14 , batch:  123 , test loss:  31.438953399658203\n","epoch:  14 , batch:  124 , test loss:  13.390082359313965\n","epoch:  14 , batch:  125 , test loss:  24.061050415039062\n","epoch:  14 , batch:  126 , test loss:  11.78296947479248\n","epoch:  14 , batch:  127 , test loss:  24.346080780029297\n","epoch:  14 , batch:  128 , test loss:  23.937528610229492\n","epoch:  14 , batch:  129 , test loss:  37.2683219909668\n","epoch:  14 , batch:  130 , test loss:  21.20974349975586\n","epoch:  14 , batch:  131 , test loss:  12.368321418762207\n","epoch:  14 , batch:  132 , test loss:  17.309539794921875\n","epoch:  14 , batch:  133 , test loss:  19.311630249023438\n","epoch:  14 , batch:  134 , test loss:  25.694307327270508\n","epoch:  14 , batch:  135 , test loss:  33.48377227783203\n","epoch:  14 , batch:  136 , test loss:  44.826629638671875\n","epoch:  14 , batch:  137 , test loss:  36.83402633666992\n","epoch:  14 , batch:  138 , test loss:  30.81300926208496\n","epoch:  14 , batch:  139 , test loss:  35.57912063598633\n","epoch:  14 , batch:  140 , test loss:  27.107574462890625\n","epoch:  14 , batch:  141 , test loss:  31.36852264404297\n","epoch:  14 , batch:  142 , test loss:  16.780057907104492\n","epoch:  14 , batch:  143 , test loss:  19.51639747619629\n","epoch:  14 , batch:  144 , test loss:  18.06431770324707\n","epoch:  14 , batch:  145 , test loss:  26.417524337768555\n","epoch:  14 , batch:  146 , test loss:  22.062955856323242\n","epoch:  14 , batch:  147 , test loss:  35.74606704711914\n","epoch:  14 , batch:  148 , test loss:  14.831038475036621\n","epoch:  14 , batch:  149 , test loss:  22.159547805786133\n","epoch:  14 , batch:  150 , test loss:  25.243797302246094\n","epoch:  14 , batch:  151 , test loss:  28.03338050842285\n","epoch:  14 , batch:  152 , test loss:  26.54708480834961\n","epoch:  14 , batch:  153 , test loss:  23.007753372192383\n","epoch:  14 , batch:  154 , test loss:  34.01482391357422\n","epoch:  14 , batch:  155 , test loss:  29.41837501525879\n","epoch:  14 , batch:  156 , test loss:  4.068711280822754\n","Accuracy of pytorch_model set: 0.8845\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x700 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["train_model([train_loader, test_loader], num_epochs=15, learning_rate=0.005)"]},{"cell_type":"code","execution_count":31,"id":"ceb5783f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceb5783f","executionInfo":{"status":"ok","timestamp":1686157046545,"user_tz":-210,"elapsed":758,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}},"outputId":"f4367171-6b40-4439-ba08-c4108a7573c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final test accuracy: 0.8845\n"]}],"source":["print(f'Final test accuracy: {test_accuracies[-1]}')"]},{"cell_type":"markdown","id":"a5e128ed","metadata":{"id":"a5e128ed"},"source":["## Visualization of the labels and predictions\n","\n","In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."]},{"cell_type":"code","execution_count":39,"id":"6c0b79fd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"6c0b79fd","executionInfo":{"status":"ok","timestamp":1686158013613,"user_tz":-210,"elapsed":1544,"user":{"displayName":"Sepehr Kazemi","userId":"01492031432251706518"}},"outputId":"cf4065a1-c0d3-45dd-bd74-023b5e1b11b0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x700 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["## FILL HERE\n","classes = FashionMNIST.classes\n","dataset_size = len(test_set)\n","indexes = np.zeros(num_classes)\n","plt.figure(figsize=(12,7))\n","for i in range(0,num_classes):\n","    index = np.random.choice(dataset_size)\n","    while test_set[index][1] != i:\n","        index = np.random.choice(dataset_size)\n","    x = test_set[index][0].to(device)\n","    y = test_set[index][1]\n","    p = model(x, parameters)\n","    y_pred = p.argmax(dim=-1)\n","    plt.subplot(2,5,i+1)\n","    plt.imshow(x.cpu()[0,:,:],cmap='gray')\n","    plt.title(\"real: \"+classes[y])\n","    plt.xlabel(\"predicted: \"+classes[y_pred])\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}